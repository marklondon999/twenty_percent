{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducible Guide: CPU Training of GPT-2 (from Scratch)\n",
    "\n",
    "This Jupyter Notebook provides a step-by-step guide to set up a Python environment and train a small GPT-2 language model on the CPU. This guide is designed to be reproducible and explicitly specifies all dependencies and versions used. It addresses the previous issue of GPU interference and ensures CPU-only training.\n",
    "\n",
    "**Important:** This guide trains on a *very small* example dataset and uses the base `gpt2` model, which is not designed for high-quality text generation. The goal here is to establish a working CPU training setup. To generate better text, you will need to use a larger model variant (like `gpt2-medium`, `gpt2-large`, or `gpt2-xl`) and train on a much more substantial and relevant dataset (see \"Next Steps\" at the end)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup: Create a New Conda Environment\n",
    "\n",
    "It's best practice to create a dedicated Conda environment to isolate project dependencies. We'll create an environment named `cpu_gpt2_env` with Python 3.10 (you can adjust the Python version if needed).",
    "\n",
    "**Open your terminal or Anaconda Prompt and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n cpu_gpt2_env python=3.10\n",
    "conda activate cpu_gpt2_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies with Explicit Versions\n",
    "\n",
    "We will now install all the necessary Python packages with specific versions to ensure reproducibility. We will install PyTorch for CPU, `transformers` from the Hugging Face GitHub development branch (to get the latest features and CPU-related fixes), and the `datasets` library.",
    "\n",
    "**Run the following `pip install` commands in your activated `cpu_gpt2_env` environment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (CPU version). Get the latest stable CPU wheel URL from pytorch.org if needed.\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# Install transformers from Hugging Face GitHub development branch (for latest features and fixes)\n",
    "pip install --no-cache-dir --force-reinstall git+https://github.com/huggingface/transformers.git\n",
    "\n",
    "# Install datasets library (latest stable version)\n",
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Python Code for CPU Training (`cpu_trainer.py`)\n",
    "\n",
    "This is the Python code that will perform the CPU-based training of GPT-2.  Save this code as `cpu_trainer.py` in your project directory. This code includes all the necessary fixes and CPU-specific settings we've discussed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"gpt2\"  # A small, readily available model\n",
    "OUTPUT_DIR = \"./cpu_trained_model\"\n",
    "\n",
    "# --- Minimal Example Data ---\n",
    "mini_data = [\n",
    "    {\"text\": \"The quick brown fox\"},\n",
    "    {\"text\": \"jumps over the lazy\"},\n",
    "    {\"text\": \"dog. This is a test.\"},\n",
    "    {\"text\": \"Another example sentence.\"}\n",
    "]\n",
    "\n",
    "# --- Tokenizer Setup ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Crucial for padding\n",
    "\n",
    "def tokenize_function(example):\n",
    "    \"\"\"Tokenizes a single example.\"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"text\"],\n",
    "        max_length=32,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\", # Request PyTorch tensors\n",
    "    )\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].clone() #labels are input ids\n",
    "    return {key: value.squeeze(0) for key, value in tokenized_inputs.items()}\n",
    "\n",
    "# --- Create and Tokenize Dataset ---\n",
    "dataset = Dataset.from_list(mini_data)\n",
    "tokenized_dataset = dataset.map(tokenize_function)\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "\n",
    "# --- TrainingArguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"epoch\",\n",
    "    device=\"cpu\",  # Explicitly set device to CPU\n",
    "    no_cuda=True, # Ensure no CUDA is used\n",
    "    # NO GPU-related settings here!\n",
    ")\n",
    "\n",
    "# --- Load Model (on CPU) ---\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.to(\"cpu\")  # Explicitly put the model on the CPU\n",
    "\n",
    "def custom_data_collator(features):\n",
    "    \"\"\"Collates a batch of tokenized examples with detailed debugging.\"\"\"\n",
    "    batch = {}\n",
    "    print(\"\\n--- Inside data collator ---\")\n",
    "    print(f\"Type of 'features': {type(features)}\")\n",
    "    if features:\n",
    "        print(f\"Length of 'features': {len(features)}\")\n",
    "        print(f\"Type of 'features[0]': {type(features[0])}\")\n",
    "\n",
    "        if isinstance(features[0], dict):\n",
    "            for key in features[0].keys():\n",
    "                print(f\"\\n  --- Key: '{key}' ---\")\n",
    "                print(f\"  Type of 'features[0][key]': {type(features[0][key])}\")\n",
    "\n",
    "                elements_for_stack = [f[key] for f in features]\n",
    "                print(f\"  Type of 'elements_for_stack': {type(elements_for_stack)}\")\n",
    "                if elements_for_stack:\n",
    "                    print(f\"  Type of 'elements_for_stack[0]': {type(elements_for_stack[0])}\")\n",
    "\n",
    "                    if isinstance(elements_for_stack[0], list):\n",
    "                        print(f\"  Type of 'elements_for_stack[0][0]': {type(elements_for_stack[0][0]) if elements_for_stack[0] else 'empty list'}\")\n",
    "                        print(f\"  Value of 'elements_for_stack[0]': {elements_for_stack[0]}\")\n",
    "                    elif isinstance(elements_for_stack[0], torch.Tensor):\n",
    "                        print(f\"  Shape of 'elements_for_stack[0]': {elements_for_stack[0].shape}\")\n",
    "                    else:\n",
    "                        print(f\"  Value of 'elements_for_stack[0]': {elements_for_stack[0]}\")\n",
    "\n",
    "                try:\n",
    "                    batch[key] = torch.stack(elements_for_stack)\n",
    "                    print(f\"  'batch[key]' stacked successfully. Type: {type(batch[key])}, Shape: {batch[key].shape}\")\n",
    "                except TypeError as e:\n",
    "                    print(f\"  TypeError during torch.stack for key '{key}': {e}\")\n",
    "        else:\n",
    "            print(\"  'features[0]' is NOT a dictionary!\")\n",
    "\n",
    "    print(\"--- End data collator ---\\n\")\n",
    "    return batch\n",
    "\n",
    "# --- Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model, # Model is already on CPU\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=custom_data_collator,\n",
    ")\n",
    "\n",
    "# --- Train (on CPU) ---\n",
    "trainer.train()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# --- Inference (Example) ---\n",
    "model.eval()\n",
    "prompt = \"The quick brown\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids.to(\"cpu\"), max_new_tokens=10, do_sample=True) # input_ids to CPU again, and model should be on CPU\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_text}\")\n",
    "\n",
    "#---Save---\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execution Instructions\n",
    "\n",
    "1.  **Save the code:** Make sure you have saved the Python code from section 3 as `cpu_trainer.py` in your current working directory.\n",
    "2.  **Activate the Conda environment:** If you haven't already, activate the `cpu_gpt2_env` environment in your terminal:\n",
    "    ```bash\n",
    "    conda activate cpu_gpt2_env\n",
    "    ```\n",
    "3.  **Set Environment Variables (Crucial for CPU forcing):**  Before running the script, set these environment variables in your terminal. This is essential to explicitly disable GPU usage and force CPU execution:\n",
    "    ```bash\n",
    "    export CUDA_VISIBLE_DEVICES=\"\"\n",
    "    export PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.8,max_split_size_mb:512\n",
    "    ```\n",
    "4.  **Run the Python script:** Execute the training script from your terminal:\n",
    "    ```bash\n",
    "    python cpu_trainer.py\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verification of CPU Usage\n",
    "\n",
    "To ensure that the training and inference are indeed running on the CPU (and not accidentally using the GPU), you can monitor your system's resource usage during the script execution.\n",
    "\n",
    "**Using System Monitoring Tools:**\n",
    "\n",
    "*   **Linux/macOS:** Open a separate terminal and use tools like `top` or `htop`. Look for high CPU utilization by the Python process running `cpu_trainer.py`. You should see very little to no GPU utilization (GPU usage should be close to 0%).\n",
    "*   **Windows:** Use Task Manager (Ctrl+Shift+Esc). Go to the \"Performance\" tab and monitor CPU and GPU usage while the script is running.  CPU usage should be significant, and GPU usage should be minimal.\n",
    "\n",
    "**Script Output:**\n",
    "\n",
    "Examine the output of `cpu_trainer.py` in your terminal. You should see:\n",
    "\n",
    "*   Training progress (progress bar and loss values).\n",
    "*   The \"Prompt:\" and \"Generated Text:\" output at the end, indicating successful inference.\n",
    "*   **Crucially, the absence of any `RuntimeError` or CUDA-related warnings** (especially the `RuntimeError: Expected all tensors to be on the same device...` error we were previously encountering).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps and Improvements (Text Quality)\n",
    "\n",
    "As mentioned earlier, the text generated by `gpt2-base` trained on `mini_data` will be of limited quality. To improve the generated text:\n",
    "\n",
    "*   **Use a Larger GPT-2 Model Variant:** Try `MODEL_NAME = \"gpt2-medium\"`, `\"gpt2-large\"`, or `\"gpt2-xl\"` in the code (be mindful of CPU RAM and training speed).\n",
    "*   **Use a Larger and More Relevant Dataset:** Explore the Hugging Face Datasets Hub ([https://huggingface.co/datasets](https://huggingface.co/datasets)) for datasets relevant to the type of text you want to generate.\n",
    "*   **Train for More Epochs:**  Consider increasing `num_train_epochs` in `TrainingArguments` (but monitor training time on CPU).\n",
    "*   **Experiment with Generation Parameters:**  Adjust parameters like `temperature`, `top_k`, `top_p`, and `num_beams` in the `model.generate()` function to control the style and quality of generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "Congratulations! You have successfully set up a reproducible environment for CPU-based training of GPT-2 and have a working script. By following these steps, you should be able to train and run inference on your CPU without encountering GPU-related device errors. Remember that improving text quality requires using larger models and, most importantly, larger and more relevant datasets. Happy experimenting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
