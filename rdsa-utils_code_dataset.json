[
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\logging.py",
        "function_name": "log_dev",
        "code_chunk": "def log_dev(self, message, *args, **kwargs):  # noqa: E302\n    \"\"\"Create a custom log level between INFO and DEBUG named DEV.\n\n    This is lifted from: https://stackoverflow.com/a/13638084\n    \"\"\"\n    if self.isEnabledFor(LOG_DEV_LEVEL_NUM):\n        # Yes, logger takes its '*args' as 'args'.\n        self._log(LOG_DEV_LEVEL_NUM, message, args, **kwargs)",
        "variables": [
            "self",
            "args",
            "kwargs",
            "message"
        ],
        "docstring": "Create a custom log level between INFO and DEBUG named DEV.\n\nThis is lifted from: https://stackoverflow.com/a/13638084"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\logging.py",
        "function_name": "init_logger_basic",
        "code_chunk": "def init_logger_basic(log_level: int) -> None:\n    \"\"\"Instantiate a basic logger object to be used across modules.\n\n    By using this function to instantiate the logger, you also have access to\n    `logger.dev` for log_level=15, as this is defined in the same module scope\n    as this function.\n\n    Parameters\n    ----------\n    log_level\n        The level of logging to be recorded. Can be defined either as the\n        integer level or the logging.<LEVEL> values in line with the\n        definitions of the logging module\n        (see - https://docs.python.org/3/library/logging.html#levels)\n\n    Returns\n    -------\n    None\n        The logger created by this function is available in any other modules\n        by using `logger = logging.getLogger(__name__)` at the global scope\n        level in a module (i.e. below imports, not in a function).\n    \"\"\"\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n\n    logger.dev(\n        \"\"\"\n    Initialised logger for pipeline.\n\n    Also have access to `logger.dev` by using this function.\n    \"\"\",\n    )",
        "variables": [
            "log_level"
        ],
        "docstring": "Instantiate a basic logger object to be used across modules.\n\nBy using this function to instantiate the logger, you also have access to\n`logger.dev` for log_level=15, as this is defined in the same module scope\nas this function.\n\nParameters\n----------\nlog_level\n    The level of logging to be recorded. Can be defined either as the\n    integer level or the logging.<LEVEL> values in line with the\n    definitions of the logging module\n    (see - https://docs.python.org/3/library/logging.html#levels)\n\nReturns\n-------\nNone\n    The logger created by this function is available in any other modules\n    by using `logger = logging.getLogger(__name__)` at the global scope\n    level in a module (i.e. below imports, not in a function)."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\logging.py",
        "function_name": "init_logger_advanced",
        "code_chunk": "def init_logger_advanced(\n    log_level: int,\n    handlers: Optional[List[logging.Handler]] = None,\n    log_format: str = None,\n    date_format: str = None,\n) -> None:\n    \"\"\"Instantiate a logger with provided handlers.\n\n    This function allows the logger to be used across modules. Logs can be\n    handled by any number of handlers, e.g., FileHandler, StreamHandler, etc.,\n    provided in the `handlers` list.\n\n    Parameters\n    ----------\n    log_level\n        The level of logging to be recorded. Can be defined either as the\n        integer level or the logging.<LEVEL> values in line with the definitions\n        of the logging module.\n        (see - https://docs.python.org/3/library/logging.html#levels)\n    handlers\n        List of handler instances to be added to the logger. Each handler\n        instance must be a subclass of `logging.Handler`. Default is an\n        empty list, and in this case, basicConfig with `log_level`,\n        `log_format`, and `date_format` is used.\n    log_format\n        The format of the log message. If not provided, a default format\n        `'%(asctime)s %(levelname)s %(name)s: %(message)s'` is used.\n    date_format\n        The format of the date in the log message. If not provided, a default\n        format `'%Y-%m-%d %H:%M:%S'` is used.\n\n    Returns\n    -------\n    None\n        The logger created by this function is available in any other modules\n        by using `logging.getLogger(__name__)` at the global scope level in a\n        module (i.e., below imports, not in a function).\n\n    Raises\n    ------\n    ValueError\n        If any item in the `handlers` list is not an instance of\n        `logging.Handler`.\n\n    Examples\n    --------\n    >>> file_handler = logging.FileHandler('logfile.log')\n    >>> rich_handler = RichHandler()\n    >>> init_logger_advanced(\n    ...     logging.DEBUG,\n    ...     [file_handler, rich_handler],\n    ...     \"%(levelname)s: %(message)s\",\n    ...     \"%H:%M:%S\"\n    ... )\n    \"\"\"\n    # Set default log format and date format if not provided\n    if log_format is None:\n        log_format = \"%(asctime)s %(levelname)s %(name)s: %(message)s\"\n    if date_format is None:\n        date_format = \"%Y-%m-%d %H:%M:%S\"\n\n    # Prepare a formatter\n    formatter = logging.Formatter(log_format, date_format)\n\n    # Create a logger\n    logger = logging.getLogger(__name__)\n    logger.setLevel(log_level)\n\n    # Check if handlers is None, if so assign an empty list to it\n    if handlers is None:\n        handlers = []\n\n    # Validate each handler\n    for handler in handlers:\n        if not isinstance(handler, logging.Handler):\n            msg = (\n                f\"Handler {handler} is not an instance of \"\n                f\"logging.Handler or its subclasses\"\n            )\n            raise ValueError(\n                msg,\n            )\n\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n\n    # If no handlers provided, use basicConfig\n    if not handlers:\n        logging.basicConfig(\n            level=log_level,\n            format=log_format,\n            datefmt=date_format,\n        )\n\n    logger.debug(\"Initialised logger for pipeline.\")",
        "variables": [
            "log_level",
            "formatter",
            "handlers",
            "logger",
            "log_format",
            "handler",
            "msg",
            "date_format"
        ],
        "docstring": "Instantiate a logger with provided handlers.\n\nThis function allows the logger to be used across modules. Logs can be\nhandled by any number of handlers, e.g., FileHandler, StreamHandler, etc.,\nprovided in the `handlers` list.\n\nParameters\n----------\nlog_level\n    The level of logging to be recorded. Can be defined either as the\n    integer level or the logging.<LEVEL> values in line with the definitions\n    of the logging module.\n    (see - https://docs.python.org/3/library/logging.html#levels)\nhandlers\n    List of handler instances to be added to the logger. Each handler\n    instance must be a subclass of `logging.Handler`. Default is an\n    empty list, and in this case, basicConfig with `log_level`,\n    `log_format`, and `date_format` is used.\nlog_format\n    The format of the log message. If not provided, a default format\n    `'%(asctime)s %(levelname)s %(name)s: %(message)s'` is used.\ndate_format\n    The format of the date in the log message. If not provided, a default\n    format `'%Y-%m-%d %H:%M:%S'` is used.\n\nReturns\n-------\nNone\n    The logger created by this function is available in any other modules\n    by using `logging.getLogger(__name__)` at the global scope level in a\n    module (i.e., below imports, not in a function).\n\nRaises\n------\nValueError\n    If any item in the `handlers` list is not an instance of\n    `logging.Handler`.\n\nExamples\n--------\n>>> file_handler = logging.FileHandler('logfile.log')\n>>> rich_handler = RichHandler()\n>>> init_logger_advanced(\n...     logging.DEBUG,\n...     [file_handler, rich_handler],\n...     \"%(levelname)s: %(message)s\",\n...     \"%H:%M:%S\"\n... )"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\logging.py",
        "function_name": "timer_args",
        "code_chunk": "def timer_args(\n    name: str,\n    logger: Optional[Callable[[str], None]] = logger.info,\n) -> Dict[str, str]:\n    \"\"\"Initialise timer args workaround for 'text' args in codetiming package.\n\n    Works with codetiming==1.4.0\n\n    Parameters\n    ----------\n    name\n        The name of the specific timer log.\n    logger\n        Optional logger function that can accept a string argument.\n\n    Returns\n    -------\n    Dict[str, str]\n        Dictionary of arguments to pass to specifc codetiming package Timer.\n    \"\"\"\n    return {\n        \"name\": name,\n        \"text\": lambda secs: name + f\": {format_timespan(secs)}\",\n        \"logger\": logger,\n        \"initial_text\": \"Running {name}\",\n    }",
        "variables": [
            "logger",
            "name"
        ],
        "docstring": "Initialise timer args workaround for 'text' args in codetiming package.\n\nWorks with codetiming==1.4.0\n\nParameters\n----------\nname\n    The name of the specific timer log.\nlogger\n    Optional logger function that can accept a string argument.\n\nReturns\n-------\nDict[str, str]\n    Dictionary of arguments to pass to specifc codetiming package Timer."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\logging.py",
        "function_name": "print_full_table_and_raise_error",
        "code_chunk": "def print_full_table_and_raise_error(\n    df: pd.DataFrame,\n    message: str,\n    stop_pipeline: bool = False,\n    show_records: bool = False,\n) -> None:\n    \"\"\"Output dataframe records to logger.\n\n    The purpose of this function is to enable a user to output a message\n    to the logger with the added functionality of stopping the pipeline\n    and showing dataframe records in a table format. It may be used for\n    instance if a user wants to check the records in a dataframe when it\n    expected to be empty.\n\n    Parameters\n    ----------\n    df\n        The dataframe to display records from.\n    message\n        The message to output to the logger.\n    stop_pipeline\n        Switch for the user to stop the pipeline and raise an error.\n    show_records\n        Switch to show records in a dataframe.\n\n    Returns\n    -------\n    None\n        Displays message to user however nothing is returned from\n        function.\n\n    Raises\n    ------\n    ValueError\n        Raises error and stops pipeline if switch applied.\n    \"\"\"\n    if show_records & stop_pipeline:\n        logger.error(df.to_string())\n        logger.error(message)\n        raise ValueError\n\n    elif stop_pipeline:\n        logger.error(message)\n        raise ValueError\n\n    elif show_records:\n        logger.info(df.to_string())\n        logger.info(message)\n\n    else:\n        logger.info(message)\n\n    return None",
        "variables": [
            "stop_pipeline",
            "message",
            "show_records",
            "df"
        ],
        "docstring": "Output dataframe records to logger.\n\nThe purpose of this function is to enable a user to output a message\nto the logger with the added functionality of stopping the pipeline\nand showing dataframe records in a table format. It may be used for\ninstance if a user wants to check the records in a dataframe when it\nexpected to be empty.\n\nParameters\n----------\ndf\n    The dataframe to display records from.\nmessage\n    The message to output to the logger.\nstop_pipeline\n    Switch for the user to stop the pipeline and raise an error.\nshow_records\n    Switch to show records in a dataframe.\n\nReturns\n-------\nNone\n    Displays message to user however nothing is returned from\n    function.\n\nRaises\n------\nValueError\n    Raises error and stops pipeline if switch applied."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\logging.py",
        "function_name": "log_spark_df_schema",
        "code_chunk": "def log_spark_df_schema(\n    _func: Callable = None,\n    *,\n    log_schema_on_input: bool = True,\n) -> Callable:\n    \"\"\"Apply decorator to log dataframe schema before and after a function.\n\n    If you use the `df.printSchema() method directly in a print/log statement\n    the code is processed and printed regardless of logging leve. Instead you\n    need to capture the output and pass this to the logger. See explanaition\n    here - https://stackoverflow.com/a/59935109\n\n    Requires that the function being decorated has a parameter called `df` and\n    that the function is called with `df` being a keyword argument (e.g.\n    `df=df`). If not the decorator will report back that it could not count the\n    number of rows of the dataframe before running the decorated function.\n\n    Parameters\n    ----------\n    log_schema_on_input\n        If set to false, then no schema is attempted to be printed for the\n        decorated function on input. This is useful for instance where function\n        has no df input but does return one (such as when reading a table).\n\n    Notes\n    -----\n    Explainer on complex decorators (and template for decorator structure):\n    https://realpython.com/primer-on-python-decorators/#both-please-but-never-mind-the-bread\n\n    Usage\n    -----\n    To use decorator to record input and output schema:\n    ```python\n    >>> @log_spark_df_schema\n    >>> def my_func_that_changes_some_columns(some_args, df, some_other_args):\n    >>>    ...\n    >>>    returns final_df\n    >>>\n    >>> some_df = my_func_that_changes_some_columns(\n    >>>     some_args='hello',\n    >>>     df=input_df,\n    >>>     some_other_args='world'\n    >>> )\n\n    Schema of dataframe before my_func_that_changes_some_columns:\n    root\n    |-- price: double (nullable = true)\n    |-- quantity: long (nullable = true)\n\n    Schema of dataframe after my_func_that_changes_some_columns:\n    root\n    |-- price: double (nullable = true)\n    |-- quantity: long (nullable = true)\n    |-- expenditure: double (nullable = true)\n    ```\n\n    To use decorator to record output schema only:\n    ```python\n    >>> @log_spark_df_schema(log_schema_on_input=False)\n    >>> def my_func_that_changes_some_columns(some_args, df, some_other_args):\n    >>>    ...\n    >>>    returns final_df\n    >>>\n    >>> some_df = my_func_that_changes_some_columns(\n    >>>     some_args='hello',\n    >>>     df=input_df,\n    >>>     some_other_args='world'\n    >>> )\n\n    Not printing schema of dataframe before my_func_that_changes_some_columns\n\n    Schema of dataframe after my_func_that_changes_some_columns:\n    root\n    |-- price: double (nullable = true)\n    |-- quantity: long (nullable = true)\n    |-- expenditure: double (nullable = true)\n    ```\n    \"\"\"  # noqa: E501\n\n    def decorator_function(func):\n        @functools.wraps(func)\n        def wrapper_decorator(*args, **kwargs):\n            # Define the name of the function being decorated for use in logs.\n            func_name = func.__name__\n\n            if log_schema_on_input:\n                if not kwargs.get(\"df\"):\n                    logger.warning(\n                        dedent(\n                            f\"\"\"\n                    Cannot find `df` in keyword named arguments.\n\n                    To use the log_spark_df_schema decorator with the function\n                    {func_name}:\n                    * it must have a parameter called df that is a spark dataframe.\n                    * it must be called specifying the argument names e.g.\n                    {func_name}(df=input_df, ... )\n                    \"\"\",\n                        ),\n                    )  # noqa: E501\n                elif isinstance(kwargs[\"df\"], SparkDF):\n                    schema = kwargs[\"df\"]._jdf.schema().treeString()\n                    logger.info(\n                        f\"Schema of dataframe before {func_name}:\\n{schema}\",\n                    )  # noqa: E501\n                else:\n                    logger.warning(\n                        dedent(\n                            f\"\"\"\n                    {func_name} keyword argument `df` has type {type(kwargs['df'])}.\n\n                    Cannot print spark schema for this type of object.\n                    \"\"\",\n                        ),\n                    )  # noqa: E501\n\n            else:\n                logger.info(\n                    f\"Not printing schema of dataframe before {func_name}\",\n                )  # noqa: E501\n\n            # Run the decorated function in its normal way, but catch its\n            # output so its schema can be printed.\n            df_return = func(*args, **kwargs)\n\n            # Check to ensure that the function returns a single value that is\n            # a spark dataframe, as otherwise the print schema operation will\n            # fail.\n            if isinstance(df_return, SparkDF):\n                schema = df_return._jdf.schema().treeString()\n                logger.info(\n                    f\"Schema of dataframe after {func_name}:\\n{schema}\",\n                )  # noqa: E501\n            else:\n                logger.warning(\n                    f\"{func_name} should return a spark dataframe for decorator, \"\n                    f\"but returned {type(df_return)}\",\n                )\n\n            return df_return\n\n        return wrapper_decorator\n\n    if _func is None:\n        return decorator_function\n    else:\n        return decorator_function(_func)",
        "variables": [
            "_func",
            "df_return",
            "log_schema_on_input",
            "func_name",
            "schema"
        ],
        "docstring": "Apply decorator to log dataframe schema before and after a function.\n\nIf you use the `df.printSchema() method directly in a print/log statement\nthe code is processed and printed regardless of logging leve. Instead you\nneed to capture the output and pass this to the logger. See explanaition\nhere - https://stackoverflow.com/a/59935109\n\nRequires that the function being decorated has a parameter called `df` and\nthat the function is called with `df` being a keyword argument (e.g.\n`df=df`). If not the decorator will report back that it could not count the\nnumber of rows of the dataframe before running the decorated function.\n\nParameters\n----------\nlog_schema_on_input\n    If set to false, then no schema is attempted to be printed for the\n    decorated function on input. This is useful for instance where function\n    has no df input but does return one (such as when reading a table).\n\nNotes\n-----\nExplainer on complex decorators (and template for decorator structure):\nhttps://realpython.com/primer-on-python-decorators/#both-please-but-never-mind-the-bread\n\nUsage\n-----\nTo use decorator to record input and output schema:\n```python\n>>> @log_spark_df_schema\n>>> def my_func_that_changes_some_columns(some_args, df, some_other_args):\n>>>    ...\n>>>    returns final_df\n>>>\n>>> some_df = my_func_that_changes_some_columns(\n>>>     some_args='hello',\n>>>     df=input_df,\n>>>     some_other_args='world'\n>>> )\n\nSchema of dataframe before my_func_that_changes_some_columns:\nroot\n|-- price: double (nullable = true)\n|-- quantity: long (nullable = true)\n\nSchema of dataframe after my_func_that_changes_some_columns:\nroot\n|-- price: double (nullable = true)\n|-- quantity: long (nullable = true)\n|-- expenditure: double (nullable = true)\n```\n\nTo use decorator to record output schema only:\n```python\n>>> @log_spark_df_schema(log_schema_on_input=False)\n>>> def my_func_that_changes_some_columns(some_args, df, some_other_args):\n>>>    ...\n>>>    returns final_df\n>>>\n>>> some_df = my_func_that_changes_some_columns(\n>>>     some_args='hello',\n>>>     df=input_df,\n>>>     some_other_args='world'\n>>> )\n\nNot printing schema of dataframe before my_func_that_changes_some_columns\n\nSchema of dataframe after my_func_that_changes_some_columns:\nroot\n|-- price: double (nullable = true)\n|-- quantity: long (nullable = true)\n|-- expenditure: double (nullable = true)\n```"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\logging.py",
        "function_name": "log_rows_in_spark_df",
        "code_chunk": "def log_rows_in_spark_df(func: Callable) -> Callable:\n    \"\"\"Apply decorator to log dataframe row count before and after a function.\n\n    Requires that the function being decorated has a parameter called `df` and\n    that the function is called with `df` being a keyword argument (e.g.\n    `df=df`). If not the decorator will report back that it could not count the\n    number of rows of the dataframe before running the decorated function.\n\n    Usage\n    -----\n    ```python\n    @log_rows_in_spark_df\n    def my_func_that_changes_no_rows(some_args, df, some_other_args):\n       ...\n       returns final_df\n\n    some_df = my_func_that_changes_no_rows(\n        some_args='hello',\n        df=input_df,\n        some_other_args='world'\n    )\n\n    >>> Rows in dataframe before my_func_that_changes_no_rows : 12345\n    >>> Rows in dataframe after my_func_that_changes_no_rows  : 6789\n    ```\n\n    Warning:\n    -------\n    `.count()` is an expensive spark operation to perform. Overuse of this\n    decorator can be detrimental to performance. This decorator will cache the\n    input dataframe prior to running the count and decorated function, as well\n    as persisting the output dataframe prior to counting. The input dataframe\n    is also unpersisted from memory prior to the decorator completing.\n    \"\"\"\n    logger.debug(\n        \"\"\"\n    log_rows_in_spark_df caches and persists spark dataframes to memory.\n    It also performs count operations. Both of these could have an adverse\n    effect on pipelines if used incorrectly, so use as decorator with care.\n    \"\"\",\n    )\n\n    @functools.wraps(func)\n    def wrapper_decorator(*args, **kwargs):\n        # Define the name of the function being decorated for use in logs.\n        func_name = func.__name__\n\n        if not kwargs.get(\"df\"):\n            logger.warning(\n                dedent(\n                    f\"\"\"\n            Cannot find `df` in keyword named arguments.\n\n            To use the log_rows_in_spark_df decorator with the function\n            {func_name}:\n            * it must have a parameter called df that is a spark dataframe.\n            * it must be called specifying the argument names e.g.\n            {func_name}(df=input_df, ... )\n            \"\"\",\n                ),\n            )\n        elif isinstance(kwargs[\"df\"], SparkDF):\n            # If not already cached, cache the dataframe prior to counting to\n            # allow more efficient processing in function. Is unpersisted at\n            # end of decorator.\n            if not kwargs[\"df\"].is_cached:\n                kwargs[\"df\"].cache()\n\n            logger.info(\n                f\"Rows in dataframe before {func_name} : {kwargs['df'].count()}\",\n            )  # noqa: E501\n        else:\n            logger.warning(\n                dedent(\n                    f\"\"\"\n            {func_name} keyword argument `df` has type {type(kwargs['df'])}.\n\n            Cannot count rows for this type of object.\n            \"\"\",\n                ),\n            )\n\n        # Run the decorated function in its normal way, but catch its output\n        # so it can be counted.\n        df_return = func(*args, **kwargs)\n\n        # Check to ensure that the function returns a single value that is a\n        # spark dataframe, as otherwise the count operation will fail.\n        if isinstance(df_return, SparkDF):\n            # Persist the dataframe to be returned prior to counting to allow\n            # more efficient processing downstream. We persist here as we will\n            # not be removing this dataframe from the in-memory cache in this\n            # decorator. Therefore, we don't want it to get pushed onto disk\n            # (and incur an expensive swap operation).\n            df_return.persist(StorageLevel.MEMORY_ONLY)\n            logger.info(\n                f\"Rows in dataframe after {func_name}  : {df_return.count()}\",\n            )  # noqa: E501\n\n        else:\n            logger.warning(\n                f\"{func_name} should return a spark dataframe for decorator, \"\n                f\"but returned {type(df_return)}\",\n            )\n\n        if kwargs.get(\"df\"):\n            # Unpersist the cached input df to manage memory.\n            kwargs[\"df\"].unpersist()\n\n        return df_return\n\n    return wrapper_decorator",
        "variables": [
            "func_name",
            "func",
            "df_return"
        ],
        "docstring": "Apply decorator to log dataframe row count before and after a function.\n\nRequires that the function being decorated has a parameter called `df` and\nthat the function is called with `df` being a keyword argument (e.g.\n`df=df`). If not the decorator will report back that it could not count the\nnumber of rows of the dataframe before running the decorated function.\n\nUsage\n-----\n```python\n@log_rows_in_spark_df\ndef my_func_that_changes_no_rows(some_args, df, some_other_args):\n   ...\n   returns final_df\n\nsome_df = my_func_that_changes_no_rows(\n    some_args='hello',\n    df=input_df,\n    some_other_args='world'\n)\n\n>>> Rows in dataframe before my_func_that_changes_no_rows : 12345\n>>> Rows in dataframe after my_func_that_changes_no_rows  : 6789\n```\n\nWarning:\n-------\n`.count()` is an expensive spark operation to perform. Overuse of this\ndecorator can be detrimental to performance. This decorator will cache the\ninput dataframe prior to running the count and decorated function, as well\nas persisting the output dataframe prior to counting. The input dataframe\nis also unpersisted from memory prior to the decorator completing."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\logging.py",
        "function_name": "add_warning_message_to_function",
        "code_chunk": "def add_warning_message_to_function(\n    _func: Callable = None,\n    *,\n    message: Optional[str] = None,\n) -> Callable:\n    \"\"\"Apply decorator to log a warning message.\n\n    If a message is passed, this decorator adds a warning log of the form\n    function_name: message\n\n    Parameters\n    ----------\n    message\n        The message to be logged along with the function name.\n\n    Notes\n    -----\n    Explainer on complex decorators (and template for decorator structure):\n    https://realpython.com/primer-on-python-decorators/#both-please-but-never-mind-the-bread\n\n    Usage\n    -----\n    To use decorator to log a warning:\n    ```python\n    >>> @_add_warning_message_to_function(message='here be dragons...')\n    >>> def my_func(some_args, some_other_args):\n    >>>    ...\n    >>>\n    >>> some_output = my_func(...)\n\n    Warning my_func: here be dragons...\n    ```\n    \"\"\"  # noqa: E501\n\n    def decorator_function(func):\n        @functools.wraps(func)\n        def wrapper_decorator(*args, **kwargs):\n            # Define the name of the function being decorated for use in logs.\n            func_name = func.__name__\n            if message:\n                logger.warning(f\"{func_name}: {message}\")\n\n            # Run the decorated function in its normal way.\n            output = func(*args, **kwargs)\n\n            return output\n\n        return wrapper_decorator\n\n    if _func is None:\n        return decorator_function\n    else:\n        return decorator_function(_func)",
        "variables": [
            "message",
            "func_name",
            "output",
            "_func"
        ],
        "docstring": "Apply decorator to log a warning message.\n\nIf a message is passed, this decorator adds a warning log of the form\nfunction_name: message\n\nParameters\n----------\nmessage\n    The message to be logged along with the function name.\n\nNotes\n-----\nExplainer on complex decorators (and template for decorator structure):\nhttps://realpython.com/primer-on-python-decorators/#both-please-but-never-mind-the-bread\n\nUsage\n-----\nTo use decorator to log a warning:\n```python\n>>> @_add_warning_message_to_function(message='here be dragons...')\n>>> def my_func(some_args, some_other_args):\n>>>    ...\n>>>\n>>> some_output = my_func(...)\n\nWarning my_func: here be dragons...\n```"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\logging.py",
        "function_name": "decorator_function",
        "code_chunk": "def decorator_function(func):\n        @functools.wraps(func)\n        def wrapper_decorator(*args, **kwargs):\n            # Define the name of the function being decorated for use in logs.\n            func_name = func.__name__\n\n            if log_schema_on_input:\n                if not kwargs.get(\"df\"):\n                    logger.warning(\n                        dedent(\n                            f\"\"\"\n                    Cannot find `df` in keyword named arguments.\n\n                    To use the log_spark_df_schema decorator with the function\n                    {func_name}:\n                    * it must have a parameter called df that is a spark dataframe.\n                    * it must be called specifying the argument names e.g.\n                    {func_name}(df=input_df, ... )\n                    \"\"\",\n                        ),\n                    )  # noqa: E501\n                elif isinstance(kwargs[\"df\"], SparkDF):\n                    schema = kwargs[\"df\"]._jdf.schema().treeString()\n                    logger.info(\n                        f\"Schema of dataframe before {func_name}:\\n{schema}\",\n                    )  # noqa: E501\n                else:\n                    logger.warning(\n                        dedent(\n                            f\"\"\"\n                    {func_name} keyword argument `df` has type {type(kwargs['df'])}.\n\n                    Cannot print spark schema for this type of object.\n                    \"\"\",\n                        ),\n                    )  # noqa: E501\n\n            else:\n                logger.info(\n                    f\"Not printing schema of dataframe before {func_name}\",\n                )  # noqa: E501\n\n            # Run the decorated function in its normal way, but catch its\n            # output so its schema can be printed.\n            df_return = func(*args, **kwargs)\n\n            # Check to ensure that the function returns a single value that is\n            # a spark dataframe, as otherwise the print schema operation will\n            # fail.\n            if isinstance(df_return, SparkDF):\n                schema = df_return._jdf.schema().treeString()\n                logger.info(\n                    f\"Schema of dataframe after {func_name}:\\n{schema}\",\n                )  # noqa: E501\n            else:\n                logger.warning(\n                    f\"{func_name} should return a spark dataframe for decorator, \"\n                    f\"but returned {type(df_return)}\",\n                )\n\n            return df_return\n\n        return wrapper_decorator",
        "variables": [
            "func_name",
            "func",
            "df_return",
            "schema"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\logging.py",
        "function_name": "wrapper_decorator",
        "code_chunk": "def wrapper_decorator(*args, **kwargs):\n        # Define the name of the function being decorated for use in logs.\n        func_name = func.__name__\n\n        if not kwargs.get(\"df\"):\n            logger.warning(\n                dedent(\n                    f\"\"\"\n            Cannot find `df` in keyword named arguments.\n\n            To use the log_rows_in_spark_df decorator with the function\n            {func_name}:\n            * it must have a parameter called df that is a spark dataframe.\n            * it must be called specifying the argument names e.g.\n            {func_name}(df=input_df, ... )\n            \"\"\",\n                ),\n            )\n        elif isinstance(kwargs[\"df\"], SparkDF):\n            # If not already cached, cache the dataframe prior to counting to\n            # allow more efficient processing in function. Is unpersisted at\n            # end of decorator.\n            if not kwargs[\"df\"].is_cached:\n                kwargs[\"df\"].cache()\n\n            logger.info(\n                f\"Rows in dataframe before {func_name} : {kwargs['df'].count()}\",\n            )  # noqa: E501\n        else:\n            logger.warning(\n                dedent(\n                    f\"\"\"\n            {func_name} keyword argument `df` has type {type(kwargs['df'])}.\n\n            Cannot count rows for this type of object.\n            \"\"\",\n                ),\n            )\n\n        # Run the decorated function in its normal way, but catch its output\n        # so it can be counted.\n        df_return = func(*args, **kwargs)\n\n        # Check to ensure that the function returns a single value that is a\n        # spark dataframe, as otherwise the count operation will fail.\n        if isinstance(df_return, SparkDF):\n            # Persist the dataframe to be returned prior to counting to allow\n            # more efficient processing downstream. We persist here as we will\n            # not be removing this dataframe from the in-memory cache in this\n            # decorator. Therefore, we don't want it to get pushed onto disk\n            # (and incur an expensive swap operation).\n            df_return.persist(StorageLevel.MEMORY_ONLY)\n            logger.info(\n                f\"Rows in dataframe after {func_name}  : {df_return.count()}\",\n            )  # noqa: E501\n\n        else:\n            logger.warning(\n                f\"{func_name} should return a spark dataframe for decorator, \"\n                f\"but returned {type(df_return)}\",\n            )\n\n        if kwargs.get(\"df\"):\n            # Unpersist the cached input df to manage memory.\n            kwargs[\"df\"].unpersist()\n\n        return df_return",
        "variables": [
            "func_name",
            "args",
            "kwargs",
            "df_return"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\logging.py",
        "function_name": "decorator_function",
        "code_chunk": "def decorator_function(func):\n        @functools.wraps(func)\n        def wrapper_decorator(*args, **kwargs):\n            # Define the name of the function being decorated for use in logs.\n            func_name = func.__name__\n            if message:\n                logger.warning(f\"{func_name}: {message}\")\n\n            # Run the decorated function in its normal way.\n            output = func(*args, **kwargs)\n\n            return output\n\n        return wrapper_decorator",
        "variables": [
            "func_name",
            "func",
            "output"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\logging.py",
        "function_name": "wrapper_decorator",
        "code_chunk": "def wrapper_decorator(*args, **kwargs):\n            # Define the name of the function being decorated for use in logs.\n            func_name = func.__name__\n\n            if log_schema_on_input:\n                if not kwargs.get(\"df\"):\n                    logger.warning(\n                        dedent(\n                            f\"\"\"\n                    Cannot find `df` in keyword named arguments.\n\n                    To use the log_spark_df_schema decorator with the function\n                    {func_name}:\n                    * it must have a parameter called df that is a spark dataframe.\n                    * it must be called specifying the argument names e.g.\n                    {func_name}(df=input_df, ... )\n                    \"\"\",\n                        ),\n                    )  # noqa: E501\n                elif isinstance(kwargs[\"df\"], SparkDF):\n                    schema = kwargs[\"df\"]._jdf.schema().treeString()\n                    logger.info(\n                        f\"Schema of dataframe before {func_name}:\\n{schema}\",\n                    )  # noqa: E501\n                else:\n                    logger.warning(\n                        dedent(\n                            f\"\"\"\n                    {func_name} keyword argument `df` has type {type(kwargs['df'])}.\n\n                    Cannot print spark schema for this type of object.\n                    \"\"\",\n                        ),\n                    )  # noqa: E501\n\n            else:\n                logger.info(\n                    f\"Not printing schema of dataframe before {func_name}\",\n                )  # noqa: E501\n\n            # Run the decorated function in its normal way, but catch its\n            # output so its schema can be printed.\n            df_return = func(*args, **kwargs)\n\n            # Check to ensure that the function returns a single value that is\n            # a spark dataframe, as otherwise the print schema operation will\n            # fail.\n            if isinstance(df_return, SparkDF):\n                schema = df_return._jdf.schema().treeString()\n                logger.info(\n                    f\"Schema of dataframe after {func_name}:\\n{schema}\",\n                )  # noqa: E501\n            else:\n                logger.warning(\n                    f\"{func_name} should return a spark dataframe for decorator, \"\n                    f\"but returned {type(df_return)}\",\n                )\n\n            return df_return",
        "variables": [
            "df_return",
            "func_name",
            "schema",
            "args",
            "kwargs"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\logging.py",
        "function_name": "wrapper_decorator",
        "code_chunk": "def wrapper_decorator(*args, **kwargs):\n            # Define the name of the function being decorated for use in logs.\n            func_name = func.__name__\n            if message:\n                logger.warning(f\"{func_name}: {message}\")\n\n            # Run the decorated function in its normal way.\n            output = func(*args, **kwargs)\n\n            return output",
        "variables": [
            "func_name",
            "args",
            "kwargs",
            "output"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\test_utils.py",
        "function_name": "suppress_py4j_logging",
        "code_chunk": "def suppress_py4j_logging():\n    \"\"\"Suppress spark logging.\"\"\"\n    logger = logging.getLogger(\"py4j\")\n    logger.setLevel(logging.WARN)",
        "variables": [
            "logger"
        ],
        "docstring": "Suppress spark logging."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\test_utils.py",
        "function_name": "spark_session",
        "code_chunk": "def spark_session():\n    \"\"\"Set up spark session fixture.\"\"\"\n    suppress_py4j_logging()\n\n    spark = (\n        SparkSession.builder.master(\"local[2]\")\n        .appName(\"rdsa_test_context\")\n        .config(\"spark.sql.shuffle.partitions\", 1)\n        # This stops progress bars appearing in the console whilst running\n        .config(\"spark.ui.showConsoleProgress\", \"false\")\n        # .config('spark.sql.execution.arrow.enabled', 'true')\n        .config(\"spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT\", 1)\n        .config(\"spark.workerEnv.ARROW_PRE_0_15_IPC_FORMAT\", 1)\n        .enableHiveSupport()\n        .getOrCreate()\n    )\n    yield spark\n    spark.stop()",
        "variables": [
            "spark"
        ],
        "docstring": "Set up spark session fixture."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\test_utils.py",
        "function_name": "parametrize_cases",
        "code_chunk": "def parametrize_cases(*cases: Case):\n    \"\"\"More user friendly parameterize cases testing.\n\n    Utilise as a decorator on top of test function.\n\n    Examples\n    --------\n    ```python\n    @parameterize_cases(\n        Case(\n            label=\"some test name\",\n            foo=10,\n            bar=\"some value\"\n        ),\n        Case(\n            label=\"some test name #2\",\n            foo=20,\n            bar=\"some other value\"\n        ),\n    )\n    def test(foo, bar):\n        ...\n    ```\n\n    See Also\n    --------\n    Source: https://github.com/ckp95/pytest-parametrize-cases\n    \"\"\"\n    all_args = set()\n    for case in cases:\n        if not isinstance(case, Case):\n            msg = f\"{case!r} is not an instance of Case\"\n            raise TypeError(msg)\n\n        all_args.update(case.kwargs.keys())\n\n    argument_string = \",\".join(sorted(all_args))\n\n    case_list = []\n    ids_list = []\n    for case in cases:\n        case_kwargs = case.kwargs.copy()\n        args = case.kwargs.keys()\n\n        # Make sure all keys are in each case, otherwise initialise with None.\n        diff = {k: None for k in set(all_args) - set(args)}\n        case_kwargs.update(diff)\n\n        case_tuple = tuple(value for key, value in sorted(case_kwargs.items()))\n\n        # If marks are given, wrap the case tuple.\n        if case.marks:\n            case_tuple = pytest.param(*case_tuple, marks=case.marks)\n\n        case_list.append(case_tuple)\n        ids_list.append(case.label)\n\n    if len(all_args) == 1:\n        # otherwise it gets passed to the test function as a singleton tuple\n        case_list = [i[0] for i in case_list]\n\n    return pytest.mark.parametrize(\n        argnames=argument_string,\n        argvalues=case_list,\n        ids=ids_list,\n    )",
        "variables": [
            "case_tuple",
            "ids_list",
            "case",
            "args",
            "k",
            "cases",
            "value",
            "diff",
            "msg",
            "i",
            "case_kwargs",
            "all_args",
            "key",
            "argument_string",
            "case_list"
        ],
        "docstring": "More user friendly parameterize cases testing.\n\nUtilise as a decorator on top of test function.\n\nExamples\n--------\n```python\n@parameterize_cases(\n    Case(\n        label=\"some test name\",\n        foo=10,\n        bar=\"some value\"\n    ),\n    Case(\n        label=\"some test name #2\",\n        foo=20,\n        bar=\"some other value\"\n    ),\n)\ndef test(foo, bar):\n    ...\n```\n\nSee Also\n--------\nSource: https://github.com/ckp95/pytest-parametrize-cases"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\test_utils.py",
        "function_name": "create_dataframe",
        "code_chunk": "def create_dataframe(data: List[Tuple[str]], **kwargs) -> pd.DataFrame:\n    \"\"\"Create pandas df from tuple data with a header.\"\"\"\n    return pd.DataFrame.from_records(data[1:], columns=data[0], **kwargs)",
        "variables": [
            "data",
            "kwargs"
        ],
        "docstring": "Create pandas df from tuple data with a header."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\test_utils.py",
        "function_name": "to_date",
        "code_chunk": "def to_date(dt: str) -> datetime.date:\n    \"\"\"Convert date string to datetime.date type.\"\"\"\n    return pd.to_datetime(dt).date()",
        "variables": [
            "dt"
        ],
        "docstring": "Convert date string to datetime.date type."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\test_utils.py",
        "function_name": "to_datetime",
        "code_chunk": "def to_datetime(dt: str) -> datetime.datetime:\n    \"\"\"Convert datetime string to datetime.datetime type.\"\"\"\n    return pd.to_datetime(dt).to_pydatetime()",
        "variables": [
            "dt"
        ],
        "docstring": "Convert datetime string to datetime.datetime type."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\test_utils.py",
        "function_name": "create_spark_df",
        "code_chunk": "def create_spark_df(spark_session):\n    \"\"\"Create Spark DataFrame from tuple data with first row as schema.\n\n    Example:\n    -------\n    create_spark_df([\n        ('column1', 'column2', 'column3'),\n        ('aaaa', 1, 1.1)\n    ])\n\n    Can specify the schema alongside the column names:\n    create_spark_df([\n        ('column1 STRING, column2 INT, column3 DOUBLE'),\n        ('aaaa', 1, 1.1)\n    ])\n    \"\"\"\n\n    def _(data):\n        return spark_session.createDataFrame(data[1:], schema=data[0])\n\n    return _",
        "variables": [
            "spark_session"
        ],
        "docstring": "Create Spark DataFrame from tuple data with first row as schema.\n\nExample:\n-------\ncreate_spark_df([\n    ('column1', 'column2', 'column3'),\n    ('aaaa', 1, 1.1)\n])\n\nCan specify the schema alongside the column names:\ncreate_spark_df([\n    ('column1 STRING, column2 INT, column3 DOUBLE'),\n    ('aaaa', 1, 1.1)\n])"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\test_utils.py",
        "function_name": "to_spark",
        "code_chunk": "def to_spark(spark_session):\n    \"\"\"Convert pandas df to spark.\"\"\"\n\n    def _(df: pd.DataFrame, *args, **kwargs):\n        return spark_session.createDataFrame(df, *args, **kwargs)\n\n    return _",
        "variables": [
            "spark_session"
        ],
        "docstring": "Convert pandas df to spark."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\test_utils.py",
        "function_name": "__init__",
        "code_chunk": "def __init__(\n        self,\n        label: Optional[str] = None,\n        marks: Optional[MarkDecorator] = None,\n        **kwargs,\n    ):\n        \"\"\"Initialise objects.\"\"\"\n        self.label = label\n        self.kwargs = kwargs\n        self.marks = marks\n        # Makes kwargs accessible with dot notation.\n        self.__dict__.update(kwargs)",
        "variables": [
            "self",
            "marks",
            "kwargs",
            "label"
        ],
        "docstring": "Initialise objects."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\test_utils.py",
        "function_name": "__repr__",
        "code_chunk": "def __repr__(self) -> str:\n        \"\"\"Return string.\"\"\"\n        return f\"Case({self.label!r}, **{self.kwargs!r})\"",
        "variables": [
            "self"
        ],
        "docstring": "Return string."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\test_utils.py",
        "function_name": "_",
        "code_chunk": "def _(data):\n        return spark_session.createDataFrame(data[1:], schema=data[0])",
        "variables": [
            "data"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\test_utils.py",
        "function_name": "_",
        "code_chunk": "def _(df: pd.DataFrame, *args, **kwargs):\n        return spark_session.createDataFrame(df, *args, **kwargs)",
        "variables": [
            "args",
            "kwargs",
            "df"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\validation.py",
        "function_name": "apply_validation",
        "code_chunk": "def apply_validation(\n    config: Mapping[str, Any],\n    Validator: Optional[BaseModel],  # noqa: N803\n) -> Mapping[str, Any]:\n    \"\"\"Apply validation model to config.\n\n    If no Validator is passed, then a warning will be logged and the input\n    config returned without validation. This mechanism is to allow the use of\n    this function to aid in tracking config sections that are unvalidated.\n\n    Parameters\n    ----------\n    config\n        The config for validating.\n    Validator, optional\n        Validator class for the config.\n\n    Returns\n    -------\n    Mapping[str, Any]\n        The input config after being passed through the validator.\n    \"\"\"\n    if not Validator:\n        msg = \"No validator provided, config contents unvalidated.\"\n        logger.warning(msg)\n        warnings.warn(msg, stacklevel=2)\n        return config\n\n    validated_config = Validator(**config).model_dump(exclude_unset=True)\n\n    logger.info(\n        f\"\"\"Validated config using {Validator.__name__}:\n    {json.dumps(validated_config, indent=4)}\n    \"\"\",\n    )\n    return validated_config",
        "variables": [
            "msg",
            "validated_config",
            "Validator",
            "config"
        ],
        "docstring": "Apply validation model to config.\n\nIf no Validator is passed, then a warning will be logged and the input\nconfig returned without validation. This mechanism is to allow the use of\nthis function to aid in tracking config sections that are unvalidated.\n\nParameters\n----------\nconfig\n    The config for validating.\nValidator, optional\n    Validator class for the config.\n\nReturns\n-------\nMapping[str, Any]\n    The input config after being passed through the validator."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\validation.py",
        "function_name": "list_convert_validator",
        "code_chunk": "def list_convert_validator(*args, **kwargs) -> Callable:  # noqa: ANN002, ANN003\n    \"\"\"Wrapper to set kwargs for list_convert validator.\"\"\"  # noqa: D401\n    decorator = validator(\n        *args,\n        **kwargs,\n        pre=True,  # Run before any other validation.\n        always=True,  # Apply even when value is not specified (i.e. None).\n        allow_reuse=True,  # Allow validator to be used in many fields/models.\n    )\n    decorated = decorator(list_convert)\n    return decorated",
        "variables": [
            "decorated",
            "args",
            "kwargs",
            "decorator"
        ],
        "docstring": "Wrapper to set kwargs for list_convert validator."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\validation.py",
        "function_name": "allowed_date_format",
        "code_chunk": "def allowed_date_format(date: str) -> str:\n    \"\"\"Ensure that the date string can be converted to a useable datetime.\n\n    Parameters\n    ----------\n    date\n        The specified date string.\n\n    Returns\n    -------\n    str\n        The input date.\n\n    Raises\n    ------\n    ValueError\n        If the date is not one of the predefined allowed formats.\n    \"\"\"\n    pd.to_datetime(date)\n\n    return date",
        "variables": [
            "date"
        ],
        "docstring": "Ensure that the date string can be converted to a useable datetime.\n\nParameters\n----------\ndate\n    The specified date string.\n\nReturns\n-------\nstr\n    The input date.\n\nRaises\n------\nValueError\n    If the date is not one of the predefined allowed formats."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "_perform",
        "code_chunk": "def _perform(command: List[str]) -> bool:\n    \"\"\"Execute a command via subprocess, capturing stdout and stderr.\n\n    This function creates a subprocess with the provided command list, then\n    communicates with it to retrieve the stdout and stderr. After the\n    command execution, it checks the process's return code to determine success\n    or failure. A zero return code indicates success.\n\n    Parameters\n    ----------\n    command\n        A list of command elements representing the system\n        command to be executed. For example, ['ls', '-l', '/home/user'].\n\n    Returns\n    -------\n    bool\n        True if the command execution is successful (return code 0),\n        otherwise False.\n\n    Raises\n    ------\n    subprocess.TimeoutExpired\n        If the process does not complete within the default timeout.\n    \"\"\"\n    try:\n        process = subprocess.Popen(\n            command,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        stdout, stderr = process.communicate(\n            timeout=15,\n        )  # added a timeout for robustness\n    except subprocess.TimeoutExpired:\n        process.kill()\n        stdout, stderr = process.communicate()\n    return process.returncode == 0",
        "variables": [
            "command",
            "stdout",
            "stderr",
            "process"
        ],
        "docstring": "Execute a command via subprocess, capturing stdout and stderr.\n\nThis function creates a subprocess with the provided command list, then\ncommunicates with it to retrieve the stdout and stderr. After the\ncommand execution, it checks the process's return code to determine success\nor failure. A zero return code indicates success.\n\nParameters\n----------\ncommand\n    A list of command elements representing the system\n    command to be executed. For example, ['ls', '-l', '/home/user'].\n\nReturns\n-------\nbool\n    True if the command execution is successful (return code 0),\n    otherwise False.\n\nRaises\n------\nsubprocess.TimeoutExpired\n    If the process does not complete within the default timeout."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "change_permissions",
        "code_chunk": "def change_permissions(\n    path: str,\n    permission: str,\n    recursive: bool = False,\n) -> bool:\n    \"\"\"Change directory and file permissions in HDFS.\n\n    Parameters\n    ----------\n    path\n        The path to the file or directory in HDFS.\n    permission\n        The permission to be set, e.g., 'go+rwx' or '777'.\n    recursive\n        If True, changes permissions for all subdirectories and\n        files within a directory.\n\n    Returns\n    -------\n    bool\n        True if the operation was successful (command return code 0),\n        otherwise False.\n    \"\"\"\n    command = [\"hadoop\", \"fs\", \"-chmod\"]\n    if recursive:\n        command.append(\"-R\")\n    command.extend([permission, path])\n    return _perform(command)",
        "variables": [
            "command",
            "path",
            "permission",
            "recursive"
        ],
        "docstring": "Change directory and file permissions in HDFS.\n\nParameters\n----------\npath\n    The path to the file or directory in HDFS.\npermission\n    The permission to be set, e.g., 'go+rwx' or '777'.\nrecursive\n    If True, changes permissions for all subdirectories and\n    files within a directory.\n\nReturns\n-------\nbool\n    True if the operation was successful (command return code 0),\n    otherwise False."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "copy",
        "code_chunk": "def copy(from_path: str, to_path: str, overwrite: bool = False) -> bool:\n    \"\"\"Copy a file in HDFS.\n\n    Parameters\n    ----------\n    from_path\n        The source path of the file in HDFS.\n    to_path\n        The target path of the file in HDFS.\n    overwrite\n        If True, the existing file at the target path will be overwritten,\n        default is False.\n\n    Returns\n    -------\n    bool\n        True if the operation was successful (command return code 0),\n        otherwise False.\n\n    Raises\n    ------\n    subprocess.TimeoutExpired\n        If the process does not complete within the default timeout.\n    \"\"\"\n    command = [\"hadoop\", \"fs\", \"-cp\"]\n    if overwrite:\n        command.append(\"-f\")\n    command.extend([from_path, to_path])\n    return _perform(command)",
        "variables": [
            "command",
            "overwrite",
            "from_path",
            "to_path"
        ],
        "docstring": "Copy a file in HDFS.\n\nParameters\n----------\nfrom_path\n    The source path of the file in HDFS.\nto_path\n    The target path of the file in HDFS.\noverwrite\n    If True, the existing file at the target path will be overwritten,\n    default is False.\n\nReturns\n-------\nbool\n    True if the operation was successful (command return code 0),\n    otherwise False.\n\nRaises\n------\nsubprocess.TimeoutExpired\n    If the process does not complete within the default timeout."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "copy_local_to_hdfs",
        "code_chunk": "def copy_local_to_hdfs(from_path: str, to_path: str) -> bool:\n    \"\"\"Copy a local file to HDFS.\n\n    Parameters\n    ----------\n    from_path\n        The path to the local file.\n    to_path\n        The path to the HDFS directory where the file will be copied.\n\n    Returns\n    -------\n    bool\n        True if the operation was successful (command return code 0),\n        otherwise False.\n    \"\"\"\n    command = [\"hadoop\", \"fs\", \"-copyFromLocal\", from_path, to_path]\n    return _perform(command)",
        "variables": [
            "command",
            "from_path",
            "to_path"
        ],
        "docstring": "Copy a local file to HDFS.\n\nParameters\n----------\nfrom_path\n    The path to the local file.\nto_path\n    The path to the HDFS directory where the file will be copied.\n\nReturns\n-------\nbool\n    True if the operation was successful (command return code 0),\n    otherwise False."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "create_dir",
        "code_chunk": "def create_dir(path: str) -> bool:\n    \"\"\"Create a directory in HDFS.\n\n    Parameters\n    ----------\n    path\n        The HDFS path where the directory should be created.\n\n    Returns\n    -------\n    bool\n        True if the operation is successful (directory created),\n        otherwise False.\n    \"\"\"\n    command = [\"hadoop\", \"fs\", \"-mkdir\", path]\n    return _perform(command)",
        "variables": [
            "command",
            "path"
        ],
        "docstring": "Create a directory in HDFS.\n\nParameters\n----------\npath\n    The HDFS path where the directory should be created.\n\nReturns\n-------\nbool\n    True if the operation is successful (directory created),\n    otherwise False."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "create_txt_from_string",
        "code_chunk": "def create_txt_from_string(\n    path: str,\n    string_to_write: str,\n    replace: Optional[bool] = False,\n) -> None:\n    \"\"\"Create and populate a text file in HDFS.\n\n    Parameters\n    ----------\n    path\n        The path to the new file to be created, for example,\n        '/some/directory/newfile.txt'.\n    string_to_write\n        The string that will populate the new text file.\n    replace\n        Flag determining whether an existing file should be replaced.\n        Defaults to False.\n\n    Returns\n    -------\n    None\n        This function doesn't return anything; it's used for its side effect\n        of creating a text file.\n\n    Raises\n    ------\n    FileNotFoundError\n        If `replace` is False and the file already exists.\n    \"\"\"\n    if replace and file_exists(path):\n        delete_file(path)\n    elif not replace and file_exists(path):\n        msg = f\"File {path} already exists and replace is set to False.\"\n        raise FileNotFoundError(\n            msg,\n        )\n\n    subprocess.call(\n        [f'echo \"{string_to_write}\" | hadoop fs -put - {path}'],\n        shell=True,\n    )",
        "variables": [
            "string_to_write",
            "replace",
            "path",
            "msg"
        ],
        "docstring": "Create and populate a text file in HDFS.\n\nParameters\n----------\npath\n    The path to the new file to be created, for example,\n    '/some/directory/newfile.txt'.\nstring_to_write\n    The string that will populate the new text file.\nreplace\n    Flag determining whether an existing file should be replaced.\n    Defaults to False.\n\nReturns\n-------\nNone\n    This function doesn't return anything; it's used for its side effect\n    of creating a text file.\n\nRaises\n------\nFileNotFoundError\n    If `replace` is False and the file already exists."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "delete_dir",
        "code_chunk": "def delete_dir(path: str) -> bool:\n    \"\"\"Delete an empty directory from HDFS.\n\n    This function attempts to delete an empty directory in HDFS.\n    If the directory is not empty, the deletion will fail.\n\n    Parameters\n    ----------\n    path\n        The HDFS path to the directory to be deleted.\n\n    Returns\n    -------\n    bool\n        True if the operation is successful (directory deleted),\n        otherwise False.\n\n    Note\n    ----\n    This function will only succeed if the directory is empty.\n    To delete directories containing files or other directories,\n    consider using `delete_path` instead.\n    \"\"\"\n    command = [\"hadoop\", \"fs\", \"-rmdir\", path]\n    return _perform(command)",
        "variables": [
            "command",
            "path"
        ],
        "docstring": "Delete an empty directory from HDFS.\n\nThis function attempts to delete an empty directory in HDFS.\nIf the directory is not empty, the deletion will fail.\n\nParameters\n----------\npath\n    The HDFS path to the directory to be deleted.\n\nReturns\n-------\nbool\n    True if the operation is successful (directory deleted),\n    otherwise False.\n\nNote\n----\nThis function will only succeed if the directory is empty.\nTo delete directories containing files or other directories,\nconsider using `delete_path` instead."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "delete_file",
        "code_chunk": "def delete_file(path: str) -> bool:\n    \"\"\"Delete a specific file in HDFS.\n\n    This function is used to delete a single file located\n    at the specified HDFS path. If the path points to a\n    directory, the command will fail.\n\n    Parameters\n    ----------\n    path\n        The path to the file in HDFS to be deleted.\n\n    Returns\n    -------\n    bool\n        True if the file was successfully deleted (command return code 0),\n        otherwise False.\n\n    Raises\n    ------\n    subprocess.TimeoutExpired\n        If the process does not complete within the default timeout.\n\n    Note\n    ----\n    This function is intended for files only. For directory deletions,\n    use `delete_dir` or `delete_path`.\n    \"\"\"\n    command = [\"hadoop\", \"fs\", \"-rm\", path]\n    return _perform(command)",
        "variables": [
            "command",
            "path"
        ],
        "docstring": "Delete a specific file in HDFS.\n\nThis function is used to delete a single file located\nat the specified HDFS path. If the path points to a\ndirectory, the command will fail.\n\nParameters\n----------\npath\n    The path to the file in HDFS to be deleted.\n\nReturns\n-------\nbool\n    True if the file was successfully deleted (command return code 0),\n    otherwise False.\n\nRaises\n------\nsubprocess.TimeoutExpired\n    If the process does not complete within the default timeout.\n\nNote\n----\nThis function is intended for files only. For directory deletions,\nuse `delete_dir` or `delete_path`."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "delete_path",
        "code_chunk": "def delete_path(path: str) -> bool:\n    \"\"\"Delete a file or directory in HDFS, including non-empty directories.\n\n    This function is capable of deleting both files and directories.\n    When applied to directories, it will recursively delete all contents\n    within the directory, making it suitable for removing directories regardless\n    of whether they are empty or contain files or other directories.\n\n    Parameters\n    ----------\n    path\n        The path to the file or directory in HDFS to be deleted.\n\n    Returns\n    -------\n    bool\n        True if the file was successfully deleted (command return code 0),\n        otherwise False.\n\n    Raises\n    ------\n    subprocess.TimeoutExpired\n        If the process does not complete within the default timeout.\n\n    Warning\n    -------\n    Use with caution: applying this function to a directory will\n    remove all contained files and subdirectories without confirmation.\n    \"\"\"\n    command = [\"hadoop\", \"fs\", \"-rm\", \"-r\", path]\n    return _perform(command)",
        "variables": [
            "command",
            "path"
        ],
        "docstring": "Delete a file or directory in HDFS, including non-empty directories.\n\nThis function is capable of deleting both files and directories.\nWhen applied to directories, it will recursively delete all contents\nwithin the directory, making it suitable for removing directories regardless\nof whether they are empty or contain files or other directories.\n\nParameters\n----------\npath\n    The path to the file or directory in HDFS to be deleted.\n\nReturns\n-------\nbool\n    True if the file was successfully deleted (command return code 0),\n    otherwise False.\n\nRaises\n------\nsubprocess.TimeoutExpired\n    If the process does not complete within the default timeout.\n\nWarning\n-------\nUse with caution: applying this function to a directory will\nremove all contained files and subdirectories without confirmation."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "file_exists",
        "code_chunk": "def file_exists(path: str) -> bool:\n    \"\"\"Check whether a file exists in HDFS.\n\n    Parameters\n    ----------\n    path\n        The path to the file in HDFS to be checked for existence.\n\n    Returns\n    -------\n    bool\n        True if the file exists (command return code 0), otherwise False.\n\n    Raises\n    ------\n    subprocess.TimeoutExpired\n        If the process does not complete within the default timeout.\n    \"\"\"\n    command = [\"hadoop\", \"fs\", \"-test\", \"-e\", path]\n    return _perform(command)",
        "variables": [
            "command",
            "path"
        ],
        "docstring": "Check whether a file exists in HDFS.\n\nParameters\n----------\npath\n    The path to the file in HDFS to be checked for existence.\n\nReturns\n-------\nbool\n    True if the file exists (command return code 0), otherwise False.\n\nRaises\n------\nsubprocess.TimeoutExpired\n    If the process does not complete within the default timeout."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "get_date_modified",
        "code_chunk": "def get_date_modified(filepath: str) -> str:\n    \"\"\"Return the last modified date of a file in HDFS.\n\n    Parameters\n    ----------\n    filepath\n        The path to the file in HDFS.\n\n    Returns\n    -------\n    str\n        The date the file was last modified.\n    \"\"\"\n    command = subprocess.Popen(\n        f\"hadoop fs -stat %y {filepath}\",\n        stdout=subprocess.PIPE,\n        shell=True,\n    )\n    return command.stdout.read().decode(\"utf-8\")[0:10]",
        "variables": [
            "command",
            "filepath"
        ],
        "docstring": "Return the last modified date of a file in HDFS.\n\nParameters\n----------\nfilepath\n    The path to the file in HDFS.\n\nReturns\n-------\nstr\n    The date the file was last modified."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "is_dir",
        "code_chunk": "def is_dir(path: str) -> bool:\n    \"\"\"Test if a directory exists in HDFS.\n\n    Parameters\n    ----------\n    path\n        The HDFS path to the directory to be tested.\n\n    Returns\n    -------\n    bool\n        True if the operation is successful (directory exists), otherwise False.\n    \"\"\"\n    command = [\"hadoop\", \"fs\", \"-test\", \"-d\", path]\n    return _perform(command)",
        "variables": [
            "command",
            "path"
        ],
        "docstring": "Test if a directory exists in HDFS.\n\nParameters\n----------\npath\n    The HDFS path to the directory to be tested.\n\nReturns\n-------\nbool\n    True if the operation is successful (directory exists), otherwise False."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "move_local_to_hdfs",
        "code_chunk": "def move_local_to_hdfs(from_path: str, to_path: str) -> bool:\n    \"\"\"Move a local file to HDFS.\n\n    Parameters\n    ----------\n    from_path\n        The path to the local file.\n    to_path\n        The path to the HDFS directory where the file will be moved.\n\n    Returns\n    -------\n    bool\n        True if the operation was successful (command return code 0),\n        otherwise False.\n    \"\"\"\n    command = [\"hadoop\", \"fs\", \"-moveFromLocal\", from_path, to_path]\n    return _perform(command)",
        "variables": [
            "command",
            "from_path",
            "to_path"
        ],
        "docstring": "Move a local file to HDFS.\n\nParameters\n----------\nfrom_path\n    The path to the local file.\nto_path\n    The path to the HDFS directory where the file will be moved.\n\nReturns\n-------\nbool\n    True if the operation was successful (command return code 0),\n    otherwise False."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "read_dir",
        "code_chunk": "def read_dir(path: str) -> List[str]:\n    \"\"\"Read the contents of a directory in HDFS.\n\n    Parameters\n    ----------\n    path\n        The path to the directory in HDFS.\n\n    Returns\n    -------\n    List[str]\n        A list of full paths of the items found in the directory.\n    \"\"\"\n    ls = subprocess.Popen([\"hadoop\", \"fs\", \"-ls\", path], stdout=subprocess.PIPE)\n    files = [\n        line.decode(\"utf-8\").split()[-1]\n        for line in ls.stdout\n        if \"Found\" not in line.decode(\"utf-8\")\n    ]\n    return files",
        "variables": [
            "files",
            "ls",
            "path",
            "line"
        ],
        "docstring": "Read the contents of a directory in HDFS.\n\nParameters\n----------\npath\n    The path to the directory in HDFS.\n\nReturns\n-------\nList[str]\n    A list of full paths of the items found in the directory."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "read_dir_files",
        "code_chunk": "def read_dir_files(path: str) -> List[str]:\n    \"\"\"Read the filenames in a directory in HDFS.\n\n    Parameters\n    ----------\n    path\n        The path to the directory in HDFS.\n\n    Returns\n    -------\n    List[str]\n        A list of filenames in the directory.\n    \"\"\"\n    return [Path(p).name for p in read_dir(path)]",
        "variables": [
            "p",
            "path"
        ],
        "docstring": "Read the filenames in a directory in HDFS.\n\nParameters\n----------\npath\n    The path to the directory in HDFS.\n\nReturns\n-------\nList[str]\n    A list of filenames in the directory."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "read_dir_files_recursive",
        "code_chunk": "def read_dir_files_recursive(path: str, return_path: bool = True) -> List[str]:\n    \"\"\"Recursively reads the contents of a directory in HDFS.\n\n    Parameters\n    ----------\n    path\n        The path to the directory in HDFS.\n    return_path\n        If True, returns the full path of the files, otherwise\n        just the filename.\n\n    Returns\n    -------\n    List[str]\n        A list of files in the directory.\n    \"\"\"\n    command = subprocess.Popen(\n        f\"hadoop fs -ls -R {path} | grep -v ^d | tr -s ' ' | cut -d ' ' -f 8-\",\n        stdout=subprocess.PIPE,\n        shell=True,\n    )\n    object_list = [obj.decode(\"utf-8\") for obj in command.stdout.read().splitlines()]\n\n    if not return_path:\n        return [Path(path).name for path in object_list]\n\n    else:\n        return object_list",
        "variables": [
            "return_path",
            "obj",
            "command",
            "path",
            "object_list"
        ],
        "docstring": "Recursively reads the contents of a directory in HDFS.\n\nParameters\n----------\npath\n    The path to the directory in HDFS.\nreturn_path\n    If True, returns the full path of the files, otherwise\n    just the filename.\n\nReturns\n-------\nList[str]\n    A list of files in the directory."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\hdfs_utils.py",
        "function_name": "rename",
        "code_chunk": "def rename(from_path: str, to_path: str, overwrite: bool = False) -> bool:\n    \"\"\"Rename (i.e., move using full path) a file in HDFS.\n\n    Parameters\n    ----------\n    from_path\n        The source path of the file in HDFS.\n    to_path\n        The target path of the file in HDFS.\n    overwrite\n        If True, the existing file at the target path will be overwritten,\n        default is False.\n\n    Returns\n    -------\n    bool\n        True if the operation was successful (command return code 0),\n        otherwise False.\n\n    Raises\n    ------\n    subprocess.TimeoutExpired\n        If the process does not complete within the default timeout.\n    \"\"\"\n    if overwrite:\n        delete_file(to_path)\n\n    command = [\"hadoop\", \"fs\", \"-mv\", from_path, to_path]\n    return _perform(command)",
        "variables": [
            "command",
            "overwrite",
            "from_path",
            "to_path"
        ],
        "docstring": "Rename (i.e., move using full path) a file in HDFS.\n\nParameters\n----------\nfrom_path\n    The source path of the file in HDFS.\nto_path\n    The target path of the file in HDFS.\noverwrite\n    If True, the existing file at the target path will be overwritten,\n    default is False.\n\nReturns\n-------\nbool\n    True if the operation was successful (command return code 0),\n    otherwise False.\n\nRaises\n------\nsubprocess.TimeoutExpired\n    If the process does not complete within the default timeout."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\impala.py",
        "function_name": "invalidate_impala_metadata",
        "code_chunk": "def invalidate_impala_metadata(\n    table: str,\n    impalad_address_port: str,\n    impalad_ca_cert: str,\n    keep_stderr: Optional[bool] = False,\n):\n    \"\"\"Automate the invalidation of a table's metadata using impala-shell.\n\n    This function uses the impala-shell command with the given\n    impalad_address_port and impalad_ca_cert, to invalidate a specified\n    table's metadata.\n\n    It proves useful during a data pipeline's execution after writing to an\n    intermediate Hive table. Using Impala Query Editor in Hue, end-users often\n    need to run \"INVALIDATE METADATA\" command to refresh a table's metadata.\n    However, this manual step can be missed, leading to potential use of\n    outdated metadata.\n\n    The function automates the \"INVALIDATE METADATA\" command for a given table,\n    ensuring up-to-date metadata for future queries. This reduces manual\n    intervention, making outdated metadata issues less likely to occur.\n\n    Parameters\n    ----------\n    table\n        Name of the table for metadata invalidation.\n    impalad_address_port\n        'address:port' of the impalad instance.\n    impalad_ca_cert\n        Path to impalad's CA certificate file.\n    keep_stderr\n        If True, will print impala-shell command's stderr output.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    >>> invalidate_impala_metadata(\n    ...     'my_table',\n    ...     'localhost:21050',\n    ...     '/path/to/ca_cert.pem'\n    ... )\n    >>> invalidate_impala_metadata(\n    ...     'my_table',\n    ...     'localhost:21050',\n    ...     '/path/to/ca_cert.pem',\n    ...     keep_stderr=True\n    ... )\n    \"\"\"\n    result = subprocess.run(\n        [\n            \"impala-shell\",\n            \"-k\",\n            \"--ssl\",\n            \"-i\",\n            impalad_address_port,\n            \"--ca_cert\",\n            impalad_ca_cert,\n            \"-q\",\n            f\"invalidate metadata {table};\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n\n    if keep_stderr:\n        logger.info(result.stderr.decode())",
        "variables": [
            "keep_stderr",
            "result",
            "table",
            "impalad_ca_cert",
            "impalad_address_port"
        ],
        "docstring": "Automate the invalidation of a table's metadata using impala-shell.\n\nThis function uses the impala-shell command with the given\nimpalad_address_port and impalad_ca_cert, to invalidate a specified\ntable's metadata.\n\nIt proves useful during a data pipeline's execution after writing to an\nintermediate Hive table. Using Impala Query Editor in Hue, end-users often\nneed to run \"INVALIDATE METADATA\" command to refresh a table's metadata.\nHowever, this manual step can be missed, leading to potential use of\noutdated metadata.\n\nThe function automates the \"INVALIDATE METADATA\" command for a given table,\nensuring up-to-date metadata for future queries. This reduces manual\nintervention, making outdated metadata issues less likely to occur.\n\nParameters\n----------\ntable\n    Name of the table for metadata invalidation.\nimpalad_address_port\n    'address:port' of the impalad instance.\nimpalad_ca_cert\n    Path to impalad's CA certificate file.\nkeep_stderr\n    If True, will print impala-shell command's stderr output.\n\nReturns\n-------\nNone\n\nExamples\n--------\n>>> invalidate_impala_metadata(\n...     'my_table',\n...     'localhost:21050',\n...     '/path/to/ca_cert.pem'\n... )\n>>> invalidate_impala_metadata(\n...     'my_table',\n...     'localhost:21050',\n...     '/path/to/ca_cert.pem',\n...     keep_stderr=True\n... )"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "remove_leading_slash",
        "code_chunk": "def remove_leading_slash(text: str) -> str:\n    \"\"\"Remove the leading forward slash from a string if present.\n\n    Parameters\n    ----------\n    text\n        The text from which the leading slash will be removed.\n\n    Returns\n    -------\n    str\n        The text stripped of its leading slash.\n\n    Examples\n    --------\n    >>> remove_leading_slash('/example/path')\n    'example/path'\n    \"\"\"\n    return text.lstrip(\"/\")",
        "variables": [
            "text"
        ],
        "docstring": "Remove the leading forward slash from a string if present.\n\nParameters\n----------\ntext\n    The text from which the leading slash will be removed.\n\nReturns\n-------\nstr\n    The text stripped of its leading slash.\n\nExamples\n--------\n>>> remove_leading_slash('/example/path')\n'example/path'"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "validate_bucket_name",
        "code_chunk": "def validate_bucket_name(bucket_name: str) -> str:\n    \"\"\"Validate the format of an AWS S3 bucket name according to AWS rules.\n\n    Parameters\n    ----------\n    bucket_name\n        The name of the bucket to validate.\n\n    Returns\n    -------\n    str\n        The validated bucket name if valid.\n\n    Raises\n    ------\n    InvalidBucketNameError\n        If the bucket name does not meet AWS specifications.\n\n    Examples\n    --------\n    >>> validate_bucket_name('valid-bucket-name')\n    'valid-bucket-name'\n\n    >>> validate_bucket_name('Invalid_Bucket_Name')\n    InvalidBucketNameError: Bucket name must not contain underscores.\n    \"\"\"\n    # Bucket name must be between 3 and 63 characters long\n    if len(bucket_name) < 3 or len(bucket_name) > 63:\n        error_msg = \"Bucket name must be between 3 and 63 characters long.\"\n        raise InvalidBucketNameError(error_msg)\n\n    # Bucket name must not contain uppercase letters\n    if bucket_name != bucket_name.lower():\n        error_msg = \"Bucket name must not contain uppercase letters.\"\n        raise InvalidBucketNameError(error_msg)\n\n    # Bucket name must not contain underscores\n    if \"_\" in bucket_name:\n        error_msg = \"Bucket name must not contain underscores.\"\n        raise InvalidBucketNameError(error_msg)\n\n    # Bucket name must start and end with a lowercase letter or number\n    if not bucket_name[0].isalnum() or not bucket_name[-1].isalnum():\n        error_msg = \"Bucket name must start and end with a lowercase letter or number.\"\n        raise InvalidBucketNameError(error_msg)\n\n    # Bucket name must not contain forward slashes\n    if \"/\" in bucket_name:\n        error_msg = \"Bucket name must not contain forward slashes.\"\n        raise InvalidBucketNameError(error_msg)\n\n    return bucket_name",
        "variables": [
            "bucket_name",
            "error_msg"
        ],
        "docstring": "Validate the format of an AWS S3 bucket name according to AWS rules.\n\nParameters\n----------\nbucket_name\n    The name of the bucket to validate.\n\nReturns\n-------\nstr\n    The validated bucket name if valid.\n\nRaises\n------\nInvalidBucketNameError\n    If the bucket name does not meet AWS specifications.\n\nExamples\n--------\n>>> validate_bucket_name('valid-bucket-name')\n'valid-bucket-name'\n\n>>> validate_bucket_name('Invalid_Bucket_Name')\nInvalidBucketNameError: Bucket name must not contain underscores."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "validate_s3_file_path",
        "code_chunk": "def validate_s3_file_path(file_path: str, allow_s3_scheme: bool) -> str:\n    \"\"\"Validate the file path based on the S3 URI scheme.\n\n    If `allow_s3_scheme` is True, the file path must contain an S3 URI scheme\n    (either 's3://' or 's3a://').\n\n    If `allow_s3_scheme` is False, the file path should not contain an S3 URI scheme.\n\n    Parameters\n    ----------\n    file_path\n        The file path to validate.\n    allow_s3_scheme\n        Whether or not to allow an S3 URI scheme in the file path.\n\n    Returns\n    -------\n    str\n        The validated file path if valid.\n\n    Raises\n    ------\n    InvalidS3FilePathError\n        If the validation fails based on the value of `allow_s3_scheme`.\n\n    Examples\n    --------\n    >>> validate_s3_file_path('data_folder/data.csv', allow_s3_scheme=False)\n    'data_folder/data.csv'\n\n    >>> validate_s3_file_path('s3a://bucket-name/data.csv', allow_s3_scheme=True)\n    's3a://bucket-name/data.csv'\n\n    >>> validate_s3_file_path('s3a://bucket-name/data.csv', allow_s3_scheme=False)\n    InvalidS3FilePathError: The file_path should not contain an S3 URI scheme\n    like 's3://' or 's3a://'.\n    \"\"\"\n    # Check if the file path is empty\n    if not file_path:\n        error_msg = \"The file path cannot be empty.\"\n        raise InvalidS3FilePathError(error_msg)\n\n    has_s3_scheme = file_path.startswith(\"s3://\") or file_path.startswith(\"s3a://\")\n\n    if allow_s3_scheme and not has_s3_scheme:\n        error_msg = (\n            \"The file_path must contain an S3 URI scheme like 's3://' or 's3a://'.\"\n        )\n        raise InvalidS3FilePathError(error_msg)\n\n    if not allow_s3_scheme and has_s3_scheme:\n        error_msg = (\n            \"The file_path should not contain an S3 URI scheme \"\n            \"like 's3://' or 's3a://'.\"\n        )\n        raise InvalidS3FilePathError(error_msg)\n\n    return file_path",
        "variables": [
            "allow_s3_scheme",
            "file_path",
            "has_s3_scheme",
            "error_msg"
        ],
        "docstring": "Validate the file path based on the S3 URI scheme.\n\nIf `allow_s3_scheme` is True, the file path must contain an S3 URI scheme\n(either 's3://' or 's3a://').\n\nIf `allow_s3_scheme` is False, the file path should not contain an S3 URI scheme.\n\nParameters\n----------\nfile_path\n    The file path to validate.\nallow_s3_scheme\n    Whether or not to allow an S3 URI scheme in the file path.\n\nReturns\n-------\nstr\n    The validated file path if valid.\n\nRaises\n------\nInvalidS3FilePathError\n    If the validation fails based on the value of `allow_s3_scheme`.\n\nExamples\n--------\n>>> validate_s3_file_path('data_folder/data.csv', allow_s3_scheme=False)\n'data_folder/data.csv'\n\n>>> validate_s3_file_path('s3a://bucket-name/data.csv', allow_s3_scheme=True)\n's3a://bucket-name/data.csv'\n\n>>> validate_s3_file_path('s3a://bucket-name/data.csv', allow_s3_scheme=False)\nInvalidS3FilePathError: The file_path should not contain an S3 URI scheme\nlike 's3://' or 's3a://'."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "is_s3_directory",
        "code_chunk": "def is_s3_directory(\n    client: boto3.client,\n    bucket_name: str,\n    object_name: str,\n) -> bool:\n    \"\"\"Check if an AWS S3 key is a directory by listing its contents.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    bucket_name\n        The name of the S3 bucket.\n    object_name\n        The S3 object name to check.\n\n    Returns\n    -------\n    bool\n        True if the key represents a directory, False otherwise.\n    \"\"\"\n    if not object_name.endswith(\"/\"):\n        object_name += \"/\"\n    try:\n        response = client.list_objects_v2(\n            Bucket=bucket_name,\n            Prefix=object_name,\n            Delimiter=\"/\",\n            MaxKeys=1,\n        )\n        if \"Contents\" in response or \"CommonPrefixes\" in response:\n            return True\n        else:\n            return False\n    except client.exceptions.ClientError as e:\n        logger.error(f\"Failed to check if key is a directory: {str(e)}\")\n        return False",
        "variables": [
            "client",
            "response",
            "object_name",
            "bucket_name",
            "e"
        ],
        "docstring": "Check if an AWS S3 key is a directory by listing its contents.\n\nParameters\n----------\nclient\n    The boto3 S3 client instance.\nbucket_name\n    The name of the S3 bucket.\nobject_name\n    The S3 object name to check.\n\nReturns\n-------\nbool\n    True if the key represents a directory, False otherwise."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "file_exists",
        "code_chunk": "def file_exists(\n    client: boto3.client,\n    bucket_name: str,\n    object_name: str,\n) -> bool:\n    \"\"\"Check if a specific file exists in an AWS S3 bucket.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client.\n    bucket_name\n        The name of the bucket.\n    object_name\n        The S3 object name to check for existence.\n\n    Returns\n    -------\n    bool\n        True if the file exists, otherwise False.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> file_exists(client, 'mybucket', 'folder/file.txt')\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    object_name = remove_leading_slash(object_name)\n\n    try:\n        client.head_object(Bucket=bucket_name, Key=object_name)\n        return True\n    except client.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            return False\n        else:\n            logger.error(f\"Failed to check file existence: {str(e)}\")\n            return False",
        "variables": [
            "client",
            "bucket_name",
            "e",
            "object_name"
        ],
        "docstring": "Check if a specific file exists in an AWS S3 bucket.\n\nParameters\n----------\nclient\n    The boto3 S3 client.\nbucket_name\n    The name of the bucket.\nobject_name\n    The S3 object name to check for existence.\n\nReturns\n-------\nbool\n    True if the file exists, otherwise False.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> file_exists(client, 'mybucket', 'folder/file.txt')\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "upload_file",
        "code_chunk": "def upload_file(\n    client: boto3.client,\n    bucket_name: str,\n    local_path: str,\n    object_name: Optional[str] = None,\n    overwrite: bool = False,\n) -> bool:\n    \"\"\"Upload a file to an Amazon S3 bucket from local directory.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    bucket_name\n        The name of the target S3 bucket.\n    local_path\n        The file path on the local system to upload.\n    object_name\n        The target S3 object name. If None, uses the base name of\n        the local file path.\n    overwrite\n        If True, the existing file on S3 will be overwritten.\n\n    Returns\n    -------\n    bool\n        True if the file was uploaded successfully, False otherwise.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> upload_file(\n    ...     client,\n    ...     'mybucket',\n    ...     '/path/to/file.txt',\n    ...     'folder/s3_file.txt'\n    ... )\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n\n    local_path = Path(local_path)\n    if not local_path.exists():\n        logger.error(\"Local file does not exist.\")\n        return False\n\n    if object_name is None:\n        object_name = local_path.name\n\n    object_name = remove_leading_slash(object_name)\n\n    if not overwrite and file_exists(client, bucket_name, object_name):\n        logger.error(\"File already exists in the bucket.\")\n        return False\n\n    try:\n        client.upload_file(str(local_path), bucket_name, object_name)\n        logger.info(\n            f\"Uploaded {local_path} to {bucket_name}/{object_name}\",\n        )\n        return True\n    except FileNotFoundError:\n        logger.error(\"The local file was not found.\")\n        return False\n    except client.exceptions.NoCredentialsError:\n        logger.error(\"Credentials not available.\")\n        return False",
        "variables": [
            "overwrite",
            "client",
            "local_path",
            "object_name",
            "bucket_name"
        ],
        "docstring": "Upload a file to an Amazon S3 bucket from local directory.\n\nParameters\n----------\nclient\n    The boto3 S3 client instance.\nbucket_name\n    The name of the target S3 bucket.\nlocal_path\n    The file path on the local system to upload.\nobject_name\n    The target S3 object name. If None, uses the base name of\n    the local file path.\noverwrite\n    If True, the existing file on S3 will be overwritten.\n\nReturns\n-------\nbool\n    True if the file was uploaded successfully, False otherwise.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> upload_file(\n...     client,\n...     'mybucket',\n...     '/path/to/file.txt',\n...     'folder/s3_file.txt'\n... )\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "download_file",
        "code_chunk": "def download_file(\n    client: boto3.client,\n    bucket_name: str,\n    object_name: str,\n    local_path: str,\n    overwrite: bool = False,\n) -> bool:\n    \"\"\"Download a file from an AWS S3 bucket to a local directory.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    bucket_name\n        The name of the S3 bucket from which to download the file.\n    object_name\n        The S3 object name of the file to download.\n    local_path\n        The local file path where the downloaded file will be saved.\n    overwrite\n        If True, overwrite the local file if it exists.\n\n    Returns\n    -------\n    bool\n        True if the file was downloaded successfully, False otherwise.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> download_file(\n    ...     client,\n    ...     'mybucket',\n    ...     'folder/s3_file.txt',\n    ...     '/path/to/download.txt'\n    ... )\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n\n    local_path = Path(local_path)\n\n    if not overwrite and local_path.exists():\n        logger.error(\"Local file already exists.\")\n        return False\n\n    object_name = remove_leading_slash(object_name)\n\n    if file_exists(client, bucket_name, object_name):\n        try:\n            client.download_file(\n                bucket_name,\n                object_name,\n                str(local_path),\n            )\n            logger.info(\n                f\"Downloaded {bucket_name}/{object_name} to {local_path}\",\n            )\n            return True\n        except client.exceptions.ClientError as e:\n            logger.error(f\"Failed to download file: {str(e)}\")\n            return False\n    else:\n        logger.error(\"File does not exist in the bucket.\")\n        return False",
        "variables": [
            "overwrite",
            "client",
            "local_path",
            "object_name",
            "bucket_name",
            "e"
        ],
        "docstring": "Download a file from an AWS S3 bucket to a local directory.\n\nParameters\n----------\nclient\n    The boto3 S3 client instance.\nbucket_name\n    The name of the S3 bucket from which to download the file.\nobject_name\n    The S3 object name of the file to download.\nlocal_path\n    The local file path where the downloaded file will be saved.\noverwrite\n    If True, overwrite the local file if it exists.\n\nReturns\n-------\nbool\n    True if the file was downloaded successfully, False otherwise.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> download_file(\n...     client,\n...     'mybucket',\n...     'folder/s3_file.txt',\n...     '/path/to/download.txt'\n... )\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "delete_file",
        "code_chunk": "def delete_file(\n    client: boto3.client,\n    bucket_name: str,\n    object_name: str,\n    overwrite: bool = False,\n) -> bool:\n    \"\"\"Delete a file from an AWS S3 bucket.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    bucket_name\n        The name of the bucket from which the file will be deleted.\n    object_name\n        The S3 object name of the file to delete.\n    overwrite\n        If False, the function will not delete the file if it does not exist;\n        set to True to ignore non-existence on delete.\n\n    Returns\n    -------\n    bool\n        True if the file was deleted successfully, otherwise False.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> delete_file(client, 'mybucket', 'folder/s3_file.txt')\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    object_name = remove_leading_slash(object_name)\n\n    if not overwrite and not file_exists(client, bucket_name, object_name):\n        logger.error(\"File does not exist in the bucket.\")\n        return False\n\n    try:\n        client.delete_object(Bucket=bucket_name, Key=object_name)\n        logger.info(f\"Deleted {bucket_name}/{object_name}\")\n        return True\n    except client.exceptions.ClientError as e:\n        logger.error(f\"Failed to delete file: {str(e)}\")\n        return False",
        "variables": [
            "overwrite",
            "client",
            "object_name",
            "bucket_name",
            "e"
        ],
        "docstring": "Delete a file from an AWS S3 bucket.\n\nParameters\n----------\nclient\n    The boto3 S3 client instance.\nbucket_name\n    The name of the bucket from which the file will be deleted.\nobject_name\n    The S3 object name of the file to delete.\noverwrite\n    If False, the function will not delete the file if it does not exist;\n    set to True to ignore non-existence on delete.\n\nReturns\n-------\nbool\n    True if the file was deleted successfully, otherwise False.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> delete_file(client, 'mybucket', 'folder/s3_file.txt')\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "copy_file",
        "code_chunk": "def copy_file(\n    client: boto3.client,\n    source_bucket_name: str,\n    source_object_name: str,\n    destination_bucket_name: str,\n    destination_object_name: str,\n    overwrite: bool = False,\n) -> bool:\n    \"\"\"Copy a file from one aWS S3 bucket to another.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    source_bucket_name\n        The name of the source bucket.\n    source_object_name\n        The S3 object name of the source file.\n    destination_bucket_name\n        The name of the destination bucket.\n    destination_object_name\n        The S3 object name of the destination file.\n    overwrite\n        If True, overwrite the destination file if it already exists.\n\n    Returns\n    -------\n    bool\n        True if the file was copied successfully, otherwise False.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> copy_file(\n    ...     client,\n    ...     'source-bucket',\n    ...     'source_file.txt',\n    ...     'destination-bucket',\n    ...     'destination_file.txt'\n    ... )\n    True\n    \"\"\"\n    source_bucket_name = validate_bucket_name(source_bucket_name)\n    destination_bucket_name = validate_bucket_name(destination_bucket_name)\n\n    source_object_name = remove_leading_slash(source_object_name)\n    destination_object_name = remove_leading_slash(destination_object_name)\n\n    if not overwrite and file_exists(\n        client,\n        destination_bucket_name,\n        destination_object_name,\n    ):\n        logger.error(\n            \"Destination file already exists in the destination bucket.\",\n        )\n        return False\n\n    copy_source = {\"Bucket\": source_bucket_name, \"Key\": source_object_name}\n    try:\n        client.copy_object(\n            CopySource=copy_source,\n            Bucket=destination_bucket_name,\n            Key=destination_object_name,\n        )\n        logger.info(\n            f\"Copied {source_bucket_name}/{source_object_name} to \"\n            f\"{destination_bucket_name}/{destination_object_name}\",\n        )\n        return True\n    except client.exceptions.ClientError as e:\n        logger.error(f\"Failed to copy file: {str(e)}\")\n        return False",
        "variables": [
            "source_object_name",
            "overwrite",
            "destination_bucket_name",
            "destination_object_name",
            "client",
            "copy_source",
            "source_bucket_name",
            "e"
        ],
        "docstring": "Copy a file from one aWS S3 bucket to another.\n\nParameters\n----------\nclient\n    The boto3 S3 client instance.\nsource_bucket_name\n    The name of the source bucket.\nsource_object_name\n    The S3 object name of the source file.\ndestination_bucket_name\n    The name of the destination bucket.\ndestination_object_name\n    The S3 object name of the destination file.\noverwrite\n    If True, overwrite the destination file if it already exists.\n\nReturns\n-------\nbool\n    True if the file was copied successfully, otherwise False.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> copy_file(\n...     client,\n...     'source-bucket',\n...     'source_file.txt',\n...     'destination-bucket',\n...     'destination_file.txt'\n... )\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "create_folder_on_s3",
        "code_chunk": "def create_folder_on_s3(\n    client: boto3.client,\n    bucket_name: str,\n    folder_path: str,\n) -> bool:\n    \"\"\"Create a folder in an AWS S3 bucket if it doesn't already exist.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    bucket_name\n        The name of the bucket where the folder will be created.\n    folder_path\n        The name of the folder to create.\n\n    Returns\n    -------\n    bool\n        True if the folder was created successfully or\n        already exists, otherwise False.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> create_folder_on_s3(client, 'mybucket', 'new_folder/')\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    folder_path = remove_leading_slash(folder_path)\n\n    if not folder_path.endswith(\"/\"):\n        folder_path += \"/\"\n\n    try:\n        client.head_object(Bucket=bucket_name, Key=folder_path)\n        logger.info(f\"Folder '{folder_path}' already exists on S3.\")\n        return True\n    except client.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            # Folder does not exist, create it\n            try:\n                client.put_object(Bucket=bucket_name, Key=folder_path)\n                logger.info(f\"Created folder '{folder_path}' on S3.\")\n                return True\n            except client.exceptions.ClientError as e:\n                logger.error(f\"Failed to create folder on S3: {str(e)}\")\n                return False\n        else:\n            logger.error(f\"Failed to check folder existence on S3: {str(e)}\")\n            return False",
        "variables": [
            "folder_path",
            "client",
            "bucket_name",
            "e"
        ],
        "docstring": "Create a folder in an AWS S3 bucket if it doesn't already exist.\n\nParameters\n----------\nclient\n    The boto3 S3 client instance.\nbucket_name\n    The name of the bucket where the folder will be created.\nfolder_path\n    The name of the folder to create.\n\nReturns\n-------\nbool\n    True if the folder was created successfully or\n    already exists, otherwise False.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> create_folder_on_s3(client, 'mybucket', 'new_folder/')\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "upload_folder",
        "code_chunk": "def upload_folder(\n    client: boto3.client,\n    bucket_name: str,\n    local_path: str,\n    prefix: str = \"\",\n    overwrite: bool = False,\n) -> bool:\n    \"\"\"Upload an entire folder from the local file system to an AWS S3 bucket.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    bucket_name\n        The name of the bucket to which the folder will be uploaded.\n    local_path\n        The path to the local folder to upload.\n    prefix\n        The prefix to prepend to each object name when uploading to S3.\n    overwrite\n        If True, overwrite existing files in the bucket.\n\n    Returns\n    -------\n    bool\n        True if the folder was uploaded successfully, otherwise False.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> upload_folder(\n    ...     client,\n    ...     'mybucket',\n    ...     '/path/to/local/folder',\n    ...     'folder_prefix',\n    ...     True\n    ... )\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    local_path = Path(local_path)\n\n    # Check if the local folder exists\n    if not local_path.is_dir():\n        logger.error(\"Local folder does not exist.\")\n        return False\n\n    prefix = remove_leading_slash(prefix)\n\n    # Ensure the folder exists on S3\n    if not create_folder_on_s3(client, bucket_name, prefix):\n        logger.error(\"Failed to create folder on S3.\")\n        return False\n\n    # Iterate over files in the local folder and its subdirectories\n    for file_path in local_path.rglob(\"*\"):\n        if file_path.is_file():\n            # Determine the S3 object key\n            object_name = prefix + \"/\" + str(file_path.relative_to(local_path))\n            # Check if the file already exists in the bucket\n            if not overwrite and file_exists(\n                client,\n                bucket_name,\n                object_name,\n            ):\n                logger.error(\n                    f\"File '{object_name}' already exists in the bucket.\",\n                )\n                return False\n            # Upload the file to S3\n            try:\n                client.upload_file(str(file_path), bucket_name, object_name)\n                logger.info(f\"Uploaded '{file_path}' to '{object_name}'.\")\n            except FileNotFoundError:\n                logger.error(f\"The local file '{file_path}' was not found.\")\n                return False\n            except client.exceptions.NoCredentialsError:\n                logger.error(\"Credentials not available.\")\n                return False\n\n    return True",
        "variables": [
            "prefix",
            "overwrite",
            "client",
            "local_path",
            "object_name",
            "file_path",
            "bucket_name"
        ],
        "docstring": "Upload an entire folder from the local file system to an AWS S3 bucket.\n\nParameters\n----------\nclient\n    The boto3 S3 client instance.\nbucket_name\n    The name of the bucket to which the folder will be uploaded.\nlocal_path\n    The path to the local folder to upload.\nprefix\n    The prefix to prepend to each object name when uploading to S3.\noverwrite\n    If True, overwrite existing files in the bucket.\n\nReturns\n-------\nbool\n    True if the folder was uploaded successfully, otherwise False.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> upload_folder(\n...     client,\n...     'mybucket',\n...     '/path/to/local/folder',\n...     'folder_prefix',\n...     True\n... )\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "list_files",
        "code_chunk": "def list_files(\n    client: boto3.client,\n    bucket_name: str,\n    prefix: str = \"\",\n) -> List[str]:\n    \"\"\"List files in an AWS S3 bucket that match a specific prefix.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client.\n    bucket_name\n        The name of the bucket.\n    prefix\n        The prefix to filter files, by default \"\".\n\n    Returns\n    -------\n    List[str]\n        A list of S3 object keys matching the prefix.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> list_files(client, 'mybucket', 'folder_prefix/')\n    ['folder_prefix/file1.txt', 'folder_prefix/file2.txt']\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    prefix = remove_leading_slash(prefix)\n\n    try:\n        files = []\n        paginator = client.get_paginator(\"list_objects_v2\")\n        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n            if \"Contents\" in page:\n                for obj in page[\"Contents\"]:\n                    files.append(obj[\"Key\"])\n        return files\n    except client.exceptions.ClientError as e:\n        logger.error(f\"Failed to list files in bucket: {str(e)}\")\n        return []",
        "variables": [
            "paginator",
            "prefix",
            "files",
            "obj",
            "client",
            "bucket_name",
            "e",
            "page"
        ],
        "docstring": "List files in an AWS S3 bucket that match a specific prefix.\n\nParameters\n----------\nclient\n    The boto3 S3 client.\nbucket_name\n    The name of the bucket.\nprefix\n    The prefix to filter files, by default \"\".\n\nReturns\n-------\nList[str]\n    A list of S3 object keys matching the prefix.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> list_files(client, 'mybucket', 'folder_prefix/')\n['folder_prefix/file1.txt', 'folder_prefix/file2.txt']"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "download_folder",
        "code_chunk": "def download_folder(\n    client: boto3.client,\n    bucket_name: str,\n    prefix: str,\n    local_path: str,\n    overwrite: bool = False,\n) -> bool:\n    \"\"\"Download a folder from an AWS S3 bucket to a local directory.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    bucket_name\n        The name of the S3 bucket from which to download the folder.\n    prefix\n        The S3 prefix of the folder to download.\n    local_path\n        The local directory path where the downloaded folder will be saved.\n    overwrite\n        If True, overwrite existing local files if they exist.\n\n    Returns\n    -------\n    bool\n        True if the folder was downloaded successfully, False otherwise.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> download_folder(\n    ...     client,\n    ...     'mybucket',\n    ...     'folder/subfolder/',\n    ...     '/path/to/local_folder',\n    ...     overwrite=False\n    ... )\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    local_path = Path(local_path)\n\n    prefix = remove_leading_slash(prefix)\n    if not is_s3_directory(client, bucket_name, prefix):\n        logger.error(f\"The provided S3 prefix {prefix} is not a directory.\")\n        return False\n\n    if not local_path.exists():\n        local_path.mkdir(parents=True)\n\n    try:\n        paginator = client.get_paginator(\"list_objects_v2\")\n        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n            for obj in page.get(\"Contents\", []):\n                if is_s3_directory(client, bucket_name, obj[\"Key\"]):\n                    continue\n                target = local_path / Path(obj[\"Key\"]).relative_to(prefix)\n                if not overwrite and target.exists():\n                    logger.info(f\"Skipping {target} as it already exists.\")\n                    continue\n                if not target.parent.exists():\n                    target.parent.mkdir(parents=True)\n                client.download_file(bucket_name, obj[\"Key\"], str(target))\n                logger.info(f'Downloaded {obj[\"Key\"]} to {target}')\n        return True\n    except client.exceptions.ClientError as e:\n        logger.error(f\"Failed to download folder: {str(e)}\")\n        return False",
        "variables": [
            "target",
            "paginator",
            "prefix",
            "overwrite",
            "obj",
            "client",
            "local_path",
            "bucket_name",
            "e",
            "page"
        ],
        "docstring": "Download a folder from an AWS S3 bucket to a local directory.\n\nParameters\n----------\nclient\n    The boto3 S3 client instance.\nbucket_name\n    The name of the S3 bucket from which to download the folder.\nprefix\n    The S3 prefix of the folder to download.\nlocal_path\n    The local directory path where the downloaded folder will be saved.\noverwrite\n    If True, overwrite existing local files if they exist.\n\nReturns\n-------\nbool\n    True if the folder was downloaded successfully, False otherwise.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> download_folder(\n...     client,\n...     'mybucket',\n...     'folder/subfolder/',\n...     '/path/to/local_folder',\n...     overwrite=False\n... )\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "move_file",
        "code_chunk": "def move_file(\n    client: boto3.client,\n    source_bucket_name: str,\n    source_object_name: str,\n    destination_bucket_name: str,\n    destination_object_name: str,\n) -> bool:\n    \"\"\"Move a file within or between AWS S3 buckets.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    source_bucket_name\n        The name of the source S3 bucket.\n    source_object_name\n        The S3 object name of the source file.\n    destination_bucket_name\n        The name of the destination S3 bucket.\n    destination_object_name\n        The S3 object name of the destination file.\n\n    Returns\n    -------\n    bool\n        True if the file was moved successfully, False otherwise.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> move_file(\n    ...     client,\n    ...     'sourcebucket',\n    ...     'source_folder/file.txt',\n    ...     'destbucket',\n    ...     'dest_folder/file.txt'\n    ... )\n    True\n    \"\"\"\n    source_bucket_name = validate_bucket_name(source_bucket_name)\n    destination_bucket_name = validate_bucket_name(destination_bucket_name)\n\n    source_object_name = remove_leading_slash(source_object_name)\n    destination_object_name = remove_leading_slash(destination_object_name)\n\n    if file_exists(client, source_bucket_name, source_object_name):\n        try:\n            copy_source = {\n                \"Bucket\": source_bucket_name,\n                \"Key\": source_object_name,\n            }\n            client.copy(\n                copy_source,\n                destination_bucket_name,\n                destination_object_name,\n            )\n            client.delete_object(\n                Bucket=source_bucket_name,\n                Key=source_object_name,\n            )\n            logger.info(\n                f\"Moved {source_bucket_name}/{source_object_name} to \"\n                f\"{destination_bucket_name}/{destination_object_name}\",\n            )\n            return True\n        except client.exceptions.ClientError as e:\n            logger.error(f\"Failed to move file: {str(e)}\")\n            return False\n    else:\n        logger.error(\"Source file does not exist.\")\n        return False",
        "variables": [
            "source_object_name",
            "destination_bucket_name",
            "destination_object_name",
            "client",
            "copy_source",
            "source_bucket_name",
            "e"
        ],
        "docstring": "Move a file within or between AWS S3 buckets.\n\nParameters\n----------\nclient\n    The boto3 S3 client instance.\nsource_bucket_name\n    The name of the source S3 bucket.\nsource_object_name\n    The S3 object name of the source file.\ndestination_bucket_name\n    The name of the destination S3 bucket.\ndestination_object_name\n    The S3 object name of the destination file.\n\nReturns\n-------\nbool\n    True if the file was moved successfully, False otherwise.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> move_file(\n...     client,\n...     'sourcebucket',\n...     'source_folder/file.txt',\n...     'destbucket',\n...     'dest_folder/file.txt'\n... )\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "delete_folder",
        "code_chunk": "def delete_folder(\n    client: boto3.client,\n    bucket_name: str,\n    folder_path: str,\n) -> bool:\n    \"\"\"Delete a folder in an AWS S3 bucket.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    bucket_name\n        The name of the S3 bucket.\n    folder_path\n        The path of the folder to delete.\n\n    Returns\n    -------\n    bool\n        True if the folder was deleted successfully, otherwise False.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> delete_folder(client, 'mybucket', 'path/to/folder/')\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    folder_path = remove_leading_slash(folder_path)\n\n    if not is_s3_directory(client, bucket_name, folder_path):\n        logger.error(f\"The provided path {folder_path} is not a directory.\")\n        return False\n\n    paginator = client.get_paginator(\"list_objects_v2\")\n    try:\n        for page in paginator.paginate(Bucket=bucket_name, Prefix=folder_path):\n            if \"Contents\" in page:\n                for obj in page[\"Contents\"]:\n                    client.delete_object(Bucket=bucket_name, Key=obj[\"Key\"])\n        logger.info(f\"Deleted folder {folder_path} in bucket {bucket_name}\")\n        return True\n    except client.exceptions.ClientError as e:\n        logger.error(\n            f\"Failed to delete folder {folder_path} \"\n            f\"in bucket {bucket_name}: {str(e)}\",\n        )\n        return False",
        "variables": [
            "folder_path",
            "paginator",
            "obj",
            "client",
            "bucket_name",
            "e",
            "page"
        ],
        "docstring": "Delete a folder in an AWS S3 bucket.\n\nParameters\n----------\nclient\n    The boto3 S3 client instance.\nbucket_name\n    The name of the S3 bucket.\nfolder_path\n    The path of the folder to delete.\n\nReturns\n-------\nbool\n    True if the folder was deleted successfully, otherwise False.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> delete_folder(client, 'mybucket', 'path/to/folder/')\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "load_csv",
        "code_chunk": "def load_csv(\n    client: boto3.client,\n    bucket_name: str,\n    filepath: str,\n    keep_columns: Optional[List[str]] = None,\n    rename_columns: Optional[Dict[str, str]] = None,\n    drop_columns: Optional[List[str]] = None,\n    **kwargs,\n) -> pd.DataFrame:\n    \"\"\"Load a CSV file from an S3 bucket into a Pandas DataFrame.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    bucket_name\n        The name of the S3 bucket.\n    filepath\n        The key (full path and filename) of the CSV file in the S3 bucket.\n    keep_columns\n        A list of column names to keep in the DataFrame, dropping all others.\n        Default value is None.\n    rename_columns\n        A dictionary to rename columns where keys are existing column\n        names and values are new column names.\n        Default value is None.\n    drop_columns\n        A list of column names to drop from the DataFrame.\n        Default value is None.\n    kwargs\n        Additional keyword arguments to pass to the `pd.read_csv` method.\n\n    Returns\n    -------\n    pd.DataFrame\n        Pandas DataFrame containing the data from the CSV file.\n\n    Raises\n    ------\n    InvalidBucketNameError\n        If the bucket name does not meet AWS specifications.\n    InvalidS3FilePathError\n        If the file_path contains an S3 URI scheme like 's3://' or 's3a://'.\n    Exception\n        If there is an error loading the file.\n    ValueError\n        If a column specified in rename_columns, drop_columns, or\n        keep_columns is not found in the DataFrame.\n\n    Notes\n    -----\n    Transformation order:\n    1. Columns are kept according to `keep_columns`.\n    2. Columns are dropped according to `drop_columns`.\n    3. Columns are renamed according to `rename_columns`.\n\n    Examples\n    --------\n    Load a CSV file and rename columns:\n\n    >>> df = load_csv(\n            client,\n            \"my-bucket\",\n            \"path/to/file.csv\",\n            rename_columns={\"old_name\": \"new_name\"}\n        )\n\n    Load a CSV file and keep only specific columns:\n\n    >>> df = load_csv(\n            client,\n            \"my-bucket\",\n            \"path/to/file.csv\",\n            keep_columns=[\"col1\", \"col2\"]\n        )\n\n    Load a CSV file and drop specific columns:\n\n    >>> df = load_csv(\n            client,\n            \"my-bucket\",\n            \"path/to/file.csv\",\n            drop_columns=[\"col1\", \"col2\"]\n        )\n\n    Load a CSV file with custom delimiter:\n\n    >>> df = load_csv(\n            client,\n            \"my-bucket\",\n            \"path/to/file.csv\",\n            sep=\";\"\n        )\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    filepath = validate_s3_file_path(filepath, allow_s3_scheme=False)\n\n    try:\n        # Get the CSV file from S3\n        response = client.get_object(Bucket=bucket_name, Key=filepath)\n        logger.info(\n            f\"Loaded CSV file from S3 bucket {bucket_name}, filepath {filepath}\",\n        )\n\n        # Read the CSV file into a Pandas DataFrame\n        df = pd.read_csv(response[\"Body\"], **kwargs)\n\n    except Exception as e:\n        error_message = (\n            f\"Error loading file from bucket {bucket_name}, filepath {filepath}: {e}\"\n        )\n        logger.error(error_message)\n        raise Exception(error_message) from e\n\n    columns = df.columns.tolist()\n\n    # Apply column transformations: keep, drop, rename\n    if keep_columns:\n        missing_columns = [col for col in keep_columns if col not in columns]\n        if missing_columns:\n            error_message = (\n                f\"Columns {missing_columns} not found in DataFrame and cannot be kept\"\n            )\n            logger.error(error_message)\n            raise ValueError(error_message)\n        df = df[keep_columns]\n\n    if drop_columns:\n        for col in drop_columns:\n            if col in columns:\n                df = df.drop(columns=[col])\n            else:\n                error_message = (\n                    f\"Column '{col}' not found in DataFrame and cannot be dropped\"\n                )\n                logger.error(error_message)\n                raise ValueError(error_message)\n\n    if rename_columns:\n        for old_name, new_name in rename_columns.items():\n            if old_name in columns:\n                df = df.rename(columns={old_name: new_name})\n            else:\n                error_message = (\n                    f\"Column '{old_name}' not found in DataFrame and \"\n                    f\"cannot be renamed to '{new_name}'\"\n                )\n                logger.error(error_message)\n                raise ValueError(error_message)\n\n    return df",
        "variables": [
            "columns",
            "rename_columns",
            "drop_columns",
            "df",
            "error_message",
            "col",
            "filepath",
            "old_name",
            "new_name",
            "client",
            "response",
            "missing_columns",
            "keep_columns",
            "bucket_name",
            "kwargs",
            "e"
        ],
        "docstring": "Load a CSV file from an S3 bucket into a Pandas DataFrame.\n\nParameters\n----------\nclient\n    The boto3 S3 client instance.\nbucket_name\n    The name of the S3 bucket.\nfilepath\n    The key (full path and filename) of the CSV file in the S3 bucket.\nkeep_columns\n    A list of column names to keep in the DataFrame, dropping all others.\n    Default value is None.\nrename_columns\n    A dictionary to rename columns where keys are existing column\n    names and values are new column names.\n    Default value is None.\ndrop_columns\n    A list of column names to drop from the DataFrame.\n    Default value is None.\nkwargs\n    Additional keyword arguments to pass to the `pd.read_csv` method.\n\nReturns\n-------\npd.DataFrame\n    Pandas DataFrame containing the data from the CSV file.\n\nRaises\n------\nInvalidBucketNameError\n    If the bucket name does not meet AWS specifications.\nInvalidS3FilePathError\n    If the file_path contains an S3 URI scheme like 's3://' or 's3a://'.\nException\n    If there is an error loading the file.\nValueError\n    If a column specified in rename_columns, drop_columns, or\n    keep_columns is not found in the DataFrame.\n\nNotes\n-----\nTransformation order:\n1. Columns are kept according to `keep_columns`.\n2. Columns are dropped according to `drop_columns`.\n3. Columns are renamed according to `rename_columns`.\n\nExamples\n--------\nLoad a CSV file and rename columns:\n\n>>> df = load_csv(\n        client,\n        \"my-bucket\",\n        \"path/to/file.csv\",\n        rename_columns={\"old_name\": \"new_name\"}\n    )\n\nLoad a CSV file and keep only specific columns:\n\n>>> df = load_csv(\n        client,\n        \"my-bucket\",\n        \"path/to/file.csv\",\n        keep_columns=[\"col1\", \"col2\"]\n    )\n\nLoad a CSV file and drop specific columns:\n\n>>> df = load_csv(\n        client,\n        \"my-bucket\",\n        \"path/to/file.csv\",\n        drop_columns=[\"col1\", \"col2\"]\n    )\n\nLoad a CSV file with custom delimiter:\n\n>>> df = load_csv(\n        client,\n        \"my-bucket\",\n        \"path/to/file.csv\",\n        sep=\";\"\n    )"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "load_json",
        "code_chunk": "def load_json(\n    client: boto3.client,\n    bucket_name: str,\n    filepath: str,\n    encoding: Optional[str] = \"utf-8\",\n) -> Dict:\n    \"\"\"Load a JSON file from an S3 bucket.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    bucket_name\n        The name of the S3 bucket.\n    filepath\n        The key (full path and filename) of the JSON file in the S3 bucket.\n    encoding\n        The encoding of the JSON file. Default is 'utf-8'.\n\n    Returns\n    -------\n    Dict\n        Dictionary containing the data from the JSON file.\n\n    Raises\n    ------\n    InvalidBucketNameError\n        If the bucket name is invalid according to AWS rules.\n    Exception\n        If there is an error loading the file from S3 or parsing the JSON.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> data = load_json(client, 'my-bucket', 'path/to/file.json')\n    >>> print(data)\n    {\n        \"name\": \"John\",\n        \"age\": 30,\n        \"city\": \"Manchester\"\n    }\n    \"\"\"\n    # Validate bucket name and clean the filepath\n    bucket_name = validate_bucket_name(bucket_name)\n    filepath = remove_leading_slash(filepath)\n\n    try:\n        # Get the JSON file from S3\n        response = client.get_object(Bucket=bucket_name, Key=filepath)\n        logger.info(\n            f\"Loaded JSON file from S3 bucket {bucket_name}, filepath {filepath}\",\n        )\n\n        # Read the JSON content\n        json_data = response[\"Body\"].read().decode(encoding)\n        data = json.loads(json_data)\n\n    except Exception as e:\n        error_message = (\n            f\"Error loading or parsing JSON file from bucket {bucket_name}, \"\n            \"filepath {filepath}: {e}\"\n        )\n        logger.error(error_message)\n        raise Exception(error_message) from e\n\n    return data",
        "variables": [
            "json_data",
            "data",
            "error_message",
            "filepath",
            "client",
            "response",
            "encoding",
            "bucket_name",
            "e"
        ],
        "docstring": "Load a JSON file from an S3 bucket.\n\nParameters\n----------\nclient\n    The boto3 S3 client instance.\nbucket_name\n    The name of the S3 bucket.\nfilepath\n    The key (full path and filename) of the JSON file in the S3 bucket.\nencoding\n    The encoding of the JSON file. Default is 'utf-8'.\n\nReturns\n-------\nDict\n    Dictionary containing the data from the JSON file.\n\nRaises\n------\nInvalidBucketNameError\n    If the bucket name is invalid according to AWS rules.\nException\n    If there is an error loading the file from S3 or parsing the JSON.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> data = load_json(client, 'my-bucket', 'path/to/file.json')\n>>> print(data)\n{\n    \"name\": \"John\",\n    \"age\": 30,\n    \"city\": \"Manchester\"\n}"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "write_csv",
        "code_chunk": "def write_csv(\n    client: boto3.client,\n    bucket_name: str,\n    data: pd.DataFrame,\n    filepath: str,\n    **kwargs,\n) -> bool:\n    \"\"\"Write a Pandas Dataframe to csv in an S3 bucket.\n\n    Uses StringIO library as a RAM buffer, so at first Pandas writes data to the\n    buffer, then the buffer returns to the beginning, and then it is sent to\n    the S3 bucket using the boto3.put_object method.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    bucket_name\n        The name of the S3 bucket.\n    data\n        The dataframe to write to the specified path.\n    filepath\n        The filepath to save the dataframe to.\n    kwargs\n        Optional dictionary of Pandas to_csv arguments.\n\n    Returns\n    -------\n    bool\n        True if the dataframe is written successfully.\n        False if it was not possible to serialise or write the file.\n\n    Raises\n    ------\n    Exception\n        If there is an error writing the file to S3.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> data = pd.DataFrame({\n    >>>     'column1': [1, 2, 3],\n    >>>     'column2': ['a', 'b', 'c']\n    >>> })\n    >>> write_csv(client, 'my_bucket', data, 'path/to/file.csv')\n    True\n    \"\"\"\n    try:\n        # Create an Input-Output buffer\n        csv_buffer = StringIO()\n\n        # Write the dataframe to the buffer in the CSV format\n        data.to_csv(csv_buffer, **kwargs)\n\n        # \"Rewind\" the stream to the start of the buffer\n        csv_buffer.seek(0)\n\n        # Write the buffer into the s3 bucket. Assign the output to a mute\n        # variable, so the output is not displayed in the console or log.\n        _ = client.put_object(\n            Bucket=bucket_name,\n            Body=csv_buffer.getvalue(),\n            Key=filepath,\n        )\n        logger.info(f\"Successfully wrote dataframe to {bucket_name}/{filepath}\")\n        return True\n\n    except Exception as e:\n        error_message = (\n            f\"Error writing to csv or saving to bucket {bucket_name}, \"\n            f\"filepath {filepath}: {e}\"\n        )\n        logger.error(error_message)\n        return False",
        "variables": [
            "_",
            "data",
            "error_message",
            "filepath",
            "client",
            "bucket_name",
            "kwargs",
            "csv_buffer",
            "e"
        ],
        "docstring": "Write a Pandas Dataframe to csv in an S3 bucket.\n\nUses StringIO library as a RAM buffer, so at first Pandas writes data to the\nbuffer, then the buffer returns to the beginning, and then it is sent to\nthe S3 bucket using the boto3.put_object method.\n\nParameters\n----------\nclient\n    The boto3 S3 client instance.\nbucket_name\n    The name of the S3 bucket.\ndata\n    The dataframe to write to the specified path.\nfilepath\n    The filepath to save the dataframe to.\nkwargs\n    Optional dictionary of Pandas to_csv arguments.\n\nReturns\n-------\nbool\n    True if the dataframe is written successfully.\n    False if it was not possible to serialise or write the file.\n\nRaises\n------\nException\n    If there is an error writing the file to S3.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> data = pd.DataFrame({\n>>>     'column1': [1, 2, 3],\n>>>     'column2': ['a', 'b', 'c']\n>>> })\n>>> write_csv(client, 'my_bucket', data, 'path/to/file.csv')\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\helpers\\s3_utils.py",
        "function_name": "write_excel",
        "code_chunk": "def write_excel(\n    client: boto3.client,\n    bucket_name: str,\n    data: pd.DataFrame,\n    filepath: str,\n    **kwargs,\n) -> bool:\n    \"\"\"Write a Pandas DataFrame to an Excel file in an S3 bucket.\n\n    Uses BytesIO as a RAM buffer. Pandas writes data to the buffer,\n    the buffer rewinds to the beginning, and then it is sent to S3\n    using the boto3.put_object method.\n\n    Parameters\n    ----------\n    client : boto3.client\n        The boto3 S3 client instance.\n    bucket_name : str\n        The name of the S3 bucket.\n    data : pd.DataFrame\n        The dataframe to write to the specified path.\n    filepath : str\n        The filepath to save the dataframe to in the S3 bucket.\n    kwargs : dict\n        Optional dictionary of Pandas `to_excel` arguments.\n\n    Returns\n    -------\n    bool\n        True if the dataframe is written successfully, False otherwise.\n\n    Raises\n    ------\n    Exception\n        If there is an error writing the file to S3.\n\n    Examples\n    --------\n    >>> client = boto3.client('s3')\n    >>> data = pd.DataFrame({\n    >>>     'column1': [1, 2, 3],\n    >>>     'column2': ['a', 'b', 'c']\n    >>> })\n    >>> write_excel(client, 'my_bucket', data, 'path/to/file.xlsx')\n    True\n    \"\"\"\n    try:\n        # Create an in-memory bytes buffer\n        excel_buffer = BytesIO()\n\n        # Write DataFrame to the buffer in Excel format\n        with pd.ExcelWriter(excel_buffer, engine=\"xlsxwriter\") as writer:\n            data.to_excel(writer, index=False, **kwargs)\n\n        # Ensure the buffer is at the beginning\n        excel_buffer.seek(0)\n\n        # Upload the buffer to S3\n        client.put_object(\n            Bucket=bucket_name,\n            Body=excel_buffer.getvalue(),\n            Key=filepath,\n        )\n\n        logger.info(f\"Successfully wrote dataframe to {bucket_name}/{filepath}\")\n        return True\n\n    except Exception as e:\n        logger.error(\n            f\"Error writing to Excel or saving to bucket {bucket_name}, \"\n            f\"filepath {filepath}: {e}\",\n        )\n        return False",
        "variables": [
            "data",
            "excel_buffer",
            "filepath",
            "client",
            "writer",
            "bucket_name",
            "kwargs",
            "e"
        ],
        "docstring": "Write a Pandas DataFrame to an Excel file in an S3 bucket.\n\nUses BytesIO as a RAM buffer. Pandas writes data to the buffer,\nthe buffer rewinds to the beginning, and then it is sent to S3\nusing the boto3.put_object method.\n\nParameters\n----------\nclient : boto3.client\n    The boto3 S3 client instance.\nbucket_name : str\n    The name of the S3 bucket.\ndata : pd.DataFrame\n    The dataframe to write to the specified path.\nfilepath : str\n    The filepath to save the dataframe to in the S3 bucket.\nkwargs : dict\n    Optional dictionary of Pandas `to_excel` arguments.\n\nReturns\n-------\nbool\n    True if the dataframe is written successfully, False otherwise.\n\nRaises\n------\nException\n    If there is an error writing the file to S3.\n\nExamples\n--------\n>>> client = boto3.client('s3')\n>>> data = pd.DataFrame({\n>>>     'column1': [1, 2, 3],\n>>>     'column2': ['a', 'b', 'c']\n>>> })\n>>> write_excel(client, 'my_bucket', data, 'path/to/file.xlsx')\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\input.py",
        "function_name": "get_current_database",
        "code_chunk": "def get_current_database(spark: SparkSession) -> str:\n    \"\"\"Retrieve the current database from the active SparkSession.\"\"\"\n    return spark.sql(\"SELECT current_database()\").collect()[0][\"current_database()\"]",
        "variables": [
            "spark"
        ],
        "docstring": "Retrieve the current database from the active SparkSession."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\input.py",
        "function_name": "get_tables_in_database",
        "code_chunk": "def get_tables_in_database(spark: SparkSession, database_name: str) -> List[str]:\n    \"\"\"Get a list of tables in a given database.\n\n    Parameters\n    ----------\n    spark\n        Active SparkSession.\n    database_name\n        The name of the database from which to list tables.\n\n    Returns\n    -------\n    List[str]\n        A list of table names in the specified database.\n\n    Raises\n    ------\n    ValueError\n        If there is an error fetching tables from the specified database.\n\n    Examples\n    --------\n    >>> tables = get_tables_in_database(spark, \"default\")\n    >>> print(tables)\n    ['table1', 'table2', 'table3']\n    \"\"\"\n    try:\n        tables_df = spark.sql(f\"SHOW TABLES IN {database_name}\")\n        tables = [row[\"tableName\"] for row in tables_df.collect()]\n        return tables\n    except Exception as e:\n        error_msg = f\"Error fetching tables from database {database_name}: {e}\"\n        logger.error(error_msg)\n        raise ValueError(error_msg) from e",
        "variables": [
            "database_name",
            "tables",
            "error_msg",
            "row",
            "spark",
            "tables_df",
            "e"
        ],
        "docstring": "Get a list of tables in a given database.\n\nParameters\n----------\nspark\n    Active SparkSession.\ndatabase_name\n    The name of the database from which to list tables.\n\nReturns\n-------\nList[str]\n    A list of table names in the specified database.\n\nRaises\n------\nValueError\n    If there is an error fetching tables from the specified database.\n\nExamples\n--------\n>>> tables = get_tables_in_database(spark, \"default\")\n>>> print(tables)\n['table1', 'table2', 'table3']"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\input.py",
        "function_name": "extract_database_name",
        "code_chunk": "def extract_database_name(\n    spark: SparkSession,\n    long_table_name: str,\n) -> Tuple[str, str]:\n    \"\"\"Extract the database component and table name from a compound table name.\n\n    This function can handle multiple scenarios:\n\n    1. For GCP's naming format '<project>.<database>.<table>',\n       the function will return the database and table name.\n\n    2. If the name is formatted as 'db_name.table_name', the function will\n       extract and return the database and table names.\n\n    3. If the long_table_name contains only the table name (e.g., 'table_name'),\n       the function will use the current database of the SparkSession.\n\n    4. For any other incorrectly formatted names, the function will raise\n       a ValueError.\n\n    Parameters\n    ----------\n    spark\n        Active SparkSession.\n    long_table_name\n        Full name of the table, which can include the GCP project\n        and/or database name.\n\n    Returns\n    -------\n    Tuple[str, str]\n        A tuple containing the name of the database and the table name.\n\n    Raises\n    ------\n    ValueError\n        If the table name doesn't match any of the expected formats.\n    \"\"\"\n    parts = long_table_name.split(\".\")\n\n    if len(parts) == 3:  # GCP format: project.database.table\n        _, db_name, table_name = parts\n\n    elif len(parts) == 2:  # Common format: database.table\n        db_name, table_name = parts\n\n    elif len(parts) == 1:  # Only table name is given\n        db_name = get_current_database(spark)\n        table_name = parts[0]\n\n    else:\n        error_msg = (\n            f\"Table name {long_table_name} is incorrectly formatted. \"\n            \"Expected formats: <project>.<database>.<table>, \"\n            \"<database>.<table>, or <table>\"\n        )\n        logger.error(error_msg)\n        raise ValueError(error_msg)\n\n    logger.info(\n        f\"Extracted database name: {db_name}, table name: \"\n        f\"{table_name} from {long_table_name}\",\n    )\n    return db_name, table_name",
        "variables": [
            "_",
            "error_msg",
            "db_name",
            "spark",
            "parts",
            "table_name",
            "long_table_name"
        ],
        "docstring": "Extract the database component and table name from a compound table name.\n\nThis function can handle multiple scenarios:\n\n1. For GCP's naming format '<project>.<database>.<table>',\n   the function will return the database and table name.\n\n2. If the name is formatted as 'db_name.table_name', the function will\n   extract and return the database and table names.\n\n3. If the long_table_name contains only the table name (e.g., 'table_name'),\n   the function will use the current database of the SparkSession.\n\n4. For any other incorrectly formatted names, the function will raise\n   a ValueError.\n\nParameters\n----------\nspark\n    Active SparkSession.\nlong_table_name\n    Full name of the table, which can include the GCP project\n    and/or database name.\n\nReturns\n-------\nTuple[str, str]\n    A tuple containing the name of the database and the table name.\n\nRaises\n------\nValueError\n    If the table name doesn't match any of the expected formats."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\input.py",
        "function_name": "load_and_validate_table",
        "code_chunk": "def load_and_validate_table(\n    spark: SparkSession,\n    table_name: str,\n    skip_validation: bool = False,\n    err_msg: str = None,\n    filter_cond: str = None,\n    keep_columns: Optional[List[str]] = None,\n    rename_columns: Optional[Dict[str, str]] = None,\n    drop_columns: Optional[List[str]] = None,\n) -> SparkDF:\n    \"\"\"Load a table, apply transformations, and validate if it is not empty.\n\n    Parameters\n    ----------\n    spark\n        Active SparkSession.\n    table_name\n        Name of the table to load.\n    skip_validation\n        If True, skips validation step, by default False.\n    err_msg\n        Error message to return if table is empty, by default None.\n    filter_cond\n        Condition to apply to SparkDF once read, by default None.\n    keep_columns\n        A list of column names to keep in the DataFrame, dropping all others.\n        Default value is None.\n    rename_columns\n        A dictionary to rename columns where keys are existing column\n        names and values are new column names.\n        Default value is None.\n    drop_columns\n        A list of column names to drop from the DataFrame.\n        Default value is None.\n\n    Returns\n    -------\n    SparkDF\n        Loaded SparkDF if validated, subject to options above.\n\n    Raises\n    ------\n    PermissionError\n        If there's an issue accessing the table or if the table\n        does not exist in the specified database.\n    ValueError\n        If the table is empty after loading, becomes empty after applying\n        a filter condition, or if columns specified in keep_columns,\n        drop_columns, or rename_columns do not exist in the DataFrame.\n\n    Notes\n    -----\n    Transformation order:\n    1. Columns are kept according to `keep_columns`.\n    2. Columns are dropped according to `drop_columns`.\n    3. Columns are renamed according to `rename_columns`.\n\n    Examples\n    --------\n    Load a table, apply a filter, and validate it:\n\n    >>> df = load_and_validate_table(\n            spark=spark,\n            table_name=\"my_table\",\n            filter_cond=\"age > 21\"\n        )\n\n    Load a table and keep only specific columns:\n\n    >>> df = load_and_validate_table(\n            spark=spark,\n            table_name=\"my_table\",\n            keep_columns=[\"name\", \"age\", \"city\"]\n        )\n\n    Load a table, drop specific columns, and rename a column:\n\n    >>> df = load_and_validate_table(\n            spark=spark,\n            table_name=\"my_table\",\n            drop_columns=[\"extra_column\"],\n            rename_columns={\"name\": \"full_name\"}\n        )\n\n    Load a table, skip validation, and apply all transformations:\n\n    >>> df = load_and_validate_table(\n            spark=spark,\n            table_name=\"my_table\",\n            skip_validation=True,\n            keep_columns=[\"name\", \"age\", \"city\"],\n            drop_columns=[\"extra_column\"],\n            rename_columns={\"name\": \"full_name\"},\n            filter_cond=\"age > 21\"\n        )\n    \"\"\"\n    try:\n        df = spark.read.table(table_name)\n        logger.info(f\"Successfully loaded table {table_name}.\")\n    except Exception as e:\n        db_name, _ = extract_database_name(spark, table_name)\n        db_err = (\n            f\"Error accessing {table_name} in the {db_name} database. \"\n            \"Check you have access to the database and that \"\n            \"the table name is correct.\"\n        )\n        logger.error(db_err)\n        raise PermissionError(db_err) from e\n\n    columns = [str(col) for col in df.columns]\n\n    # Apply column transformations: keep, drop, rename\n    if keep_columns:\n        missing_columns = [col for col in keep_columns if col not in columns]\n        if missing_columns:\n            error_message = (\n                f\"Columns {missing_columns} not found in DataFrame and cannot be kept\"\n            )\n            logger.error(error_message)\n            raise ValueError(error_message)\n        df = df.select(*keep_columns)\n\n    if drop_columns:\n        for col in drop_columns:\n            if col in columns:\n                df = df.drop(col)\n            else:\n                error_message = (\n                    f\"Column '{col}' not found in DataFrame and cannot be dropped\"\n                )\n                logger.error(error_message)\n                raise ValueError(error_message)\n\n    if rename_columns:\n        for old_name, new_name in rename_columns.items():\n            if old_name in columns:\n                df = df.withColumnRenamed(old_name, new_name)\n            else:\n                error_message = (\n                    f\"Column '{old_name}' not found in DataFrame and \"\n                    f\"cannot be renamed to '{new_name}'\"\n                )\n                logger.error(error_message)\n                raise ValueError(error_message)\n\n    # Validate the table if skip_validation is not True\n    if not skip_validation:\n        if df.rdd.isEmpty():\n            err_msg = err_msg or f\"Table {table_name} is empty.\"\n            raise DataframeEmptyError(err_msg)\n\n    # Apply the filter condition if provided\n    if filter_cond:\n        df = df.filter(filter_cond)\n        if not skip_validation and df.rdd.isEmpty():\n            err_msg = (\n                err_msg\n                or f\"Table {table_name} is empty after applying \"\n                f\"filter condition [{filter_cond}].\"\n            )\n            raise DataframeEmptyError(err_msg)\n\n    logger.info(\n        (\n            f\"Loaded and validated table {table_name}. \"\n            f\"Filter condition applied: {filter_cond}. \"\n            f\"Keep columns: {keep_columns}, Drop columns: {drop_columns}, \"\n            f\"Rename columns: {rename_columns}.\"\n        ),\n    )\n\n    return df",
        "variables": [
            "drop_columns",
            "filter_cond",
            "new_name",
            "keep_columns",
            "err_msg",
            "columns",
            "db_err",
            "spark",
            "_",
            "rename_columns",
            "error_message",
            "db_name",
            "skip_validation",
            "df",
            "col",
            "old_name",
            "missing_columns",
            "table_name",
            "e"
        ],
        "docstring": "Load a table, apply transformations, and validate if it is not empty.\n\nParameters\n----------\nspark\n    Active SparkSession.\ntable_name\n    Name of the table to load.\nskip_validation\n    If True, skips validation step, by default False.\nerr_msg\n    Error message to return if table is empty, by default None.\nfilter_cond\n    Condition to apply to SparkDF once read, by default None.\nkeep_columns\n    A list of column names to keep in the DataFrame, dropping all others.\n    Default value is None.\nrename_columns\n    A dictionary to rename columns where keys are existing column\n    names and values are new column names.\n    Default value is None.\ndrop_columns\n    A list of column names to drop from the DataFrame.\n    Default value is None.\n\nReturns\n-------\nSparkDF\n    Loaded SparkDF if validated, subject to options above.\n\nRaises\n------\nPermissionError\n    If there's an issue accessing the table or if the table\n    does not exist in the specified database.\nValueError\n    If the table is empty after loading, becomes empty after applying\n    a filter condition, or if columns specified in keep_columns,\n    drop_columns, or rename_columns do not exist in the DataFrame.\n\nNotes\n-----\nTransformation order:\n1. Columns are kept according to `keep_columns`.\n2. Columns are dropped according to `drop_columns`.\n3. Columns are renamed according to `rename_columns`.\n\nExamples\n--------\nLoad a table, apply a filter, and validate it:\n\n>>> df = load_and_validate_table(\n        spark=spark,\n        table_name=\"my_table\",\n        filter_cond=\"age > 21\"\n    )\n\nLoad a table and keep only specific columns:\n\n>>> df = load_and_validate_table(\n        spark=spark,\n        table_name=\"my_table\",\n        keep_columns=[\"name\", \"age\", \"city\"]\n    )\n\nLoad a table, drop specific columns, and rename a column:\n\n>>> df = load_and_validate_table(\n        spark=spark,\n        table_name=\"my_table\",\n        drop_columns=[\"extra_column\"],\n        rename_columns={\"name\": \"full_name\"}\n    )\n\nLoad a table, skip validation, and apply all transformations:\n\n>>> df = load_and_validate_table(\n        spark=spark,\n        table_name=\"my_table\",\n        skip_validation=True,\n        keep_columns=[\"name\", \"age\", \"city\"],\n        drop_columns=[\"extra_column\"],\n        rename_columns={\"name\": \"full_name\"},\n        filter_cond=\"age > 21\"\n    )"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\output.py",
        "function_name": "insert_df_to_hive_table",
        "code_chunk": "def insert_df_to_hive_table(\n    spark: SparkSession,\n    df: SparkDF,\n    table_name: str,\n    overwrite: bool = False,\n    fill_missing_cols: bool = False,\n    repartition_data_by: Union[int, str, None] = None,\n) -> None:\n    \"\"\"Write SparkDF to Hive table with optional configuration.\n\n    This function writes data from a SparkDF into a Hive table, handling missing\n    columns and optional repartitioning. It ensures the table's column order matches\n    the DataFrame and manages different overwrite behaviors for partitioned and\n    non-partitioned data.\n\n    Parameters\n    ----------\n    spark\n        Active SparkSession.\n    df\n        SparkDF containing data to be written.\n    table_name\n        Name of the Hive table to write data into.\n    overwrite\n        Controls how existing data is handled, default is False:\n\n        For non-partitioned data:\n        - True: Replaces entire table with DataFrame data.\n        - False: Appends DataFrame data to existing table.\n\n        For partitioned data:\n        - True: Replaces data only in partitions present in DataFrame.\n        - False: Appends data to existing partitions or creates new ones.\n    fill_missing_cols\n        If True, adds missing columns as NULL values. If False, raises an error\n        on schema mismatch, default is False.\n\n        - Explicitly casts DataFrame columns to match the Hive table schema to\n          avoid type mismatch errors.\n        - Adds missing columns as NULL values when `fill_missing_cols` is True,\n          regardless of their data type (e.g., String, Integer, Double, Boolean, etc.).\n    repartition_data_by\n        Controls data repartitioning, default is None:\n        - int: Sets target number of partitions.\n        - str: Specifies column to repartition by.\n        - None: No repartitioning performed.\n\n    Notes\n    -----\n    When using repartition with a number:\n    - Affects physical file structure but preserves Hive partitioning scheme.\n    - Controls number of output files per write operation per Hive partition.\n    - Maintains partition-based query optimization.\n\n    When repartitioning by column:\n    - Helps balance file sizes across Hive partitions.\n    - Reduces creation of small files.\n\n    Raises\n    ------\n    AnalysisException\n        If there's an error reading the table. This can occur if the table\n        doesn't exist or if there's no access to it.\n    ValueError\n        If the SparkDF schema does not match the Hive table schema and\n        'fill_missing_cols' is set to False.\n    DataframeEmptyError\n        If input DataFrame is empty.\n    Exception\n        For other general exceptions when writing data to the table.\n\n    Examples\n    --------\n    Write a DataFrame to a Hive table without overwriting:\n    >>> insert_df_to_hive_table(\n    ...     spark=spark,\n    ...     df=df,\n    ...     table_name=\"my_database.my_table\"\n    ... )\n\n    Overwrite an existing table with a DataFrame:\n    >>> insert_df_to_hive_table(\n    ...     spark=spark,\n    ...     df=df,\n    ...     table_name=\"my_database.my_table\",\n    ...     overwrite=True\n    ... )\n\n    Write a DataFrame to a Hive table with missing columns filled:\n    >>> insert_df_to_hive_table(\n    ...     spark=spark,\n    ...     df=df,\n    ...     table_name=\"my_database.my_table\",\n    ...     fill_missing_cols=True\n    ... )\n\n    Repartition by column before writing to Hive:\n    >>> insert_df_to_hive_table(\n    ...     spark=spark,\n    ...     df=df,\n    ...     table_name=\"my_database.my_table\",\n    ...     repartition_data_by=\"partition_column\"\n    ... )\n\n    Repartition into a fixed number of partitions before writing:\n    >>> insert_df_to_hive_table(\n    ...     spark=spark,\n    ...     df=df,\n    ...     table_name=\"my_database.my_table\",\n    ...     repartition_data_by=10\n    ... )\n    \"\"\"\n    logger.info(f\"Preparing to write data to {table_name} with overwrite={overwrite}.\")\n\n    # Check if the table exists; if not, set flag for later creation\n    table_exists = True\n    try:\n        table_schema = spark.read.table(table_name).schema\n        table_columns = spark.read.table(table_name).columns\n    except AnalysisException:\n        logger.info(\n            f\"Table {table_name} does not exist and will be \"\n            \"created after transformations.\",\n        )\n        table_exists = False\n        table_columns = df.columns  # Use DataFrame columns as initial schema\n\n    # Validate SparkDF before writing\n    if is_df_empty(df):\n        msg = f\"Cannot write an empty SparkDF to {table_name}\"\n        raise DataframeEmptyError(msg)\n\n    # Handle missing columns if specified\n    if fill_missing_cols and table_exists:\n        missing_columns = list(set(table_columns) - set(df.columns))\n        for col in missing_columns:\n            column_type = [\n                field.dataType for field in table_schema if field.name == col\n            ][0]\n            df = df.withColumn(col, F.lit(None).cast(column_type))\n    elif not fill_missing_cols and table_exists:\n        # Validate schema before writing\n        if set(table_columns) != set(df.columns):\n            msg = (\n                f\"SparkDF schema does not match table {table_name} \"\n                f\"schema and 'fill_missing_cols' is False.\"\n            )\n            raise ValueError(msg)\n\n    # Ensure column order\n    df = df.select(table_columns) if table_exists else df\n\n    # Apply repartitioning if specified\n    if repartition_data_by is not None:\n        if isinstance(repartition_data_by, int):\n            logger.info(f\"Repartitioning data into {repartition_data_by} partitions.\")\n            df = df.repartition(repartition_data_by)\n        elif isinstance(repartition_data_by, str):\n            logger.info(f\"Repartitioning data by column {repartition_data_by}.\")\n            df = df.repartition(repartition_data_by)\n\n    # Write DataFrame to Hive table based on existence and overwrite parameter\n    try:\n        if table_exists:\n            if overwrite:\n                logger.info(f\"Overwriting existing table {table_name}.\")\n                df.write.mode(\"overwrite\").saveAsTable(table_name)\n            else:\n                logger.info(\n                    f\"Inserting into existing table {table_name} without overwrite.\",\n                )\n                df.write.insertInto(table_name)\n        else:\n            df.write.saveAsTable(table_name)\n            logger.info(f\"Table {table_name} created successfully.\")\n        logger.info(f\"Successfully wrote data to {table_name}.\")\n    except Exception:\n        logger.error(f\"Error writing data to {table_name}.\")\n        raise",
        "variables": [
            "repartition_data_by",
            "table_exists",
            "column_type",
            "field",
            "df",
            "col",
            "overwrite",
            "missing_columns",
            "spark",
            "table_columns",
            "msg",
            "table_name",
            "table_schema",
            "fill_missing_cols"
        ],
        "docstring": "Write SparkDF to Hive table with optional configuration.\n\nThis function writes data from a SparkDF into a Hive table, handling missing\ncolumns and optional repartitioning. It ensures the table's column order matches\nthe DataFrame and manages different overwrite behaviors for partitioned and\nnon-partitioned data.\n\nParameters\n----------\nspark\n    Active SparkSession.\ndf\n    SparkDF containing data to be written.\ntable_name\n    Name of the Hive table to write data into.\noverwrite\n    Controls how existing data is handled, default is False:\n\n    For non-partitioned data:\n    - True: Replaces entire table with DataFrame data.\n    - False: Appends DataFrame data to existing table.\n\n    For partitioned data:\n    - True: Replaces data only in partitions present in DataFrame.\n    - False: Appends data to existing partitions or creates new ones.\nfill_missing_cols\n    If True, adds missing columns as NULL values. If False, raises an error\n    on schema mismatch, default is False.\n\n    - Explicitly casts DataFrame columns to match the Hive table schema to\n      avoid type mismatch errors.\n    - Adds missing columns as NULL values when `fill_missing_cols` is True,\n      regardless of their data type (e.g., String, Integer, Double, Boolean, etc.).\nrepartition_data_by\n    Controls data repartitioning, default is None:\n    - int: Sets target number of partitions.\n    - str: Specifies column to repartition by.\n    - None: No repartitioning performed.\n\nNotes\n-----\nWhen using repartition with a number:\n- Affects physical file structure but preserves Hive partitioning scheme.\n- Controls number of output files per write operation per Hive partition.\n- Maintains partition-based query optimization.\n\nWhen repartitioning by column:\n- Helps balance file sizes across Hive partitions.\n- Reduces creation of small files.\n\nRaises\n------\nAnalysisException\n    If there's an error reading the table. This can occur if the table\n    doesn't exist or if there's no access to it.\nValueError\n    If the SparkDF schema does not match the Hive table schema and\n    'fill_missing_cols' is set to False.\nDataframeEmptyError\n    If input DataFrame is empty.\nException\n    For other general exceptions when writing data to the table.\n\nExamples\n--------\nWrite a DataFrame to a Hive table without overwriting:\n>>> insert_df_to_hive_table(\n...     spark=spark,\n...     df=df,\n...     table_name=\"my_database.my_table\"\n... )\n\nOverwrite an existing table with a DataFrame:\n>>> insert_df_to_hive_table(\n...     spark=spark,\n...     df=df,\n...     table_name=\"my_database.my_table\",\n...     overwrite=True\n... )\n\nWrite a DataFrame to a Hive table with missing columns filled:\n>>> insert_df_to_hive_table(\n...     spark=spark,\n...     df=df,\n...     table_name=\"my_database.my_table\",\n...     fill_missing_cols=True\n... )\n\nRepartition by column before writing to Hive:\n>>> insert_df_to_hive_table(\n...     spark=spark,\n...     df=df,\n...     table_name=\"my_database.my_table\",\n...     repartition_data_by=\"partition_column\"\n... )\n\nRepartition into a fixed number of partitions before writing:\n>>> insert_df_to_hive_table(\n...     spark=spark,\n...     df=df,\n...     table_name=\"my_database.my_table\",\n...     repartition_data_by=10\n... )"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\output.py",
        "function_name": "write_and_read_hive_table",
        "code_chunk": "def write_and_read_hive_table(\n    spark: SparkSession,\n    df: SparkDF,\n    table_name: str,\n    database: str,\n    filter_id: Union[int, str],\n    filter_col: str = \"run_id\",\n    fill_missing_cols: bool = False,\n) -> SparkDF:\n    \"\"\"Write a SparkDF to an existing Hive table and then read it back.\n\n    Parameters\n    ----------\n    spark\n        Active SparkSession.\n    df\n        The SparkDF to be written to the Hive table.\n    table_name\n        The name of the Hive table to write to and read from.\n    database\n        The Hive database name.\n    filter_id\n        The identifier to filter on when reading data back from the Hive table.\n    filter_col\n        The column name to use for filtering data when reading back from\n        the Hive table, by default 'run_id'.\n    fill_missing_cols\n        If True, missing columns in the DataFrame will be filled with nulls\n        when writing to the Hive table, by default False.\n\n    Returns\n    -------\n    SparkDF\n        The DataFrame read from the Hive table.\n\n    Raises\n    ------\n    ValueError\n        If the specified Hive table does not exist in the given database or\n        if the provided DataFrame doesn't contain the specified filter column.\n    Exception\n        For general exceptions encountered during execution.\n\n    Notes\n    -----\n    This function assumes the Hive table already exists. The DataFrame `df`\n    should have the same schema as the Hive table for the write to succeed.\n\n    The function allows for more effective memory management when dealing\n    with large PySpark DataFrames by leveraging Hive's on-disk storage.\n\n    Predicate pushdown is used when reading the data back into a PySpark\n    DataFrame, minimizing the memory usage and optimizing the read\n    operation.\n\n    As part of the design, there is always a column called filter_col in the\n    DataFrame and Hive table to track pipeline runs.\n\n    The Hive table contains all the runs, and we only read back the run that we\n    just wrote to the Hive Table using the `filter_id` parameter. If no\n    `filter_col` is specified, 'run_id' is used as default.\n    \"\"\"\n    try:\n        # Check for existence of the Hive table\n        if not spark.catalog.tableExists(database, table_name):\n            msg = f\"The specified Hive table {database}.{table_name} does not exist.\"\n            raise TableNotFoundError(\n                msg,\n            )\n\n        # Ensure the filter_col exists in DataFrame\n        if filter_col not in df.columns:\n            msg = (\n                \"The provided DataFrame doesn't contain the \"\n                f\"specified filter column: {filter_col}\"\n            )\n            raise ColumnNotInDataframeError(\n                msg,\n            )\n\n        # Write DataFrame to Hive using the helper function\n        insert_df_to_hive_table(\n            spark,\n            df,\n            f\"{database}.{table_name}\",\n            fill_missing_cols=fill_missing_cols,\n        )\n\n        # Read DataFrame back from Hive with filter condition\n        df_read = load_and_validate_table(\n            spark,\n            f\"{database}.{table_name}\",\n            skip_validation=False,\n            err_msg=None,\n            filter_cond=f\"{filter_col} = '{filter_id}'\",\n        )\n        return df_read\n\n    except Exception as e:\n        logger.error(f\"An error occurred: {e}\")\n        raise",
        "variables": [
            "df",
            "filter_col",
            "df_read",
            "filter_id",
            "spark",
            "msg",
            "table_name",
            "fill_missing_cols",
            "e",
            "database"
        ],
        "docstring": "Write a SparkDF to an existing Hive table and then read it back.\n\nParameters\n----------\nspark\n    Active SparkSession.\ndf\n    The SparkDF to be written to the Hive table.\ntable_name\n    The name of the Hive table to write to and read from.\ndatabase\n    The Hive database name.\nfilter_id\n    The identifier to filter on when reading data back from the Hive table.\nfilter_col\n    The column name to use for filtering data when reading back from\n    the Hive table, by default 'run_id'.\nfill_missing_cols\n    If True, missing columns in the DataFrame will be filled with nulls\n    when writing to the Hive table, by default False.\n\nReturns\n-------\nSparkDF\n    The DataFrame read from the Hive table.\n\nRaises\n------\nValueError\n    If the specified Hive table does not exist in the given database or\n    if the provided DataFrame doesn't contain the specified filter column.\nException\n    For general exceptions encountered during execution.\n\nNotes\n-----\nThis function assumes the Hive table already exists. The DataFrame `df`\nshould have the same schema as the Hive table for the write to succeed.\n\nThe function allows for more effective memory management when dealing\nwith large PySpark DataFrames by leveraging Hive's on-disk storage.\n\nPredicate pushdown is used when reading the data back into a PySpark\nDataFrame, minimizing the memory usage and optimizing the read\noperation.\n\nAs part of the design, there is always a column called filter_col in the\nDataFrame and Hive table to track pipeline runs.\n\nThe Hive table contains all the runs, and we only read back the run that we\njust wrote to the Hive Table using the `filter_id` parameter. If no\n`filter_col` is specified, 'run_id' is used as default."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\output.py",
        "function_name": "save_csv_to_hdfs",
        "code_chunk": "def save_csv_to_hdfs(\n    df: SparkDF,\n    file_name: str,\n    file_path: str,\n    overwrite: bool = True,\n) -> None:\n    \"\"\"Save DataFrame as CSV on HDFS, coalescing to a single partition.\n\n    This function saves a PySpark DataFrame to HDFS in CSV format. By\n    coalescing the DataFrame into a single partition before saving, it\n    accomplishes two main objectives:\n\n    1. Single Part File: The output is a single CSV file rather than\n       multiple part files. This method reduces complexity and\n       cuts through the clutter of multi-part files, offering users\n       and systems a more cohesive and hassle-free experience.\n\n    2. Preserving Row Order: Coalescing into a single partition maintains\n       the order of rows as they appear in the DataFrame. This is essential\n       when the row order matters for subsequent processing or analysis.\n       It's important to note, however, that coalescing can have performance\n       implications for very large DataFrames by concentrating\n       all data processing on a single node.\n\n    Parameters\n    ----------\n    df\n        PySpark DataFrame to be saved.\n    file_name\n        Name of the CSV file. Must include the \".csv\" extension.\n    file_path\n        HDFS path where the CSV file should be saved.\n    overwrite\n        If True, overwrite any existing file with the same name. If False\n        and the file exists, the function will raise an error.\n\n    Raises\n    ------\n    ValueError\n        If the file_name does not end with \".csv\".\n    IOError\n        If overwrite is False and the target file already exists.\n\n    Examples\n    --------\n    Saving to an S3 bucket using the `s3a://` scheme:\n\n    ```python\n    # Assume `df` is a pre-defined PySpark DataFrame\n    file_name = \"data_output.csv\"\n    file_path = \"s3a://my-bucket/data_folder/\"\n    save_csv_to_hdfs(df, file_name, file_path, overwrite=True)\n    ```\n\n    Saving to a normal HDFS path:\n\n    ```python\n    # Assume `df` is a pre-defined PySpark DataFrame\n    file_name = \"data_output.csv\"\n    file_path = \"/user/hdfs/data_folder/\"\n    save_csv_to_hdfs(df, file_name, file_path, overwrite=True)\n    ```\n    \"\"\"\n    if not file_name.endswith(\".csv\"):\n        error_msg = \"The file_name must end with '.csv' extension.\"\n        raise ValueError(error_msg)\n\n    destination_path = f\"{file_path.rstrip('/')}/{file_name}\"\n\n    if not overwrite and file_exists(destination_path):\n        error_msg = (\n            f\"File '{destination_path}' already exists \"\n            \"and overwrite is set to False.\"\n        )\n        raise IOError(error_msg)\n\n    logger.info(f\"Saving DataFrame to {file_name} in HDFS at {file_path}\")\n\n    # Coalesce the DataFrame to a single partition\n    df = df.coalesce(1)\n\n    # Temporary path for saving the single part file\n    temp_path = f\"{file_path.rstrip('/')}/temp_{file_name}\"\n\n    # Save the DataFrame to HDFS in CSV format in a temporary directory\n    df.write.csv(temp_path, header=True, mode=\"overwrite\")\n\n    # Identify the part file: pattern matching for the single part file\n    part_file = f\"{temp_path}/part-00000*.csv\"\n\n    # Rename the part file to the final file name\n    if not rename(part_file, destination_path, overwrite):\n        error_msg = f\"Failed to rename the part file to '{destination_path}'\"\n        raise IOError(error_msg)\n\n    logger.info(f\"DataFrame successfully saved to {destination_path}\")\n\n    # Clean up the temporary directory\n    delete_path(temp_path)\n    logger.info(f\"Temporary directory {temp_path} deleted\")",
        "variables": [
            "temp_path",
            "error_msg",
            "df",
            "overwrite",
            "destination_path",
            "file_name",
            "file_path",
            "part_file"
        ],
        "docstring": "Save DataFrame as CSV on HDFS, coalescing to a single partition.\n\nThis function saves a PySpark DataFrame to HDFS in CSV format. By\ncoalescing the DataFrame into a single partition before saving, it\naccomplishes two main objectives:\n\n1. Single Part File: The output is a single CSV file rather than\n   multiple part files. This method reduces complexity and\n   cuts through the clutter of multi-part files, offering users\n   and systems a more cohesive and hassle-free experience.\n\n2. Preserving Row Order: Coalescing into a single partition maintains\n   the order of rows as they appear in the DataFrame. This is essential\n   when the row order matters for subsequent processing or analysis.\n   It's important to note, however, that coalescing can have performance\n   implications for very large DataFrames by concentrating\n   all data processing on a single node.\n\nParameters\n----------\ndf\n    PySpark DataFrame to be saved.\nfile_name\n    Name of the CSV file. Must include the \".csv\" extension.\nfile_path\n    HDFS path where the CSV file should be saved.\noverwrite\n    If True, overwrite any existing file with the same name. If False\n    and the file exists, the function will raise an error.\n\nRaises\n------\nValueError\n    If the file_name does not end with \".csv\".\nIOError\n    If overwrite is False and the target file already exists.\n\nExamples\n--------\nSaving to an S3 bucket using the `s3a://` scheme:\n\n```python\n# Assume `df` is a pre-defined PySpark DataFrame\nfile_name = \"data_output.csv\"\nfile_path = \"s3a://my-bucket/data_folder/\"\nsave_csv_to_hdfs(df, file_name, file_path, overwrite=True)\n```\n\nSaving to a normal HDFS path:\n\n```python\n# Assume `df` is a pre-defined PySpark DataFrame\nfile_name = \"data_output.csv\"\nfile_path = \"/user/hdfs/data_folder/\"\nsave_csv_to_hdfs(df, file_name, file_path, overwrite=True)\n```"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\output.py",
        "function_name": "save_csv_to_s3",
        "code_chunk": "def save_csv_to_s3(\n    df: SparkDF,\n    bucket_name: str,\n    file_name: str,\n    file_path: str,\n    s3_client: boto3.client,\n    overwrite: bool = True,\n) -> None:\n    \"\"\"Save DataFrame as CSV on S3, coalescing to a single partition.\n\n    This function saves a PySpark DataFrame to S3 in CSV format. By\n    coalescing the DataFrame into a single partition before saving, it\n    accomplishes two main objectives:\n\n    1. Single Part File: The output is a single CSV file rather than\n       multiple part files. This method reduces complexity and\n       cuts through the clutter of multi-part files, offering users\n       and systems a more cohesive and hassle-free experience.\n\n    2. Preserving Row Order: Coalescing into a single partition maintains\n       the order of rows as they appear in the DataFrame. This is essential\n       when the row order matters for subsequent processing or analysis.\n       It's important to note, however, that coalescing can have performance\n       implications for very large DataFrames by concentrating\n       all data processing on a single node.\n\n    Parameters\n    ----------\n    df\n        PySpark DataFrame to be saved.\n    bucket_name\n        The name of the S3 bucket where the CSV file should be saved.\n    file_name\n        Name of the CSV file. Must include the \".csv\" extension.\n    file_path\n        S3 path where the CSV file should be saved.\n    s3_client\n        The boto3 S3 client instance.\n    overwrite\n        If True, overwrite any existing file with the same name. If False\n        and the file exists, the function will raise an error.\n\n    Raises\n    ------\n    ValueError\n        If the file_name does not end with \".csv\".\n    InvalidBucketNameError\n        If the bucket name does not meet AWS specifications.\n    InvalidS3FilePathError\n        If the file_path contains an S3 URI scheme like 's3://' or 's3a://'.\n    IOError\n        If overwrite is False and the target file already exists.\n\n    Examples\n    --------\n    Saving to an S3 bucket:\n\n    ```python\n    # Assume `df` is a pre-defined PySpark DataFrame\n    file_name = \"data_output.csv\"\n    file_path = \"data_folder/\"\n    s3_client = boto3.client('s3')\n    save_csv_to_s3(\n        df,\n        'my-bucket',\n        file_name,\n        file_path,\n        s3_client,\n        overwrite=True\n    )\n    ```\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    file_path = validate_s3_file_path(file_path, allow_s3_scheme=False)\n    file_path = remove_leading_slash(file_path)\n\n    if not file_name.endswith(\".csv\"):\n        error_msg = \"The file_name must end with '.csv' extension.\"\n        raise ValueError(error_msg)\n\n    destination_path = f\"{file_path.rstrip('/')}/{file_name}\"\n\n    if not overwrite and file_exists(s3_client, bucket_name, destination_path):\n        error_msg = (\n            f\"File '{destination_path}' already exists \"\n            \"and overwrite is set to False.\"\n        )\n        raise IOError(error_msg)\n\n    logger.info(\n        f\"Saving DataFrame to {file_name} in S3 at s3://{bucket_name}/{file_path}\",\n    )\n\n    # Coalesce the DataFrame to a single partition\n    df = df.coalesce(1)\n\n    # Temporary S3 path for saving the single part file\n    temp_path = f\"{file_path.rstrip('/')}/temp_{uuid.uuid4().hex}_{file_name}\"\n\n    # Save the DataFrame to S3 in CSV format in a temporary directory\n    df.write.csv(\n        f\"s3a://{bucket_name}/{temp_path}\",\n        header=True,\n        mode=\"overwrite\",\n    )\n\n    # Identify the part file using the list_files helper function\n    part_file_prefix = f\"{temp_path}/part-00000\"\n    part_files = list_files(s3_client, bucket_name, part_file_prefix)\n    if not part_files:\n        error_msg = \"No part files found in the temporary directory.\"\n        raise IOError(error_msg)\n\n    # Get the first part file from the list\n    # Since the DataFrame is coalesced to a single partition, there should\n    # only be one part file\n    part_file_key = part_files[0]\n\n    # Rename the part file to the final file name\n    if not copy_file(\n        s3_client,\n        bucket_name,\n        part_file_key,\n        bucket_name,\n        destination_path,\n        overwrite,\n    ):\n        error_msg = f\"Failed to rename the part file to '{destination_path}'\"\n        raise IOError(error_msg)\n\n    logger.info(\n        f\"DataFrame successfully saved to s3://{bucket_name}/{destination_path}\",\n    )\n\n    # Clean up the temporary directory\n    delete_folder(s3_client, bucket_name, temp_path)",
        "variables": [
            "temp_path",
            "error_msg",
            "df",
            "overwrite",
            "part_file_prefix",
            "destination_path",
            "file_name",
            "part_file_key",
            "file_path",
            "bucket_name",
            "part_files",
            "s3_client"
        ],
        "docstring": "Save DataFrame as CSV on S3, coalescing to a single partition.\n\nThis function saves a PySpark DataFrame to S3 in CSV format. By\ncoalescing the DataFrame into a single partition before saving, it\naccomplishes two main objectives:\n\n1. Single Part File: The output is a single CSV file rather than\n   multiple part files. This method reduces complexity and\n   cuts through the clutter of multi-part files, offering users\n   and systems a more cohesive and hassle-free experience.\n\n2. Preserving Row Order: Coalescing into a single partition maintains\n   the order of rows as they appear in the DataFrame. This is essential\n   when the row order matters for subsequent processing or analysis.\n   It's important to note, however, that coalescing can have performance\n   implications for very large DataFrames by concentrating\n   all data processing on a single node.\n\nParameters\n----------\ndf\n    PySpark DataFrame to be saved.\nbucket_name\n    The name of the S3 bucket where the CSV file should be saved.\nfile_name\n    Name of the CSV file. Must include the \".csv\" extension.\nfile_path\n    S3 path where the CSV file should be saved.\ns3_client\n    The boto3 S3 client instance.\noverwrite\n    If True, overwrite any existing file with the same name. If False\n    and the file exists, the function will raise an error.\n\nRaises\n------\nValueError\n    If the file_name does not end with \".csv\".\nInvalidBucketNameError\n    If the bucket name does not meet AWS specifications.\nInvalidS3FilePathError\n    If the file_path contains an S3 URI scheme like 's3://' or 's3a://'.\nIOError\n    If overwrite is False and the target file already exists.\n\nExamples\n--------\nSaving to an S3 bucket:\n\n```python\n# Assume `df` is a pre-defined PySpark DataFrame\nfile_name = \"data_output.csv\"\nfile_path = \"data_folder/\"\ns3_client = boto3.client('s3')\nsave_csv_to_s3(\n    df,\n    'my-bucket',\n    file_name,\n    file_path,\n    s3_client,\n    overwrite=True\n)\n```"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\pipeline_runlog.py",
        "function_name": "_write_entry",
        "code_chunk": "def _write_entry(entry_df: DataFrame, log_table: str) -> None:\n    \"\"\"Write a DataFrame entry into a specified table.\n\n    Parameters\n    ----------\n    entry_df\n        The DataFrame containing the data to be written to the table.\n    log_table\n        The name of the table into which the data should be written.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    try:\n        entry_df.write.insertInto(log_table)\n    except AnalysisException as e:\n        logger.error(f\"Error writing entry to table {log_table}: {e}\")",
        "variables": [
            "entry_df",
            "log_table",
            "e"
        ],
        "docstring": "Write a DataFrame entry into a specified table.\n\nParameters\n----------\nentry_df\n    The DataFrame containing the data to be written to the table.\nlog_table\n    The name of the table into which the data should be written.\n\nReturns\n-------\nNone"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\pipeline_runlog.py",
        "function_name": "create_runlog_table",
        "code_chunk": "def create_runlog_table(\n    spark: SparkSession,\n    database: str,\n    tablename: Optional[str] = \"pipeline_runlog\",\n) -> None:\n    \"\"\"Create runlog and _reserved_ids tables in the target database if needed.\n\n    This function executes two SQL queries to create two tables, if they do\n    not already exist in the target database. The first table's structure\n    includes columns for run_id, desc, user, datetime, pipeline_name,\n    pipeline_version, and config, while the second table includes run_id and\n    reserved_date.\n\n    Parameters\n    ----------\n    spark\n        A running spark session which will be used to execute SQL queries.\n    database\n        The name of the target database where tables will be created.\n    tablename\n        The name of the main table to be created (default is \"pipeline_runlog\").\n        The associated _reserved_ids table will be suffixed with this name.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    >>> spark = SparkSession.builder.appName(\"test_session\").getOrCreate()\n    >>> create_runlog_table(spark, \"test_db\", \"test_table\")\n    \"\"\"\n    # Create the main table if it does not exist\n    runlog_sql_str = f\"\"\"\n        CREATE TABLE IF NOT EXISTS {database}.{tablename} (\n            run_id int,\n            desc string,\n            user string,\n            datetime timestamp,\n            pipeline_name string,\n            pipeline_version string,\n            config string\n        )\n        STORED AS parquet\n    \"\"\"\n    spark.sql(runlog_sql_str)\n\n    # Create the _reserved_ids table if it does not exist\n    reserved_ids_sql_str = f\"\"\"\n        CREATE TABLE IF NOT EXISTS {database}.{tablename}_reserved_ids (\n            run_id int,\n            reserved_date timestamp\n        )\n        STORED AS parquet\n    \"\"\"\n    spark.sql(reserved_ids_sql_str)",
        "variables": [
            "reserved_ids_sql_str",
            "runlog_sql_str",
            "tablename",
            "spark",
            "database"
        ],
        "docstring": "Create runlog and _reserved_ids tables in the target database if needed.\n\nThis function executes two SQL queries to create two tables, if they do\nnot already exist in the target database. The first table's structure\nincludes columns for run_id, desc, user, datetime, pipeline_name,\npipeline_version, and config, while the second table includes run_id and\nreserved_date.\n\nParameters\n----------\nspark\n    A running spark session which will be used to execute SQL queries.\ndatabase\n    The name of the target database where tables will be created.\ntablename\n    The name of the main table to be created (default is \"pipeline_runlog\").\n    The associated _reserved_ids table will be suffixed with this name.\n\nReturns\n-------\nNone\n\nExamples\n--------\n>>> spark = SparkSession.builder.appName(\"test_session\").getOrCreate()\n>>> create_runlog_table(spark, \"test_db\", \"test_table\")"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\pipeline_runlog.py",
        "function_name": "reserve_id",
        "code_chunk": "def reserve_id(\n    spark: SparkSession,\n    log_table: Optional[str] = \"pipeline_runlog\",\n) -> int:\n    \"\"\"Reserve a run id in the reserved ids table linked to the runlog table.\n\n    The function reads the last run id from the reserved ids table,\n    increments it to create a new id,and writes the new id with the\n    current timestamp to the reserved ids table.\n\n    Parameters\n    ----------\n    spark\n        A running SparkSession instance.\n    log_table\n        The name of the main pipeline runlog table associated with\n        this reserved id table, by default \"pipeline_runlog\".\n\n    Returns\n    -------\n    int\n        The new run id.\n    \"\"\"\n    current_time = datetime.now()  # noqa: DTZ005\n\n    last_id = (\n        spark.read.table(f\"{log_table}_reserved_ids\").select(F.max(\"run_id\")).first()[0]\n    )\n\n    new_id = last_id + 1 if last_id else 1\n\n    new_entry = [(new_id, current_time)]\n    df = spark.createDataFrame(new_entry, \"run_id INT, reserved_date TIMESTAMP\")\n\n    _write_entry(df, f\"{log_table}_reserved_ids\")\n\n    return new_id",
        "variables": [
            "new_entry",
            "df",
            "current_time",
            "last_id",
            "log_table",
            "spark",
            "new_id"
        ],
        "docstring": "Reserve a run id in the reserved ids table linked to the runlog table.\n\nThe function reads the last run id from the reserved ids table,\nincrements it to create a new id,and writes the new id with the\ncurrent timestamp to the reserved ids table.\n\nParameters\n----------\nspark\n    A running SparkSession instance.\nlog_table\n    The name of the main pipeline runlog table associated with\n    this reserved id table, by default \"pipeline_runlog\".\n\nReturns\n-------\nint\n    The new run id."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\pipeline_runlog.py",
        "function_name": "_get_run_ids",
        "code_chunk": "def _get_run_ids(\n    spark: SparkSession,\n    limit: int,\n    pipeline: Optional[str] = None,\n    log_table: str = \"pipeline_runlog\",\n) -> List[int]:\n    \"\"\"Retrieve the most recent run ids.\n\n    Parameters\n    ----------\n    spark\n        A running Spark session.\n    limit\n        The number of recent run ids to retrieve.\n    pipeline\n        If specified, the result will be for the listed pipeline only.\n    log_table\n        The target runlog table. If the database is not set, this should\n        include the database.\n\n    Returns\n    -------\n    list\n        List of the most recent run ids. Returns an empty list\n        if the log table is empty.\n    \"\"\"\n    log = spark.read.table(log_table)\n\n    if pipeline:\n        log = log.filter(log.pipeline_name == pipeline)\n\n    result = (\n        log.orderBy(\"datetime\", ascending=False).select(\"run_id\").limit(limit).collect()\n    )\n\n    return [row[0] for row in result]",
        "variables": [
            "limit",
            "log_table",
            "result",
            "log",
            "row",
            "spark",
            "pipeline"
        ],
        "docstring": "Retrieve the most recent run ids.\n\nParameters\n----------\nspark\n    A running Spark session.\nlimit\n    The number of recent run ids to retrieve.\npipeline\n    If specified, the result will be for the listed pipeline only.\nlog_table\n    The target runlog table. If the database is not set, this should\n    include the database.\n\nReturns\n-------\nlist\n    List of the most recent run ids. Returns an empty list\n    if the log table is empty."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\pipeline_runlog.py",
        "function_name": "get_last_run_id",
        "code_chunk": "def get_last_run_id(\n    spark: SparkSession,\n    pipeline: Optional[str] = None,\n    log_table: str = \"pipeline_runlog\",\n) -> Optional[int]:\n    \"\"\"Retrieve the last run_id, either in general or for a specific pipeline.\n\n    Parameters\n    ----------\n    spark\n        A running Spark session.\n    pipeline\n        If specified, the result will be for the listed pipeline only.\n    log_table\n        The target runlog table. If the database is not set, this should\n        include the database.\n\n    Returns\n    -------\n    int or None\n        The id of the last run. Returns None if the log table is empty.\n    \"\"\"\n    result = _get_run_ids(spark, 1, pipeline, log_table)\n\n    if result:\n        return result[0]\n\n    return None",
        "variables": [
            "log_table",
            "pipeline",
            "spark",
            "result"
        ],
        "docstring": "Retrieve the last run_id, either in general or for a specific pipeline.\n\nParameters\n----------\nspark\n    A running Spark session.\npipeline\n    If specified, the result will be for the listed pipeline only.\nlog_table\n    The target runlog table. If the database is not set, this should\n    include the database.\n\nReturns\n-------\nint or None\n    The id of the last run. Returns None if the log table is empty."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\pipeline_runlog.py",
        "function_name": "get_penultimate_run_id",
        "code_chunk": "def get_penultimate_run_id(\n    spark: SparkSession,\n    pipeline: Optional[str] = None,\n    log_table: str = \"pipeline_runlog\",\n) -> Optional[int]:\n    \"\"\"Retrieve penultimate run_id in general or a specific pipeline.\n\n    Parameters\n    ----------\n    spark\n        A running Spark session.\n    pipeline\n        If specified, the result will be for the listed pipeline only.\n    log_table\n        The target runlog table. If the database is not set, this should\n        include the database.\n\n    Returns\n    -------\n    int or None\n        The id of the penultimate run. Returns None if the log table is empty\n        or has less than two entries.\n    \"\"\"\n    result = _get_run_ids(spark, 2, pipeline, log_table)\n\n    if len(result) > 1:\n        return result[1]\n\n    return None",
        "variables": [
            "log_table",
            "pipeline",
            "spark",
            "result"
        ],
        "docstring": "Retrieve penultimate run_id in general or a specific pipeline.\n\nParameters\n----------\nspark\n    A running Spark session.\npipeline\n    If specified, the result will be for the listed pipeline only.\nlog_table\n    The target runlog table. If the database is not set, this should\n    include the database.\n\nReturns\n-------\nint or None\n    The id of the penultimate run. Returns None if the log table is empty\n    or has less than two entries."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\pipeline_runlog.py",
        "function_name": "create_runlog_entry",
        "code_chunk": "def create_runlog_entry(\n    spark: SparkSession,\n    run_id: int,\n    desc: str,\n    version: str,\n    config: Union[ConfigParser, Dict[str, str]],\n    pipeline: Optional[str] = None,\n) -> DataFrame:\n    \"\"\"Create an entry for the runlog.\n\n    Parameters\n    ----------\n    spark\n        A running spark session.\n    run_id\n        Entry run id.\n    desc\n        Description to attach to the log entry.\n    version\n        Version of the pipeline.\n    config\n        Configuration object for the run.\n    pipeline\n        Pipeline name. If None, derives from spark app name.\n\n    Returns\n    -------\n    DataFrame\n        The log entry returned as a spark dataframe.\n    \"\"\"\n    cols = [\n        \"run_id\",\n        \"desc\",\n        \"user\",\n        \"datetime\",\n        \"pipeline_name\",\n        \"pipeline_version\",\n        \"config\",\n    ]\n    user = os.getenv(\"HADOOP_USER_NAME\", \"unknown\")\n\n    if not pipeline:\n        pipeline = spark.sparkContext.appName\n\n    timestamp = datetime.now()  # noqa: DTZ005\n\n    try:\n        conf = json.dumps(\n            config._sections if isinstance(config, ConfigParser) else config,\n        )\n    except Exception as err:\n        msg = (\n            \"Problem converting config object. \"\n            \"Either use a ConfigParser object or \"\n            \"something that can be encoded with json.dumps\"\n        )\n        raise ValueError(msg) from err\n\n    entry = [(run_id, desc, user, timestamp, pipeline, version, conf)]\n    return spark.createDataFrame(entry, cols)",
        "variables": [
            "conf",
            "version",
            "timestamp",
            "err",
            "entry",
            "user",
            "desc",
            "spark",
            "msg",
            "pipeline",
            "cols",
            "run_id",
            "config"
        ],
        "docstring": "Create an entry for the runlog.\n\nParameters\n----------\nspark\n    A running spark session.\nrun_id\n    Entry run id.\ndesc\n    Description to attach to the log entry.\nversion\n    Version of the pipeline.\nconfig\n    Configuration object for the run.\npipeline\n    Pipeline name. If None, derives from spark app name.\n\nReturns\n-------\nDataFrame\n    The log entry returned as a spark dataframe."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\pipeline_runlog.py",
        "function_name": "add_runlog_entry",
        "code_chunk": "def add_runlog_entry(\n    spark: SparkSession,\n    desc: str,\n    version: str,\n    config: Union[ConfigParser, Dict[str, str]],\n    pipeline: Optional[str] = None,\n    log_table: str = \"pipeline_runlog\",\n    run_id: Optional[int] = None,\n) -> DataFrame:\n    \"\"\"Add an entry to a target runlog.\n\n    Parameters\n    ----------\n    spark\n        A running spark session.\n    desc\n        Description to attach to the log entry.\n    version\n        Version of the pipeline.\n    config\n        Configuration object for the run.\n    pipeline\n        Pipeline name. If None, uses the spark application name.\n    log_table\n        Target runlog table. If database not set, this should\n        include the database.\n    run_id\n        Run id to use if already reserved. If not specified,\n        a new one is generated.\n\n    Returns\n    -------\n    DataFrame\n        The log entry returned as a spark dataframe.\n    \"\"\"\n    if not run_id:\n        run_id = reserve_id(spark, log_table)\n\n    entry = create_runlog_entry(spark, run_id, desc, version, config, pipeline)\n    _write_entry(entry, log_table)\n    return entry",
        "variables": [
            "run_id",
            "log_table",
            "entry",
            "desc",
            "spark",
            "pipeline",
            "version",
            "config"
        ],
        "docstring": "Add an entry to a target runlog.\n\nParameters\n----------\nspark\n    A running spark session.\ndesc\n    Description to attach to the log entry.\nversion\n    Version of the pipeline.\nconfig\n    Configuration object for the run.\npipeline\n    Pipeline name. If None, uses the spark application name.\nlog_table\n    Target runlog table. If database not set, this should\n    include the database.\nrun_id\n    Run id to use if already reserved. If not specified,\n    a new one is generated.\n\nReturns\n-------\nDataFrame\n    The log entry returned as a spark dataframe."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\pipeline_runlog.py",
        "function_name": "_parse_runlog_as_string",
        "code_chunk": "def _parse_runlog_as_string(\n    spark: SparkSession,\n    runlog_table: str,\n    runlog_id: int,\n) -> str:\n    \"\"\"Parse a single runlog entry as a string.\n\n    Parameters\n    ----------\n    spark\n        A running spark session.\n    runlog_table\n        Table containing the runlog entries.\n    runlog_id\n        ID of the desired entry.\n\n    Returns\n    -------\n    str\n        Parsed runlog entry.\n    \"\"\"\n    query = f\"SELECT * FROM {runlog_table} WHERE run_id = {runlog_id}\"\n    df = spark.sql(query)\n\n    config = literal_eval(df.select(\"config\").first()[0])\n\n    meta = \"\\n\".join(\n        f\"{col}: {df.select(col).first()[0]}\" for col in df.drop(\"config\").columns\n    )\n    config_str = \"\\n\\n\".join(\n        f\"{k.replace('_', ' ').title()}:\\n\\n\"\n        + \"\\n\".join(f\"{key}: {value}\" for key, value in v.items())\n        for k, v in config.items()\n    )\n\n    return f\"Metadata:\\n\\n{meta}\\n\\n{config_str}\\n\".replace(\"$\", \"\")",
        "variables": [
            "runlog_table",
            "df",
            "k",
            "col",
            "query",
            "value",
            "spark",
            "runlog_id",
            "key",
            "config_str",
            "meta",
            "v",
            "config"
        ],
        "docstring": "Parse a single runlog entry as a string.\n\nParameters\n----------\nspark\n    A running spark session.\nrunlog_table\n    Table containing the runlog entries.\nrunlog_id\n    ID of the desired entry.\n\nReturns\n-------\nstr\n    Parsed runlog entry."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\cdp\\io\\pipeline_runlog.py",
        "function_name": "write_runlog_file",
        "code_chunk": "def write_runlog_file(\n    spark: SparkSession,\n    runlog_table: str,\n    runlog_id: int,\n    path: str,\n) -> None:\n    \"\"\"Write metadata from runlog entry to a text file.\n\n    Parameters\n    ----------\n    spark\n        A running SparkSession instance.\n    runlog_table\n        The name of the table containing the runlog entries.\n    runlog_id\n        The id of the desired entry.\n    path\n        The HDFS path where the file will be written.\n\n    Returns\n    -------\n    None\n        This function doesn't return anything; it's used for its\n        side effect of creating a text file.\n    \"\"\"\n    string_to_write = _parse_runlog_as_string(spark, runlog_table, runlog_id)\n    create_txt_from_string(path, string_to_write)",
        "variables": [
            "runlog_table",
            "string_to_write",
            "path",
            "spark",
            "runlog_id"
        ],
        "docstring": "Write metadata from runlog entry to a text file.\n\nParameters\n----------\nspark\n    A running SparkSession instance.\nrunlog_table\n    The name of the table containing the runlog entries.\nrunlog_id\n    The id of the desired entry.\npath\n    The HDFS path where the file will be written.\n\nReturns\n-------\nNone\n    This function doesn't return anything; it's used for its\n    side effect of creating a text file."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "run_bq_query",
        "code_chunk": "def run_bq_query(query: str) -> bigquery.QueryJob:\n    \"\"\"Run an SQL query in BigQuery.\"\"\"\n    return bigquery.Client().query(query)",
        "variables": [
            "query"
        ],
        "docstring": "Run an SQL query in BigQuery."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "get_table_columns",
        "code_chunk": "def get_table_columns(table_path) -> List[str]:\n    \"\"\"Return the column names for given bigquery table.\"\"\"\n    client = bigquery.Client()\n\n    table = client.get_table(table_path)\n    return [column.name for column in table.schema]",
        "variables": [
            "client",
            "table_path",
            "column",
            "table"
        ],
        "docstring": "Return the column names for given bigquery table."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "table_exists",
        "code_chunk": "def table_exists(table_path: TablePath) -> bool:\n    \"\"\"Check the big query catalogue to see if a table exists.\n\n    Returns True if a table exists.\n    See code sample explanation here:\n    https://cloud.google.com/bigquery/docs/samples/bigquery-table-exists#bigquery_table_exists-python\n\n    Parameters\n    ----------\n    table_path\n        The target BigQuery table name of form:\n        <project_id>.<database>.<table_name>\n\n    Returns\n    -------\n    bool\n        Returns True if table exists and False if table does not exist.\n    \"\"\"\n    try:\n        bigquery.Client().get_table(table_path)\n        table_exists = True\n        logger.debug(f\"Table {table_path} exists.\")\n\n    except NotFound:\n        table_exists = False\n        logger.warning(f\"Table {table_path} not found.\")\n\n    return table_exists",
        "variables": [
            "table_exists",
            "table_path"
        ],
        "docstring": "Check the big query catalogue to see if a table exists.\n\nReturns True if a table exists.\nSee code sample explanation here:\nhttps://cloud.google.com/bigquery/docs/samples/bigquery-table-exists#bigquery_table_exists-python\n\nParameters\n----------\ntable_path\n    The target BigQuery table name of form:\n    <project_id>.<database>.<table_name>\n\nReturns\n-------\nbool\n    Returns True if table exists and False if table does not exist."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "load_config_gcp",
        "code_chunk": "def load_config_gcp(config_path: str) -> Tuple[Dict, Dict]:\n    \"\"\"Load the config and dev_config files to dictionaries.\n\n    Parameters\n    ----------\n    config_path\n        The path of the config file in a yaml format.\n\n    Returns\n    -------\n    Tuple[Dict, Dict]\n        The contents of the config files.\n    \"\"\"\n    logger.info(f\"\"\"loading config from file: {config_path}\"\"\")\n\n    storage_client = storage.Client()\n\n    bucket_name = config_path.split(\"//\")[1].split(\"/\")[0]\n    blob_name = \"/\".join(config_path.split(\"//\")[1].split(\"/\")[1:])\n\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    contents = blob.download_as_string()\n\n    config_file = yaml.safe_load(contents)\n\n    logger.info(json.dumps(config_file, indent=4))\n    return config_file",
        "variables": [
            "storage_client",
            "bucket",
            "config_path",
            "config_file",
            "blob_name",
            "bucket_name",
            "contents",
            "blob"
        ],
        "docstring": "Load the config and dev_config files to dictionaries.\n\nParameters\n----------\nconfig_path\n    The path of the config file in a yaml format.\n\nReturns\n-------\nTuple[Dict, Dict]\n    The contents of the config files."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "remove_leading_slash",
        "code_chunk": "def remove_leading_slash(text: str) -> str:\n    \"\"\"Remove the leading forward slash from a string if present.\n\n    Parameters\n    ----------\n    text\n        The text from which the leading slash will be removed.\n\n    Returns\n    -------\n    str\n        The text stripped of its leading slash.\n\n    Examples\n    --------\n    >>> remove_leading_slash('/example/path')\n    'example/path'\n    \"\"\"\n    return text.lstrip(\"/\")",
        "variables": [
            "text"
        ],
        "docstring": "Remove the leading forward slash from a string if present.\n\nParameters\n----------\ntext\n    The text from which the leading slash will be removed.\n\nReturns\n-------\nstr\n    The text stripped of its leading slash.\n\nExamples\n--------\n>>> remove_leading_slash('/example/path')\n'example/path'"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "validate_bucket_name",
        "code_chunk": "def validate_bucket_name(bucket_name: str) -> str:\n    \"\"\"Validate the format of a GCS bucket name according to GCS rules.\n\n    Parameters\n    ----------\n    bucket_name\n        The name of the bucket to validate.\n\n    Returns\n    -------\n    str\n        The validated bucket name if valid.\n\n    Raises\n    ------\n    InvalidBucketNameError\n        If the bucket name does not meet GCS specifications.\n\n    Examples\n    --------\n    >>> validate_bucket_name('valid-bucket-name')\n    'valid-bucket-name'\n\n    >>> validate_bucket_name('Invalid_Bucket_Name')\n    InvalidBucketNameError: Bucket name must not contain underscores.\n    \"\"\"\n    # Bucket name must be between 3 and 63 characters long\n    if len(bucket_name) < 3 or len(bucket_name) > 63:\n        error_msg = \"Bucket name must be between 3 and 63 characters long.\"\n        raise InvalidBucketNameError(error_msg)\n\n    # Bucket name must not contain uppercase letters\n    if bucket_name != bucket_name.lower():\n        error_msg = \"Bucket name must not contain uppercase letters.\"\n        raise InvalidBucketNameError(error_msg)\n\n    # Bucket name must not contain underscores\n    if \"_\" in bucket_name:\n        error_msg = \"Bucket name must not contain underscores.\"\n        raise InvalidBucketNameError(error_msg)\n\n    # Bucket name must start and end with a lowercase letter or number\n    if not bucket_name[0].isalnum() or not bucket_name[-1].isalnum():\n        error_msg = \"Bucket name must start and end with a lowercase letter or number.\"\n        raise InvalidBucketNameError(error_msg)\n\n    # Bucket name must not contain forward slashes\n    if \"/\" in bucket_name:\n        error_msg = \"Bucket name must not contain forward slashes.\"\n        raise InvalidBucketNameError(error_msg)\n\n    return bucket_name",
        "variables": [
            "bucket_name",
            "error_msg"
        ],
        "docstring": "Validate the format of a GCS bucket name according to GCS rules.\n\nParameters\n----------\nbucket_name\n    The name of the bucket to validate.\n\nReturns\n-------\nstr\n    The validated bucket name if valid.\n\nRaises\n------\nInvalidBucketNameError\n    If the bucket name does not meet GCS specifications.\n\nExamples\n--------\n>>> validate_bucket_name('valid-bucket-name')\n'valid-bucket-name'\n\n>>> validate_bucket_name('Invalid_Bucket_Name')\nInvalidBucketNameError: Bucket name must not contain underscores."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "is_gcs_directory",
        "code_chunk": "def is_gcs_directory(\n    client: storage.Client,\n    bucket_name: str,\n    object_name: str,\n) -> bool:\n    \"\"\"Check if a GCS key is a directory by listing its contents.\n\n    Parameters\n    ----------\n    client\n        The GCS client instance.\n    bucket_name\n        The name of the GCS bucket.\n    object_name\n        The GCS object name to check.\n\n    Returns\n    -------\n    bool\n        True if the key represents a directory, False otherwise.\n    \"\"\"\n    bucket = client.bucket(bucket_name)\n    if not object_name.endswith(\"/\"):\n        object_name += \"/\"\n    blobs = list(client.list_blobs(bucket, prefix=object_name, max_results=1))\n    return len(blobs) > 0",
        "variables": [
            "bucket",
            "blobs",
            "client",
            "object_name",
            "bucket_name"
        ],
        "docstring": "Check if a GCS key is a directory by listing its contents.\n\nParameters\n----------\nclient\n    The GCS client instance.\nbucket_name\n    The name of the GCS bucket.\nobject_name\n    The GCS object name to check.\n\nReturns\n-------\nbool\n    True if the key represents a directory, False otherwise."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "file_exists",
        "code_chunk": "def file_exists(\n    client: storage.Client,\n    bucket_name: str,\n    object_name: str,\n) -> bool:\n    \"\"\"Check if a specific file exists in a GCS bucket.\n\n    Parameters\n    ----------\n    client\n        The GCS client.\n    bucket_name\n        The name of the bucket.\n    object_name\n        The GCS object name to check for existence.\n\n    Returns\n    -------\n    bool\n        True if the file exists, otherwise False.\n\n    Examples\n    --------\n    >>> client = storage.Client()\n    >>> file_exists(client, 'mybucket', 'folder/file.txt')\n    True\n    \"\"\"\n    bucket = client.bucket(bucket_name)\n    blob = bucket.blob(object_name)\n    return blob.exists()",
        "variables": [
            "bucket",
            "client",
            "object_name",
            "bucket_name",
            "blob"
        ],
        "docstring": "Check if a specific file exists in a GCS bucket.\n\nParameters\n----------\nclient\n    The GCS client.\nbucket_name\n    The name of the bucket.\nobject_name\n    The GCS object name to check for existence.\n\nReturns\n-------\nbool\n    True if the file exists, otherwise False.\n\nExamples\n--------\n>>> client = storage.Client()\n>>> file_exists(client, 'mybucket', 'folder/file.txt')\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "upload_file",
        "code_chunk": "def upload_file(\n    client: storage.Client,\n    bucket_name: str,\n    local_path: str,\n    object_name: Optional[str] = None,\n    overwrite: bool = False,\n) -> bool:\n    \"\"\"Upload a file to a GCS bucket from local directory.\n\n    Parameters\n    ----------\n    client\n        The GCS client instance.\n    bucket_name\n        The name of the target GCS bucket.\n    local_path\n        The file path on the local system to upload.\n    object_name\n        The target GCS object name. If None, uses the base name of\n        the local file path.\n    overwrite\n        If True, the existing file on GCS will be overwritten.\n\n    Returns\n    -------\n    bool\n        True if the file was uploaded successfully, False otherwise.\n\n    Examples\n    --------\n    >>> client = storage.Client()\n    >>> upload_file(\n    ...     client,\n    ...     'mybucket',\n    ...     '/path/to/file.txt',\n    ...     'folder/gcs_file.txt'\n    ... )\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n\n    local_path = Path(local_path)\n    if not local_path.exists():\n        logger.error(\"Local file does not exist.\")\n        return False\n\n    if object_name is None:\n        object_name = local_path.name\n\n    object_name = remove_leading_slash(object_name)\n    bucket = client.bucket(bucket_name)\n    blob = bucket.blob(object_name)\n\n    if not overwrite and blob.exists():\n        logger.error(\"File already exists in the bucket.\")\n        return False\n\n    try:\n        blob.upload_from_filename(str(local_path))\n        logger.info(\n            f\"Uploaded {local_path} to {bucket_name}/{object_name}\",\n        )\n        return True\n    except Exception as e:\n        logger.error(f\"Failed to upload file: {str(e)}\")\n        return False",
        "variables": [
            "bucket",
            "overwrite",
            "client",
            "local_path",
            "object_name",
            "bucket_name",
            "e",
            "blob"
        ],
        "docstring": "Upload a file to a GCS bucket from local directory.\n\nParameters\n----------\nclient\n    The GCS client instance.\nbucket_name\n    The name of the target GCS bucket.\nlocal_path\n    The file path on the local system to upload.\nobject_name\n    The target GCS object name. If None, uses the base name of\n    the local file path.\noverwrite\n    If True, the existing file on GCS will be overwritten.\n\nReturns\n-------\nbool\n    True if the file was uploaded successfully, False otherwise.\n\nExamples\n--------\n>>> client = storage.Client()\n>>> upload_file(\n...     client,\n...     'mybucket',\n...     '/path/to/file.txt',\n...     'folder/gcs_file.txt'\n... )\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "download_file",
        "code_chunk": "def download_file(\n    client: storage.Client,\n    bucket_name: str,\n    object_name: str,\n    local_path: str,\n    overwrite: bool = False,\n) -> bool:\n    \"\"\"Download a file from a GCS bucket to a local directory.\n\n    Parameters\n    ----------\n    client\n        The GCS client instance.\n    bucket_name\n        The name of the GCS bucket from which to download the file.\n    object_name\n        The GCS object name of the file to download.\n    local_path\n        The local file path where the downloaded file will be saved.\n    overwrite\n        If True, overwrite the local file if it exists.\n\n    Returns\n    -------\n    bool\n        True if the file was downloaded successfully, False otherwise.\n\n    Examples\n    --------\n    >>> client = storage.Client()\n    >>> download_file(\n    ...     client,\n    ...     'mybucket',\n    ...     'folder/gcs_file.txt',\n    ...     '/path/to/download.txt'\n    ... )\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n\n    local_path = Path(local_path)\n    bucket = client.bucket(bucket_name)\n    blob = bucket.blob(object_name)\n\n    if not blob.exists():\n        logger.error(\"File does not exist in the bucket.\")\n        return False\n\n    if not overwrite and local_path.exists():\n        logger.error(\"Local file already exists.\")\n        return False\n\n    try:\n        blob.download_to_filename(str(local_path))\n        logger.info(\n            f\"Downloaded {bucket_name}/{object_name} to {local_path}\",\n        )\n        return True\n    except Exception as e:\n        logger.error(f\"Failed to download file: {str(e)}\")\n        return False",
        "variables": [
            "bucket",
            "overwrite",
            "client",
            "local_path",
            "object_name",
            "bucket_name",
            "e",
            "blob"
        ],
        "docstring": "Download a file from a GCS bucket to a local directory.\n\nParameters\n----------\nclient\n    The GCS client instance.\nbucket_name\n    The name of the GCS bucket from which to download the file.\nobject_name\n    The GCS object name of the file to download.\nlocal_path\n    The local file path where the downloaded file will be saved.\noverwrite\n    If True, overwrite the local file if it exists.\n\nReturns\n-------\nbool\n    True if the file was downloaded successfully, False otherwise.\n\nExamples\n--------\n>>> client = storage.Client()\n>>> download_file(\n...     client,\n...     'mybucket',\n...     'folder/gcs_file.txt',\n...     '/path/to/download.txt'\n... )\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "delete_file",
        "code_chunk": "def delete_file(\n    client: storage.Client,\n    bucket_name: str,\n    object_name: str,\n) -> bool:\n    \"\"\"Delete a file from a GCS bucket.\n\n    Parameters\n    ----------\n    client\n        The GCS client instance.\n    bucket_name\n        The name of the bucket from which the file will be deleted.\n    object_name\n        The GCS object name of the file to delete.\n\n    Returns\n    -------\n    bool\n        True if the file was deleted successfully, otherwise False.\n\n    Examples\n    --------\n    >>> client = storage.Client()\n    >>> delete_file(client, 'mybucket', 'folder/gcs_file.txt')\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    object_name = remove_leading_slash(object_name)\n\n    bucket = client.bucket(bucket_name)\n    blob = bucket.blob(object_name)\n\n    if not blob.exists():\n        logger.error(\"File does not exist in the bucket.\")\n        return False\n\n    try:\n        blob.delete()\n        logger.info(f\"Deleted {bucket_name}/{object_name}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Failed to delete file: {str(e)}\")\n        return False",
        "variables": [
            "bucket",
            "client",
            "object_name",
            "bucket_name",
            "e",
            "blob"
        ],
        "docstring": "Delete a file from a GCS bucket.\n\nParameters\n----------\nclient\n    The GCS client instance.\nbucket_name\n    The name of the bucket from which the file will be deleted.\nobject_name\n    The GCS object name of the file to delete.\n\nReturns\n-------\nbool\n    True if the file was deleted successfully, otherwise False.\n\nExamples\n--------\n>>> client = storage.Client()\n>>> delete_file(client, 'mybucket', 'folder/gcs_file.txt')\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "copy_file",
        "code_chunk": "def copy_file(\n    client: storage.Client,\n    source_bucket_name: str,\n    source_object_name: str,\n    destination_bucket_name: str,\n    destination_object_name: str,\n    overwrite: bool = False,\n) -> bool:\n    \"\"\"Copy a file from one GCS bucket to another.\n\n    Parameters\n    ----------\n    client\n        The GCS client instance.\n    source_bucket_name\n        The name of the source bucket.\n    source_object_name\n        The GCS object name of the source file.\n    destination_bucket_name\n        The name of the destination bucket.\n    destination_object_name\n        The GCS object name of the destination file.\n    overwrite\n        If True, overwrite the destination file if it already exists.\n\n    Returns\n    -------\n    bool\n        True if the file was copied successfully, otherwise False.\n\n    Examples\n    --------\n    >>> client = storage.Client()\n    >>> copy_file(\n    ...     client,\n    ...     'source-bucket',\n    ...     'source_file.txt',\n    ...     'destination-bucket',\n    ...     'destination_file.txt'\n    ... )\n    True\n    \"\"\"\n    source_bucket_name = validate_bucket_name(source_bucket_name)\n    destination_bucket_name = validate_bucket_name(destination_bucket_name)\n\n    source_object_name = remove_leading_slash(source_object_name)\n    destination_object_name = remove_leading_slash(destination_object_name)\n\n    if is_gcs_directory(client, source_bucket_name, source_object_name):\n        logger.error(\"Source object is a directory.\")\n        return False\n\n    source_bucket = client.bucket(source_bucket_name)\n    destination_bucket = client.bucket(destination_bucket_name)\n    source_blob = source_bucket.blob(source_object_name)\n    destination_blob = destination_bucket.blob(destination_object_name)\n\n    if not overwrite and destination_blob.exists():\n        logger.error(\n            \"Destination file already exists in the destination bucket.\",\n        )\n        return False\n\n    if not source_blob.exists():\n        logger.error(\"Source file does not exist in the bucket.\")\n        return False\n\n    try:\n        source_bucket.copy_blob(\n            source_blob,\n            destination_bucket,\n            destination_object_name,\n        )\n        logger.info(\n            f\"Copied {source_bucket_name}/{source_object_name} to \"\n            f\"{destination_bucket_name}/{destination_object_name}\",\n        )\n        return True\n    except Exception as e:\n        logger.error(f\"Failed to copy file: {str(e)}\")\n        return False",
        "variables": [
            "destination_blob",
            "source_object_name",
            "overwrite",
            "destination_bucket_name",
            "destination_object_name",
            "client",
            "source_blob",
            "destination_bucket",
            "source_bucket_name",
            "source_bucket",
            "e"
        ],
        "docstring": "Copy a file from one GCS bucket to another.\n\nParameters\n----------\nclient\n    The GCS client instance.\nsource_bucket_name\n    The name of the source bucket.\nsource_object_name\n    The GCS object name of the source file.\ndestination_bucket_name\n    The name of the destination bucket.\ndestination_object_name\n    The GCS object name of the destination file.\noverwrite\n    If True, overwrite the destination file if it already exists.\n\nReturns\n-------\nbool\n    True if the file was copied successfully, otherwise False.\n\nExamples\n--------\n>>> client = storage.Client()\n>>> copy_file(\n...     client,\n...     'source-bucket',\n...     'source_file.txt',\n...     'destination-bucket',\n...     'destination_file.txt'\n... )\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "create_folder_on_gcs",
        "code_chunk": "def create_folder_on_gcs(\n    client: storage.Client,\n    bucket_name: str,\n    folder_path: str,\n) -> bool:\n    \"\"\"Create a folder in a GCS bucket if it doesn't already exist.\n\n    Parameters\n    ----------\n    client\n        The GCS client instance.\n    bucket_name\n        The name of the bucket where the folder will be created.\n    folder_path\n        The name of the folder to create.\n\n    Returns\n    -------\n    bool\n        True if the folder was created successfully or\n        already exists, otherwise False.\n\n    Examples\n    --------\n    >>> client = storage.Client()\n    >>> create_folder_on_gcs(client, 'mybucket', 'new_folder/')\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    folder_path = remove_leading_slash(folder_path)\n\n    if not folder_path.endswith(\"/\"):\n        folder_path += \"/\"\n\n    bucket = client.bucket(bucket_name)\n    blob = bucket.blob(folder_path)\n\n    if blob.exists():\n        logger.info(f\"Folder '{folder_path}' already exists on GCS.\")\n        return True\n    else:\n        try:\n            blob.upload_from_string(\"\")\n            logger.info(f\"Created folder '{folder_path}' on GCS.\")\n            return True\n        except Exception as e:\n            logger.error(f\"Failed to create folder on GCS: {str(e)}\")\n            return False",
        "variables": [
            "folder_path",
            "bucket",
            "client",
            "bucket_name",
            "e",
            "blob"
        ],
        "docstring": "Create a folder in a GCS bucket if it doesn't already exist.\n\nParameters\n----------\nclient\n    The GCS client instance.\nbucket_name\n    The name of the bucket where the folder will be created.\nfolder_path\n    The name of the folder to create.\n\nReturns\n-------\nbool\n    True if the folder was created successfully or\n    already exists, otherwise False.\n\nExamples\n--------\n>>> client = storage.Client()\n>>> create_folder_on_gcs(client, 'mybucket', 'new_folder/')\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "upload_folder",
        "code_chunk": "def upload_folder(\n    client: storage.Client,\n    bucket_name: str,\n    local_path: str,\n    prefix: str = \"\",\n    overwrite: bool = False,\n) -> bool:\n    \"\"\"Upload an entire folder from the local file system to a GCS bucket.\n\n    Parameters\n    ----------\n    client\n        The GCS client instance.\n    bucket_name\n        The name of the bucket to which the folder will be uploaded.\n    local_path\n        The path to the local folder to upload.\n    prefix\n        The prefix to prepend to each object name when uploading to GCS.\n    overwrite\n        If True, overwrite existing files in the bucket.\n\n    Returns\n    -------\n    bool\n        True if the folder was uploaded successfully, otherwise False.\n\n    Examples\n    --------\n    >>> client = storage.Client()\n    >>> upload_folder(\n    ...     client,\n    ...     'mybucket',\n    ...     '/path/to/local/folder',\n    ...     'folder_prefix',\n    ...     True\n    ... )\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    local_path = Path(local_path)\n\n    # Check if the local folder exists\n    if not local_path.is_dir():\n        logger.error(\"Local folder does not exist.\")\n        return False\n\n    prefix = remove_leading_slash(prefix)\n\n    if prefix and not prefix.endswith(\"/\"):\n        prefix += \"/\"\n\n    # Iterate over files in the local folder and its subdirectories\n    for file_path in local_path.rglob(\"*\"):\n        if file_path.is_file():\n            # Determine the GCS object key\n            object_name = f\"{prefix}{file_path.relative_to(local_path)}\"\n            # Check if the file already exists in the bucket\n            bucket = client.bucket(bucket_name)\n            blob = bucket.blob(object_name)\n            if not overwrite and blob.exists():\n                logger.error(\n                    f\"File '{object_name}' already exists in the bucket.\",\n                )\n                return False\n            # Upload the file to GCS\n            try:\n                blob.upload_from_filename(str(file_path))\n                logger.info(f\"Uploaded '{file_path}' to '{object_name}'.\")\n            except Exception as e:\n                logger.error(f\"Failed to upload file: {str(e)}\")\n                return False\n\n    return True",
        "variables": [
            "prefix",
            "bucket",
            "overwrite",
            "client",
            "local_path",
            "object_name",
            "file_path",
            "bucket_name",
            "e",
            "blob"
        ],
        "docstring": "Upload an entire folder from the local file system to a GCS bucket.\n\nParameters\n----------\nclient\n    The GCS client instance.\nbucket_name\n    The name of the bucket to which the folder will be uploaded.\nlocal_path\n    The path to the local folder to upload.\nprefix\n    The prefix to prepend to each object name when uploading to GCS.\noverwrite\n    If True, overwrite existing files in the bucket.\n\nReturns\n-------\nbool\n    True if the folder was uploaded successfully, otherwise False.\n\nExamples\n--------\n>>> client = storage.Client()\n>>> upload_folder(\n...     client,\n...     'mybucket',\n...     '/path/to/local/folder',\n...     'folder_prefix',\n...     True\n... )\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "list_files",
        "code_chunk": "def list_files(\n    client: storage.Client,\n    bucket_name: str,\n    prefix: str = \"\",\n) -> List[str]:\n    \"\"\"List files in a GCS bucket that match a specific prefix.\n\n    Parameters\n    ----------\n    client\n        The GCS client.\n    bucket_name\n        The name of the bucket.\n    prefix\n        The prefix to filter files, by default \"\".\n\n    Returns\n    -------\n    List[str]\n        A list of GCS object keys matching the prefix.\n\n    Examples\n    --------\n    >>> client = storage.Client()\n    >>> list_files(client, 'mybucket', 'folder_prefix/')\n    ['folder_prefix/file1.txt', 'folder_prefix/file2.txt']\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    prefix = remove_leading_slash(prefix)\n\n    bucket = client.bucket(bucket_name)\n    blobs = client.list_blobs(bucket, prefix=prefix)\n\n    return [blob.name for blob in blobs]",
        "variables": [
            "prefix",
            "bucket",
            "blobs",
            "client",
            "bucket_name",
            "blob"
        ],
        "docstring": "List files in a GCS bucket that match a specific prefix.\n\nParameters\n----------\nclient\n    The GCS client.\nbucket_name\n    The name of the bucket.\nprefix\n    The prefix to filter files, by default \"\".\n\nReturns\n-------\nList[str]\n    A list of GCS object keys matching the prefix.\n\nExamples\n--------\n>>> client = storage.Client()\n>>> list_files(client, 'mybucket', 'folder_prefix/')\n['folder_prefix/file1.txt', 'folder_prefix/file2.txt']"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "download_folder",
        "code_chunk": "def download_folder(\n    client: storage.Client,\n    bucket_name: str,\n    prefix: str,\n    local_path: str,\n    overwrite: bool = False,\n) -> bool:\n    \"\"\"Download a folder from a GCS bucket to a local directory.\n\n    Parameters\n    ----------\n    client\n        The GCS client instance.\n    bucket_name\n        The name of the GCS bucket from which to download the folder.\n    prefix\n        The GCS prefix of the folder to download.\n    local_path\n        The local directory path where the downloaded folder will be saved.\n    overwrite\n        If True, overwrite existing local files if they exist.\n\n    Returns\n    -------\n    bool\n        True if the folder was downloaded successfully, False otherwise.\n\n    Examples\n    --------\n    >>> client = storage.Client()\n    >>> download_folder(\n    ...     client,\n    ...     'mybucket',\n    ...     'folder/subfolder/',\n    ...     '/path/to/local_folder',\n    ...     overwrite=False\n    ... )\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    local_path = Path(local_path)\n\n    prefix = remove_leading_slash(prefix)\n    if not is_gcs_directory(client, bucket_name, prefix):\n        logger.error(f\"The provided GCS prefix {prefix} is not a directory.\")\n        return False\n\n    if not local_path.exists():\n        local_path.mkdir(parents=True)\n\n    try:\n        blobs = client.list_blobs(bucket_name, prefix=prefix)\n        for blob in blobs:\n            # Ensure the blob is strictly within the given directory\n            if not blob.name.startswith(f\"{prefix}/\") and blob.name != prefix:\n                continue\n\n            target = local_path / Path(blob.name).relative_to(prefix)\n            if not overwrite and target.exists():\n                logger.info(f\"Skipping {target} as it already exists.\")\n                continue\n            if not target.parent.exists():\n                target.parent.mkdir(parents=True)\n            blob.download_to_filename(str(target))\n            logger.info(f\"Downloaded {blob.name} to {target}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Failed to download folder: {str(e)}\")\n        return False",
        "variables": [
            "target",
            "prefix",
            "overwrite",
            "blobs",
            "client",
            "local_path",
            "bucket_name",
            "e",
            "blob"
        ],
        "docstring": "Download a folder from a GCS bucket to a local directory.\n\nParameters\n----------\nclient\n    The GCS client instance.\nbucket_name\n    The name of the GCS bucket from which to download the folder.\nprefix\n    The GCS prefix of the folder to download.\nlocal_path\n    The local directory path where the downloaded folder will be saved.\noverwrite\n    If True, overwrite existing local files if they exist.\n\nReturns\n-------\nbool\n    True if the folder was downloaded successfully, False otherwise.\n\nExamples\n--------\n>>> client = storage.Client()\n>>> download_folder(\n...     client,\n...     'mybucket',\n...     'folder/subfolder/',\n...     '/path/to/local_folder',\n...     overwrite=False\n... )\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "move_file",
        "code_chunk": "def move_file(\n    client: storage.Client,\n    source_bucket_name: str,\n    source_object_name: str,\n    destination_bucket_name: str,\n    destination_object_name: str,\n) -> bool:\n    \"\"\"Move a file within or between GCS buckets.\n\n    Parameters\n    ----------\n    client\n        The GCS client instance.\n    source_bucket_name\n        The name of the source GCS bucket.\n    source_object_name\n        The GCS object name of the source file.\n    destination_bucket_name\n        The name of the destination GCS bucket.\n    destination_object_name\n        The GCS object name of the destination file.\n\n    Returns\n    -------\n    bool\n        True if the file was moved successfully, False otherwise.\n\n    Examples\n    --------\n    >>> client = storage.Client()\n    >>> move_file(\n    ...     client,\n    ...     'sourcebucket',\n    ...     'source_folder/file.txt',\n    ...     'destbucket',\n    ...     'dest_folder/file.txt'\n    ... )\n    True\n    \"\"\"\n    source_bucket_name = validate_bucket_name(source_bucket_name)\n    destination_bucket_name = validate_bucket_name(destination_bucket_name)\n\n    source_object_name = remove_leading_slash(source_object_name)\n    destination_object_name = remove_leading_slash(destination_object_name)\n\n    if is_gcs_directory(client, source_bucket_name, source_object_name):\n        logger.error(\"Source object is a directory.\")\n        return False\n\n    source_bucket = client.bucket(source_bucket_name)\n    destination_bucket = client.bucket(destination_bucket_name)\n    source_blob = source_bucket.blob(source_object_name)\n\n    if not source_blob.exists():\n        logger.error(\"Source file does not exist in the bucket.\")\n        return False\n\n    try:\n        source_bucket.copy_blob(\n            source_blob,\n            destination_bucket,\n            destination_object_name,\n        )\n        source_blob.delete()\n        logger.info(\n            f\"Moved {source_bucket_name}/{source_object_name} to \"\n            f\"{destination_bucket_name}/{destination_object_name}\",\n        )\n        return True\n    except Exception as e:\n        logger.error(f\"Failed to move file: {str(e)}\")\n        return False",
        "variables": [
            "source_object_name",
            "destination_bucket_name",
            "destination_object_name",
            "client",
            "source_blob",
            "destination_bucket",
            "source_bucket_name",
            "source_bucket",
            "e"
        ],
        "docstring": "Move a file within or between GCS buckets.\n\nParameters\n----------\nclient\n    The GCS client instance.\nsource_bucket_name\n    The name of the source GCS bucket.\nsource_object_name\n    The GCS object name of the source file.\ndestination_bucket_name\n    The name of the destination GCS bucket.\ndestination_object_name\n    The GCS object name of the destination file.\n\nReturns\n-------\nbool\n    True if the file was moved successfully, False otherwise.\n\nExamples\n--------\n>>> client = storage.Client()\n>>> move_file(\n...     client,\n...     'sourcebucket',\n...     'source_folder/file.txt',\n...     'destbucket',\n...     'dest_folder/file.txt'\n... )\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\helpers\\gcp_utils.py",
        "function_name": "delete_folder",
        "code_chunk": "def delete_folder(\n    client: storage.Client,\n    bucket_name: str,\n    folder_path: str,\n) -> bool:\n    \"\"\"Delete a folder in a GCS bucket.\n\n    Parameters\n    ----------\n    client\n        The GCS client instance.\n    bucket_name\n        The name of the GCS bucket.\n    folder_path\n        The path of the folder to delete.\n\n    Returns\n    -------\n    bool\n        True if the folder was deleted successfully, otherwise False.\n\n    Examples\n    --------\n    >>> client = storage.Client()\n    >>> delete_folder(client, 'mybucket', 'path/to/folder/')\n    True\n    \"\"\"\n    bucket_name = validate_bucket_name(bucket_name)\n    folder_path = remove_leading_slash(folder_path)\n\n    if not is_gcs_directory(client, bucket_name, folder_path):\n        logger.error(f\"The provided path {folder_path} is not a directory.\")\n        return False\n\n    try:\n        blobs = client.list_blobs(bucket_name, prefix=folder_path)\n        for blob in blobs:\n            blob.delete()\n        logger.info(f\"Deleted folder {folder_path} in bucket {bucket_name}\")\n        return True\n    except Exception as e:\n        logger.error(\n            f\"Failed to delete folder {folder_path} \"\n            f\"in bucket {bucket_name}: {str(e)}\",\n        )\n        return False",
        "variables": [
            "folder_path",
            "blobs",
            "client",
            "bucket_name",
            "e",
            "blob"
        ],
        "docstring": "Delete a folder in a GCS bucket.\n\nParameters\n----------\nclient\n    The GCS client instance.\nbucket_name\n    The name of the GCS bucket.\nfolder_path\n    The path of the folder to delete.\n\nReturns\n-------\nbool\n    True if the folder was deleted successfully, otherwise False.\n\nExamples\n--------\n>>> client = storage.Client()\n>>> delete_folder(client, 'mybucket', 'path/to/folder/')\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\io\\inputs.py",
        "function_name": "read_table",
        "code_chunk": "def read_table(\n    spark: SparkSession,\n    table_path: TablePath,\n    columns: Optional[Sequence[str]] = None,\n    date_column: Optional[str] = None,\n    date_range: Optional[Sequence[str]] = None,\n    column_filter_dict: Optional[Dict[str, Sequence[str]]] = None,\n    run_id_column: Optional[str] = \"run_id\",\n    run_id: Optional[str] = None,\n    flatten_struct_cols: bool = False,\n    partition_column: Optional[str] = None,\n    partition_type: Optional[BigQueryTimePartitions] = None,\n    partition_value: Optional[Union[Tuple[str, str], str]] = None,\n) -> SparkDF:\n    \"\"\"Read BigQuery table given table path and column selection.\n\n    Parameters\n    ----------\n    spark\n        Spark session.\n    table_path\n        The target BigQuery table name of form:\n        <project_id>.<database>.<table_name>\n    columns\n        The column selection. Selects all columns if None passed.\n    date_column\n        The name of the column to be used to filter the date range on.\n    date_range\n        Sequence with two values, a lower and upper value for dates to load in.\n    column_filter_dict\n        A dictionary containing column: [values] where the values correspond to\n        terms in the column that are to be filtered by.\n    run_id_column\n        The name of the column to be used to filter to the specified run_id.\n    run_id\n        The unique identifier for a run within the table that the read data is\n        filtered to.\n    partition_column\n        The name of the column that the table is partitioned by.\n    partition_type\n        The unit of time the table is partitioned by, must be one of:\n            * `hour`\n            * `day`\n            * `month`\n            * `year`\n    partition_value\n        The value or pair of values for filtering the partition column to.\n    flatten_struct_cols\n        When true, any struct type columns in the loaded dataframe are replaced\n        with individual columns for each of the fields in the structs.\n\n    Returns\n    -------\n    SparkDF\n    \"\"\"\n    if not table_exists(table_path=table_path):\n        msg = f\"{table_path=} cannot be found.\"\n        logger.error(msg)\n        raise TableNotFoundError(msg)\n\n    if not column_filter_dict:\n        column_filter_dict = {}\n\n    # If columns are specified, ensure they exist in the table to be read.\n    if columns:\n        table_cols = get_table_columns(table_path)\n        list_cols_not_in_table_cols = [col for col in columns if col not in table_cols]\n        if list_cols_not_in_table_cols:\n            message = f\"\"\"\n            Columns: {list_cols_not_in_table_cols} are not in dateset.\n            Choose columns from: {', '.join(table_cols)}.\n            \"\"\"\n            logger.error(message)\n            raise ColumnNotInDataframeError(message)\n\n    # If a run_id is passed, it is added to the column filter dictionary for\n    # use in reading and filtering the table.\n    if run_id:\n        column_filter_dict[run_id_column] = run_id\n\n    query = build_sql_query(\n        table_path=table_path,\n        columns=columns,\n        date_column=date_column,\n        date_range=date_range,\n        column_filter_dict=column_filter_dict,\n        partition_column=partition_column,\n        partition_type=partition_type,\n        partition_value=partition_value,\n    )\n\n    logger.info(f\"Reading table using query: \\n{query}\")\n\n    df = spark.read.load(query, format=\"bigquery\")\n\n    if is_df_empty(df):\n        logger.warning(f\"No data has been read from {table_path}\")\n\n    if flatten_struct_cols:\n        df = convert_struc_col_to_columns(df=df)\n\n    return df",
        "variables": [
            "partition_value",
            "query",
            "columns",
            "table_path",
            "column_filter_dict",
            "date_column",
            "flatten_struct_cols",
            "run_id_column",
            "spark",
            "message",
            "run_id",
            "partition_column",
            "list_cols_not_in_table_cols",
            "partition_type",
            "df",
            "col",
            "date_range",
            "msg",
            "table_cols"
        ],
        "docstring": "Read BigQuery table given table path and column selection.\n\nParameters\n----------\nspark\n    Spark session.\ntable_path\n    The target BigQuery table name of form:\n    <project_id>.<database>.<table_name>\ncolumns\n    The column selection. Selects all columns if None passed.\ndate_column\n    The name of the column to be used to filter the date range on.\ndate_range\n    Sequence with two values, a lower and upper value for dates to load in.\ncolumn_filter_dict\n    A dictionary containing column: [values] where the values correspond to\n    terms in the column that are to be filtered by.\nrun_id_column\n    The name of the column to be used to filter to the specified run_id.\nrun_id\n    The unique identifier for a run within the table that the read data is\n    filtered to.\npartition_column\n    The name of the column that the table is partitioned by.\npartition_type\n    The unit of time the table is partitioned by, must be one of:\n        * `hour`\n        * `day`\n        * `month`\n        * `year`\npartition_value\n    The value or pair of values for filtering the partition column to.\nflatten_struct_cols\n    When true, any struct type columns in the loaded dataframe are replaced\n    with individual columns for each of the fields in the structs.\n\nReturns\n-------\nSparkDF"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\io\\inputs.py",
        "function_name": "build_sql_query",
        "code_chunk": "def build_sql_query(  # noqa: C901\n    table_path: TablePath,\n    columns: Optional[Sequence[str]] = None,\n    date_column: Optional[str] = None,\n    date_range: Optional[Sequence[str]] = None,\n    column_filter_dict: Optional[Dict[str, Sequence[str]]] = None,\n    partition_column: Optional[str] = None,\n    partition_type: Optional[str] = None,\n    partition_value: Optional[Union[Tuple[str, str], str]] = None,\n) -> str:\n    \"\"\"Create SQL query to load data with the specified filter conditions.\n\n    Parameters\n    ----------\n    spark\n        Spark session.\n    table_path\n        BigQuery table path in format \"database_name.table_name\".\n    columns\n        The column selection. Selects all columns if None passed.\n    date_column\n        The name of the column to be used to filter the date range on.\n    date_range\n        Sequence with two values, a lower and upper value for dates to load in.\n    column_filter_dict\n        A dictionary containing column: [values] where the values correspond to\n        terms in the column that are to be filtered by.\n    partition_column\n        The name of the column that the table is partitioned by.\n    partition_type\n        The unit of time the table is partitioned by, must be one of:\n            * `hour`\n            * `day`\n            * `month`\n            * `year`\n    partition_value\n        The value or pair of values for filtering the partition column to.\n\n    Returns\n    -------\n    str\n        The string containing the SQL query.\n    \"\"\"\n    # Create empty list to store all parts of query - combined at end.\n    sql_query = []\n\n    # Flag to check whether or not to use a WHERE or AND statement as only one\n    # instance of WHERE is allowed in a query.\n    first_filter_applied = False\n\n    filter_start = {\n        False: \"WHERE\",\n        True: \"AND\",\n    }\n\n    # Join columns to comma-separated string for the SQL query.\n    selection = \", \".join(columns) if columns else \"*\"\n\n    sql_query.append(f\"SELECT {selection}\\nFROM {table_path}\")\n\n    if partition_column and partition_value and partition_type:\n        sql_query.append(f\"{filter_start[first_filter_applied]} (\")\n\n        # If a single partition value is being used we use an \"=\" for\n        # comparison, otherwise we use the \"BETWEEN\" SQL function.\n        partition_value = tuple_convert(partition_value)\n        if len(partition_value) == 1:\n            sql_query.append(\n                f\"TIMESTAMP_TRUNC({partition_column}, {partition_type}) \"\n                f\"= TIMESTAMP_TRUNC(TIMESTAMP('{partition_value[0]}'), {partition_type})\",  # noqa: E501\n            )\n        elif len(partition_value) == 2:\n            partition_value = convert_date_strings_to_datetimes(\n                *partition_value,\n            )\n\n            sql_query.append(f\"{partition_column}\")\n            sql_query.append(f\"BETWEEN '{partition_value[0]}'\")\n            sql_query.append(f\"AND '{partition_value[1]}'\")\n\n        else:\n            msg = f\"{partition_value=} must have either 1 or 2 values only.\"\n            logger.error(msg)\n            raise ValueError(msg)\n\n        sql_query.append(\")\")\n\n        first_filter_applied = True\n\n    if date_column and date_range:\n        sql_query.append(f\"{filter_start[first_filter_applied]} (\")\n        sql_query.append(f\"{date_column} >= '{date_range[0]}'\")\n        sql_query.append(f\"AND {date_column} < '{date_range[1]}'\")\n        sql_query.append(\")\")\n\n        first_filter_applied = True\n\n    # Add any column-value specific filters onto the query. Addtional queries\n    # are of the form:\n    # AND (column_A = 'value1' OR column_A = 'value2' OR ...)\n    if column_filter_dict:\n        for column in column_filter_dict.keys():\n            # Ensure values are in a list if not already.\n            column_filter_dict[column] = list_convert(\n                column_filter_dict[column],\n            )\n\n            sql_query.append(f\"{filter_start[first_filter_applied]} (\\n\")\n\n            # If the value is a string we wrap it in quotes, otherwise we don't\n            # which avoids turning e.g. an integer into a string (1 -> '1').\n            if isinstance(column_filter_dict[column][0], str):\n                sql_query.append(\n                    f\"{column} = '{column_filter_dict[column][0]}'\",\n                )\n            else:\n                sql_query.append(\n                    f\"{column} = {column_filter_dict[column][0]}\",\n                )\n\n            first_filter_applied = True\n\n            # Subsequent queries on column are the same form but use OR.\n            if len(column_filter_dict[column]) > 1:\n                for item in column_filter_dict[column][1:]:\n                    if isinstance(item, str):\n                        sql_query.append(f\"OR {column} = '{item}'\\n\")\n                    else:\n                        sql_query.append(f\"OR {column} = {item}\\n\")\n\n            # close off the column filter query\n            sql_query.append(\")\\n\")\n\n    # Join entries in list into one nicely formatted string for easier unit\n    # testing. Use textwrap.dedent to remove leading whitespace from multiline\n    # strings.\n    return \"\\n\".join([textwrap.dedent(line.strip()) for line in sql_query])",
        "variables": [
            "columns",
            "table_path",
            "column",
            "sql_query",
            "first_filter_applied",
            "partition_type",
            "selection",
            "partition_value",
            "item",
            "date_range",
            "column_filter_dict",
            "date_column",
            "line",
            "msg",
            "filter_start",
            "partition_column"
        ],
        "docstring": "Create SQL query to load data with the specified filter conditions.\n\nParameters\n----------\nspark\n    Spark session.\ntable_path\n    BigQuery table path in format \"database_name.table_name\".\ncolumns\n    The column selection. Selects all columns if None passed.\ndate_column\n    The name of the column to be used to filter the date range on.\ndate_range\n    Sequence with two values, a lower and upper value for dates to load in.\ncolumn_filter_dict\n    A dictionary containing column: [values] where the values correspond to\n    terms in the column that are to be filtered by.\npartition_column\n    The name of the column that the table is partitioned by.\npartition_type\n    The unit of time the table is partitioned by, must be one of:\n        * `hour`\n        * `day`\n        * `month`\n        * `year`\npartition_value\n    The value or pair of values for filtering the partition column to.\n\nReturns\n-------\nstr\n    The string containing the SQL query."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\gcp\\io\\outputs.py",
        "function_name": "write_table",
        "code_chunk": "def write_table(\n    df: Union[PandasDF, SparkDF],\n    table_name: TablePath,\n    mode: Literal[\"append\", \"error\", \"ignore\", \"overwrite\"] = \"error\",\n    partition_col: Optional[str] = None,\n    partition_type: Optional[BigQueryTimePartitions] = None,\n    partition_expiry_days: Optional[float] = None,\n    clustered_fields: Optional[Union[str, List[str]]] = None,\n) -> None:\n    \"\"\"Write dataframe out to a Google BigQuery table.\n\n    In the case the table already exists, behavior of this function depends on\n    the save mode, specified by the mode function (default to throwing an\n    exception). When mode is Overwrite, the schema of the DataFrame does not\n    need to be the same as that of the existing table (the column order\n    doesn't need be the same).\n\n    If you use the `df.printSchema()` method directly in a print/log statement\n    the code is processed and printed regardless of logging level. Instead you\n    need to capture the output and pass this to the logger. See explanation\n    here - https://stackoverflow.com/a/59935109\n\n    To learn more about the partitioning of tables and how to use them in\n    BigQuery: https://cloud.google.com/bigquery/docs/partitioned-tables\n\n    To learn more about the clustering of tables and how to use them in\n    BigQuery: https://cloud.google.com/bigquery/docs/clustered-tables\n\n    To learn more about how spark dataframes are saved to BigQuery:\n    https://github.com/GoogleCloudDataproc/spark-bigquery-connector/blob/master/README.md\n\n    Parameters\n    ----------\n    df\n        The dataframe to be saved.\n    table_name\n        The target BigQuery table name of form:\n        <project_id>.<database>.<table_name>\n    mode\n        Whether to overwrite or append to the BigQuery table.\n            * `append`: Append contents of this :class:`DataFrame` to table.\n            * `overwrite`: Overwrite existing data.\n            * `error`: Throw exception if data already exists.\n            * `ignore`: Silently ignore this operation if data already exists.\n    partition_col\n        A date or timestamp type column in the dataframe to use for the table\n        partitioning.\n    partition_type\n        The unit of time to partition the table by, must be one of:\n            * `hour`\n            * `day`\n            * `month`\n            * `year`\n\n        If `partition_col` is specified and `partition_type = None` then\n        BigQuery will default to using `day` partition type.\n\n        If 'partition_type` is specified and `partition_col = None` then the\n        table will be partitioned by the ingestion time pseudo column, and can\n        be referenced in BigQuery via either  `_PARTITIONTIME as pt` or\n        `_PARTITIONDATE' as pd`.\n\n        See https://cloud.google.com/bigquery/docs/querying-partitioned-tables\n        for more information on querying partitioned tables.\n    partition_expiry_days\n        If specified, this is the number of days (any decimal values are\n        converted to that proportion of a day) that BigQuery keeps the data\n        in each partition.\n    clustered_fields\n        If specified, the columns (up to four) in the dataframe to cluster the\n        data by when outputting. The order the columns are specified is\n        important as will be the ordering of the clustering on the BigQuery\n        table.\n\n        See: https://cloud.google.com/bigquery/docs/querying-clustered-tables\n        for more information on querying clustered tables.\n\n    Returns\n    -------\n    None\n    \"\"\"  # noqa: E501\n    logger.info(f\"Writing to table {table_name} with mode {mode.upper()}\")\n\n    # Pandas df should always be small enough to be saved as a single\n    # file/partition.\n    if isinstance(df, PandasDF):\n        logger.debug(\"Converting pandas dataframe to spark\")\n        df = df.sql_ctx.createDataFrame(df).coalesce(1)\n\n    logger.info(\n        f\"Output dataframe has schema\\n{df._jdf.schema().treeString()}\",\n    )\n\n    if is_df_empty(df):\n        logger.warning(\n            \"The output contains no records. No data will be appended to the \"\n            f\"{table_name} table.\",\n        )\n\n    write = df.write.format(\"bigquery\").mode(mode)\n\n    if partition_col:\n        logger.info(f\"Data in BigQuery will be partitioned on {partition_col}\")\n        write = write.option(\"partitionField\", partition_col)\n\n    if partition_type:\n        logger.info(\n            f\"Data in BigQuery will be partitioned by {partition_type}\",\n        )\n        write = write.option(\"partitionType\", partition_type.upper())\n\n    if clustered_fields:\n        clustered_fields = list_convert(clustered_fields)\n\n        if len(clustered_fields) > 4:\n            msg = (\n                f\"Cannot save {table_name=} with clustered columns\"\n                f\"Number of columns specified = {len(clustered_fields)} > 4\"\n            )\n            logger.error(msg)\n            raise ValueError(msg)\n\n        cluster_string = \",\".join(clustered_fields)\n\n        logger.info(f\"Data in BigQuery will be clustered on {cluster_string}\")\n        write = write.option(\"clusteredFields\", cluster_string)\n\n    write.save(table_name)\n\n    # For any partitioned table it is best practice to require partition\n    # filtering for any SQL queries in BigQuery.\n    if partition_col or partition_type:\n        logger.info(\"Setting BigQuery require_partition_filter to True\")\n        run_bq_query(\n            f\"\"\"\n            ALTER TABLE {table_name}\n            SET OPTIONS (\n                require_partition_filter = true\n            );\n            \"\"\",\n        )\n\n    if partition_expiry_days:\n        logger.info(f\"Setting BigQuery {partition_expiry_days=}\")\n        run_bq_query(\n            f\"\"\"\n            ALTER TABLE {table_name}\n            SET OPTIONS (\n                partition_expiration_days = {partition_expiry_days}\n            );\n            \"\"\",\n        )",
        "variables": [
            "partition_expiry_days",
            "partition_type",
            "df",
            "clustered_fields",
            "write",
            "cluster_string",
            "partition_col",
            "msg",
            "table_name",
            "mode"
        ],
        "docstring": "Write dataframe out to a Google BigQuery table.\n\nIn the case the table already exists, behavior of this function depends on\nthe save mode, specified by the mode function (default to throwing an\nexception). When mode is Overwrite, the schema of the DataFrame does not\nneed to be the same as that of the existing table (the column order\ndoesn't need be the same).\n\nIf you use the `df.printSchema()` method directly in a print/log statement\nthe code is processed and printed regardless of logging level. Instead you\nneed to capture the output and pass this to the logger. See explanation\nhere - https://stackoverflow.com/a/59935109\n\nTo learn more about the partitioning of tables and how to use them in\nBigQuery: https://cloud.google.com/bigquery/docs/partitioned-tables\n\nTo learn more about the clustering of tables and how to use them in\nBigQuery: https://cloud.google.com/bigquery/docs/clustered-tables\n\nTo learn more about how spark dataframes are saved to BigQuery:\nhttps://github.com/GoogleCloudDataproc/spark-bigquery-connector/blob/master/README.md\n\nParameters\n----------\ndf\n    The dataframe to be saved.\ntable_name\n    The target BigQuery table name of form:\n    <project_id>.<database>.<table_name>\nmode\n    Whether to overwrite or append to the BigQuery table.\n        * `append`: Append contents of this :class:`DataFrame` to table.\n        * `overwrite`: Overwrite existing data.\n        * `error`: Throw exception if data already exists.\n        * `ignore`: Silently ignore this operation if data already exists.\npartition_col\n    A date or timestamp type column in the dataframe to use for the table\n    partitioning.\npartition_type\n    The unit of time to partition the table by, must be one of:\n        * `hour`\n        * `day`\n        * `month`\n        * `year`\n\n    If `partition_col` is specified and `partition_type = None` then\n    BigQuery will default to using `day` partition type.\n\n    If 'partition_type` is specified and `partition_col = None` then the\n    table will be partitioned by the ingestion time pseudo column, and can\n    be referenced in BigQuery via either  `_PARTITIONTIME as pt` or\n    `_PARTITIONDATE' as pd`.\n\n    See https://cloud.google.com/bigquery/docs/querying-partitioned-tables\n    for more information on querying partitioned tables.\npartition_expiry_days\n    If specified, this is the number of days (any decimal values are\n    converted to that proportion of a day) that BigQuery keeps the data\n    in each partition.\nclustered_fields\n    If specified, the columns (up to four) in the dataframe to cluster the\n    data by when outputting. The order the columns are specified is\n    important as will be the ordering of the clustering on the BigQuery\n    table.\n\n    See: https://cloud.google.com/bigquery/docs/querying-clustered-tables\n    for more information on querying clustered tables.\n\nReturns\n-------\nNone"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "create_colname_to_value_map",
        "code_chunk": "def create_colname_to_value_map(cols: Sequence[str]) -> SparkCol:\n    \"\"\"Create a column name to value MapType column.\"\"\"\n    colname_value_tups = [(F.lit(name), F.col(name)) for name in cols]\n    # Chain chains multiple lists into one for create_map.\n    return F.create_map(*(itertools.chain(*colname_value_tups)))",
        "variables": [
            "colname_value_tups",
            "cols",
            "name"
        ],
        "docstring": "Create a column name to value MapType column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "set_df_columns_nullable",
        "code_chunk": "def set_df_columns_nullable(\n    df: SparkDF,\n    column_list: List[str],\n    nullable: Optional[bool] = True,\n) -> SparkDF:\n    \"\"\"Change specified columns nullable value.\n\n    Sometimes find that spark creates columns that have the nullable attribute\n    set to False, which can cause issues if this dataframe is saved to a table\n    as it will set the schema for that column to not allow missing values.\n\n    Changing this parameter for a column appears to be very difficult (and also\n    potentially costly [see so answer comments] - SO USE ONLY IF NEEDED).\n\n    The solution implemented is taken from Stack Overflow post:\n    https://stackoverflow.com/a/51821437\n\n    Parameters\n    ----------\n    df\n        The dataframe with columns to have nullable attribute changed.\n    column_list\n        List of columns to change nullable attribute.\n    nullable\n        The value to set the nullable attribute to for the specified columns.\n\n    Returns\n    -------\n    SparkDF\n        The input dataframe but with nullable attribute changed for specified\n        columns.\n    \"\"\"\n    for struct_field in df.schema:\n        if struct_field.name in column_list:\n            struct_field.nullable = nullable\n\n    # Create a new dataframe using the underlying RDDs and the updated schemas.\n    return SparkSession._instantiatedSession.createDataFrame(df.rdd, df.schema)",
        "variables": [
            "nullable",
            "column_list",
            "df",
            "struct_field"
        ],
        "docstring": "Change specified columns nullable value.\n\nSometimes find that spark creates columns that have the nullable attribute\nset to False, which can cause issues if this dataframe is saved to a table\nas it will set the schema for that column to not allow missing values.\n\nChanging this parameter for a column appears to be very difficult (and also\npotentially costly [see so answer comments] - SO USE ONLY IF NEEDED).\n\nThe solution implemented is taken from Stack Overflow post:\nhttps://stackoverflow.com/a/51821437\n\nParameters\n----------\ndf\n    The dataframe with columns to have nullable attribute changed.\ncolumn_list\n    List of columns to change nullable attribute.\nnullable\n    The value to set the nullable attribute to for the specified columns.\n\nReturns\n-------\nSparkDF\n    The input dataframe but with nullable attribute changed for specified\n    columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "melt",
        "code_chunk": "def melt(\n    df: SparkDF,\n    id_vars: Union[str, Sequence[str]],\n    value_vars: Union[str, Sequence[str]],\n    var_name: str = \"variable\",\n    value_name: str = \"value\",\n) -> SparkDF:\n    \"\"\"Melt a spark dataframe in a pandas like fashion.\n\n    Parameters\n    ----------\n    df\n        The pyspark dataframe to melt.\n    id_vars\n        The names of the columns to use as identifier variables.\n    value_vars\n        The names of the columns containing the data to unpivot.\n    var_name\n        The name of the target column containing variable names\n        (i.e. the original column names).\n    value_name\n        The name of the target column containing the unpivoted\n        data.\n\n    Returns\n    -------\n    SparkDF\n        The \"melted\" input data as a pyspark data frame.\n\n    Examples\n    --------\n    >>> df = spark.createDataFrame(\n    ...     [[1, 2, 3, 4],\n    ...      [5, 6, 7, 8],\n    ...      [9, 10, 11, 12]],\n    ...     [\"col1\", \"col2\", \"col3\", \"col4\"])\n    >>> melt(df=df, id_vars=\"col1\", value_vars=[\"col2\", \"col3\"]).show()\n    +----+--------+-----+\n    |col1|variable|value|\n    +----+--------+-----+\n    |   1|    col2|    2|\n    |   1|    col3|    3|\n    |   5|    col2|    6|\n    |   5|    col3|    7|\n    |   9|    col2|   10|\n    |   9|    col3|   11|\n    +----+--------+-----+\n\n    >>> melt(df=df, id_vars=[\"col1\", \"col2\"], value_vars=[\"col3\", \"col4\"]\n    ... ).show()\n    +----+----+--------+-----+\n    |col1|col2|variable|value|\n    +----+----+--------+-----+\n    |   1|   2|    col3|    3|\n    |   1|   2|    col4|    4|\n    |   5|   6|    col3|    7|\n    |   5|   6|    col4|    8|\n    |   9|  10|    col3|   11|\n    |   9|  10|    col4|   12|\n    +----+----+--------+-----+\n    \"\"\"\n    # Create array<struct<variable: str, value: ...>, <struct<...>>\n    # Essentially a list of column placeholders\n    # Tuple comprehension ensures we have a lit element and col reference for\n    # each column to melt\n    _vars_and_vals = F.array(\n        *(\n            F.struct(F.lit(c).alias(var_name), F.col(c).alias(value_name))\n            for c in value_vars\n        ),\n    )\n\n    # Add to the DataFrame and explode, which extends the dataframe\n    _tmp = df.withColumn(\"_vars_and_vals\", F.explode(_vars_and_vals))\n\n    # We only want to select certain columns\n    cols = id_vars + [\n        F.col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]\n    ]\n\n    return _tmp.select(*cols)",
        "variables": [
            "value_vars",
            "_tmp",
            "value_name",
            "df",
            "var_name",
            "id_vars",
            "x",
            "c",
            "_vars_and_vals",
            "cols"
        ],
        "docstring": "Melt a spark dataframe in a pandas like fashion.\n\nParameters\n----------\ndf\n    The pyspark dataframe to melt.\nid_vars\n    The names of the columns to use as identifier variables.\nvalue_vars\n    The names of the columns containing the data to unpivot.\nvar_name\n    The name of the target column containing variable names\n    (i.e. the original column names).\nvalue_name\n    The name of the target column containing the unpivoted\n    data.\n\nReturns\n-------\nSparkDF\n    The \"melted\" input data as a pyspark data frame.\n\nExamples\n--------\n>>> df = spark.createDataFrame(\n...     [[1, 2, 3, 4],\n...      [5, 6, 7, 8],\n...      [9, 10, 11, 12]],\n...     [\"col1\", \"col2\", \"col3\", \"col4\"])\n>>> melt(df=df, id_vars=\"col1\", value_vars=[\"col2\", \"col3\"]).show()\n+----+--------+-----+\n|col1|variable|value|\n+----+--------+-----+\n|   1|    col2|    2|\n|   1|    col3|    3|\n|   5|    col2|    6|\n|   5|    col3|    7|\n|   9|    col2|   10|\n|   9|    col3|   11|\n+----+--------+-----+\n\n>>> melt(df=df, id_vars=[\"col1\", \"col2\"], value_vars=[\"col3\", \"col4\"]\n... ).show()\n+----+----+--------+-----+\n|col1|col2|variable|value|\n+----+----+--------+-----+\n|   1|   2|    col3|    3|\n|   1|   2|    col4|    4|\n|   5|   6|    col3|    7|\n|   5|   6|    col4|    8|\n|   9|  10|    col3|   11|\n|   9|  10|    col4|   12|\n+----+----+--------+-----+"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "to_spark_col",
        "code_chunk": "def to_spark_col(_func=None, *, exclude: Sequence[str] = None) -> Callable:\n    \"\"\"Convert str args to Spark Column if not already.\n\n    Usage\n    -----\n    Use as a decorator on a function.\n\n    To convert all string arguments to spark column\n    >>> @to_spark_col\n    >>> def my_func(arg1, arg2)\n\n    To exclude a string arguments from being converted to a spark column\n    >>> @to_spark_col(exclude=['arg2'])\n    >>> def my_func(arg1, arg2)\n    \"\"\"\n    if not exclude:\n        exclude = []\n\n    def caller(func: Callable[[Union[str, SparkCol]], SparkCol]):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            varnames = func.__code__.co_varnames\n            if args:\n                args = [\n                    (\n                        _convert_to_spark_col(arg)\n                        if varnames[i] not in exclude\n                        else arg\n                    )  # noqa: E501\n                    for i, arg in enumerate(args)\n                ]\n            if kwargs:\n                kwargs = {\n                    k: (\n                        _convert_to_spark_col(kwarg) if k not in exclude else kwarg\n                    )  # noqa: E501\n                    for k, kwarg in kwargs.items()\n                }\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    if _func is None:\n        return caller\n    else:\n        return caller(_func)",
        "variables": [
            "_func",
            "exclude",
            "k",
            "varnames",
            "kwarg",
            "arg",
            "i",
            "args",
            "kwargs"
        ],
        "docstring": "Convert str args to Spark Column if not already.\n\nUsage\n-----\nUse as a decorator on a function.\n\nTo convert all string arguments to spark column\n>>> @to_spark_col\n>>> def my_func(arg1, arg2)\n\nTo exclude a string arguments from being converted to a spark column\n>>> @to_spark_col(exclude=['arg2'])\n>>> def my_func(arg1, arg2)"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "_convert_to_spark_col",
        "code_chunk": "def _convert_to_spark_col(s: Union[str, SparkCol]) -> SparkCol:\n    \"\"\"Convert strings to Spark Columns, otherwise returns input.\"\"\"\n    if isinstance(s, str):\n        return F.col(s)\n    elif isinstance(s, SparkCol):\n        return s\n    else:\n        msg = f\"\"\"\n        expecting a string or pyspark column but received obj\n        of type {type(s)}\n        \"\"\"\n        raise ValueError(msg)",
        "variables": [
            "msg",
            "s"
        ],
        "docstring": "Convert strings to Spark Columns, otherwise returns input."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "to_list",
        "code_chunk": "def to_list(df: SparkDF) -> List[Union[Any, List[Any]]]:\n    \"\"\"Convert Spark DF to a list.\n\n    Returns\n    -------\n    list or list of lists\n        If the input DataFrame has a single column then a list of column\n        values will be returned. If the DataFrame has multiple columns\n        then a list of row data as lists will be returned.\n    \"\"\"\n    if len(df.columns) == 1:\n        return df.toPandas().squeeze().tolist()\n    else:\n        return df.toPandas().to_numpy().tolist()",
        "variables": [
            "df"
        ],
        "docstring": "Convert Spark DF to a list.\n\nReturns\n-------\nlist or list of lists\n    If the input DataFrame has a single column then a list of column\n    values will be returned. If the DataFrame has multiple columns\n    then a list of row data as lists will be returned."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "map_column_names",
        "code_chunk": "def map_column_names(df: SparkDF, mapper: Mapping[str, str]) -> SparkDF:\n    \"\"\"Map column names to the given values in the mapper.\n\n    If the column name is not in the mapper the name doesn't change.\n    \"\"\"\n    cols = [\n        F.col(col_name).alias(mapper.get(col_name, col_name)) for col_name in df.columns\n    ]\n    return df.select(*cols)",
        "variables": [
            "col_name",
            "cols",
            "df",
            "mapper"
        ],
        "docstring": "Map column names to the given values in the mapper.\n\nIf the column name is not in the mapper the name doesn't change."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "transform",
        "code_chunk": "def transform(self, f, *args, **kwargs):\n    \"\"\"Chain Pyspark function.\"\"\"\n    return f(self, *args, **kwargs)",
        "variables": [
            "self",
            "args",
            "kwargs",
            "f"
        ],
        "docstring": "Chain Pyspark function."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "is_df_empty",
        "code_chunk": "def is_df_empty(df: SparkDF) -> bool:\n    \"\"\"Check whether a spark dataframe contains any records.\"\"\"\n    if not df.head(1):\n        return True\n    else:\n        return False",
        "variables": [
            "df"
        ],
        "docstring": "Check whether a spark dataframe contains any records."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "unpack_list_col",
        "code_chunk": "def unpack_list_col(\n    df: SparkDF,\n    list_col: str,\n    unpacked_col: str,\n) -> SparkDF:\n    \"\"\"Unpack a spark column containing a list into multiple rows.\n\n    Parameters\n    ----------\n    df\n        Contains the list column to unpack.\n    list_col\n        The name of the column which contains lists.\n    unpacked_col\n        The name of the column containing the unpacked list items.\n\n    Returns\n    -------\n    SparkDF\n        Contains a new row for each unpacked list item.\n    \"\"\"\n    return df.withColumn(unpacked_col, F.explode(list_col))",
        "variables": [
            "list_col",
            "unpacked_col",
            "df"
        ],
        "docstring": "Unpack a spark column containing a list into multiple rows.\n\nParameters\n----------\ndf\n    Contains the list column to unpack.\nlist_col\n    The name of the column which contains lists.\nunpacked_col\n    The name of the column containing the unpacked list items.\n\nReturns\n-------\nSparkDF\n    Contains a new row for each unpacked list item."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "get_window_spec",
        "code_chunk": "def get_window_spec(\n    partition_cols: Optional[Union[str, Sequence[str]]] = None,\n    order_cols: Optional[Union[str, Sequence[str]]] = None,\n) -> WindowSpec:\n    \"\"\"Return ordered and partitioned WindowSpec, defaulting to whole df.\n\n    Particularly useful when you don't know if the variable being used for\n    partition_cols will contain values or not in advance.\n\n    Parameters\n    ----------\n    partition_cols\n        If present the columns to partition a spark dataframe on.\n    order_cols\n        If present the columns to order a spark dataframe on (where order in\n        sequence is order that orderBy is applied).\n\n    Returns\n    -------\n    WindowSpec\n        The WindowSpec object to be applied.\n\n    Usage\n    -----\n    window_spec = get_window_spec(...)\n\n    F.sum(values).over(window_spec)\n    \"\"\"\n    if partition_cols and order_cols:\n        window_spec = Window.partitionBy(partition_cols).orderBy(order_cols)\n\n    elif partition_cols:\n        window_spec = Window.partitionBy(partition_cols)\n\n    elif order_cols:\n        window_spec = Window.orderBy(order_cols)\n\n    else:\n        window_spec = Window.rowsBetween(\n            Window.unboundedPreceding,\n            Window.unboundedFollowing,\n        )\n\n    return window_spec",
        "variables": [
            "window_spec",
            "partition_cols",
            "order_cols"
        ],
        "docstring": "Return ordered and partitioned WindowSpec, defaulting to whole df.\n\nParticularly useful when you don't know if the variable being used for\npartition_cols will contain values or not in advance.\n\nParameters\n----------\npartition_cols\n    If present the columns to partition a spark dataframe on.\norder_cols\n    If present the columns to order a spark dataframe on (where order in\n    sequence is order that orderBy is applied).\n\nReturns\n-------\nWindowSpec\n    The WindowSpec object to be applied.\n\nUsage\n-----\nwindow_spec = get_window_spec(...)\n\nF.sum(values).over(window_spec)"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "rank_numeric",
        "code_chunk": "def rank_numeric(\n    numeric: Union[str, Sequence[str]],\n    group: Union[str, Sequence[str]],\n    ascending: bool = False,\n) -> SparkCol:\n    \"\"\"Rank a numeric and assign a unique value to each row.\n\n    The `F.row_number()` method has been selected as a method to rank as\n    gives a unique number to each row. Other methods such as `F.rank()`\n    and `F.dense_rank()` do not assign unique values per row.\n\n    Parameters\n    ----------\n    numeric\n        The column name or list of column names containing values which will\n        be ranked.\n    group\n        The grouping levels to rank the numeric column or columns over.\n    ascending\n        Dictates whether high or low values are ranked as the top value.\n\n    Returns\n    -------\n    SparkCol\n        Contains a rank for the row in its grouping level.\n    \"\"\"\n    if ascending:\n        # Defaults to ascending.\n        window = Window.partitionBy(group).orderBy(numeric)\n    else:\n        if type(numeric) is list:\n            message = f\"\"\"\n            The function parameter numeric must be a string when using\n            F.desc(numeric).\n            Currently numeric={numeric}.\n            \"\"\"\n            logger.error(message)\n            raise ValueError(message)\n        window = Window.partitionBy(group).orderBy(F.desc(numeric))\n\n    return F.row_number().over(window)",
        "variables": [
            "numeric",
            "ascending",
            "message",
            "window",
            "group"
        ],
        "docstring": "Rank a numeric and assign a unique value to each row.\n\nThe `F.row_number()` method has been selected as a method to rank as\ngives a unique number to each row. Other methods such as `F.rank()`\nand `F.dense_rank()` do not assign unique values per row.\n\nParameters\n----------\nnumeric\n    The column name or list of column names containing values which will\n    be ranked.\ngroup\n    The grouping levels to rank the numeric column or columns over.\nascending\n    Dictates whether high or low values are ranked as the top value.\n\nReturns\n-------\nSparkCol\n    Contains a rank for the row in its grouping level."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "calc_median_price",
        "code_chunk": "def calc_median_price(\n    groups: Union[str, Sequence[str]],\n    price_col: str = \"price\",\n) -> SparkCol:\n    \"\"\"Calculate the median price per grouping level.\n\n    Parameters\n    ----------\n    groups\n        The grouping levels for calculating the average price.\n    price_col\n        Column name containing the product prices.\n\n    Returns\n    -------\n    SparkCol\n        A single entry for each grouping level, and its median price.\n    \"\"\"\n    # Note median in [1,2,3,4] would return as 2 using below.\n    median = f\"percentile_approx({price_col}, 0.5)\"\n\n    return F.expr(median).over(Window.partitionBy(groups))",
        "variables": [
            "price_col",
            "median",
            "groups"
        ],
        "docstring": "Calculate the median price per grouping level.\n\nParameters\n----------\ngroups\n    The grouping levels for calculating the average price.\nprice_col\n    Column name containing the product prices.\n\nReturns\n-------\nSparkCol\n    A single entry for each grouping level, and its median price."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "convert_cols_to_struct_col",
        "code_chunk": "def convert_cols_to_struct_col(\n    df: SparkDF,\n    struct_col_name: str,\n    struct_cols: Optional[Sequence[str]],\n    no_struct_col_type: T.DataTypeSingleton = T.BooleanType(),\n    no_struct_col_value: Any = None,\n) -> SparkDF:\n    \"\"\"Convert specified selection of columns to a single struct column.\n\n    As BigQuery tables do not take to having an empty struct column appended to\n    them, this function will create a placeholder column to put into the struct\n    column if no column names to combine are passed.\n\n    Parameters\n    ----------\n    df\n        The input dataframe that contains the columns for combining.\n    struct_col_name\n        The name of the resulting struct column.\n    struct_cols\n        A sequence of columns present in df for combining.\n    no_struct_col_type\n        If no struct_cols are present, this is the type that the dummy column\n        to place in the struct will be, default = BooleanType.\n    no_struct_col_value\n        If no struct_cols are present, this is the value that will be used in\n        the dummy column, default = None.\n\n    Returns\n    -------\n        The input dataframe with the specified struct_cols dropped and replaced\n        with a single struct type column containing those columns.\n\n    Raises\n    ------\n    ValueError\n        If not all the specified struct_cols are present in df.\n    \"\"\"\n    if struct_cols and not all(col in df.columns for col in struct_cols):\n        message = f\"\"\"\n        Cannot create struct columns due to column mismatch.\n\n        Want to create struct column from columns: {struct_cols}\n\n        But dataframe has columns {df.columns}\n        \"\"\"\n        logger.error(message)\n        raise ValueError(message)\n\n    if not struct_cols:\n        df = df.withColumn(\n            f\"no_{struct_col_name}\",\n            F.lit(no_struct_col_value).cast(no_struct_col_type),\n        )\n        struct_cols = [f\"no_{struct_col_name}\"]\n\n    return df.withColumn(struct_col_name, F.struct(*struct_cols)).drop(*struct_cols)",
        "variables": [
            "df",
            "no_struct_col_type",
            "col",
            "struct_col_name",
            "message",
            "struct_cols",
            "no_struct_col_value"
        ],
        "docstring": "Convert specified selection of columns to a single struct column.\n\nAs BigQuery tables do not take to having an empty struct column appended to\nthem, this function will create a placeholder column to put into the struct\ncolumn if no column names to combine are passed.\n\nParameters\n----------\ndf\n    The input dataframe that contains the columns for combining.\nstruct_col_name\n    The name of the resulting struct column.\nstruct_cols\n    A sequence of columns present in df for combining.\nno_struct_col_type\n    If no struct_cols are present, this is the type that the dummy column\n    to place in the struct will be, default = BooleanType.\nno_struct_col_value\n    If no struct_cols are present, this is the value that will be used in\n    the dummy column, default = None.\n\nReturns\n-------\n    The input dataframe with the specified struct_cols dropped and replaced\n    with a single struct type column containing those columns.\n\nRaises\n------\nValueError\n    If not all the specified struct_cols are present in df."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "select_first_obs_appearing_in_group",
        "code_chunk": "def select_first_obs_appearing_in_group(\n    df: SparkDF,\n    group: Sequence[str],\n    date_col: str,\n    ascending: bool,\n) -> SparkDF:\n    \"\"\"Rank and select observation in group based on earliest or latest date.\n\n    Given that there can be multiple observations per group, select\n    observation that appears first or last (depending on whether ascending is\n    set to True or False, respectively).\n\n    Parameters\n    ----------\n    df\n        The input dataframe that contains the group and date_col.\n    group\n        The grouping levels required to find the observation that appears first\n        or last (depending on whether ascending is set to True or False,\n        respectively)\n    date_col\n        Column name containing the dates of each observation.\n    ascending\n        Dictates whether first or last observation within a grouping is\n        selected (depending on whether ascending is set to True or False,\n        respectively).\n\n    Returns\n    -------\n    SparkDF\n        The input dataframe that contains each observation per group that\n        appeared first or last (depending on whether ascending is set to\n        True or False, respectively) according to date_col.\n    \"\"\"\n    rank_by_date = rank_numeric(\n        numeric=date_col,\n        group=group,\n        ascending=ascending,\n    )\n    return df.withColumn(\"rank\", rank_by_date).filter(F.col(\"rank\") == 1).drop(\"rank\")",
        "variables": [
            "ascending",
            "df",
            "rank_by_date",
            "date_col",
            "group"
        ],
        "docstring": "Rank and select observation in group based on earliest or latest date.\n\nGiven that there can be multiple observations per group, select\nobservation that appears first or last (depending on whether ascending is\nset to True or False, respectively).\n\nParameters\n----------\ndf\n    The input dataframe that contains the group and date_col.\ngroup\n    The grouping levels required to find the observation that appears first\n    or last (depending on whether ascending is set to True or False,\n    respectively)\ndate_col\n    Column name containing the dates of each observation.\nascending\n    Dictates whether first or last observation within a grouping is\n    selected (depending on whether ascending is set to True or False,\n    respectively).\n\nReturns\n-------\nSparkDF\n    The input dataframe that contains each observation per group that\n    appeared first or last (depending on whether ascending is set to\n    True or False, respectively) according to date_col."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "convert_struc_col_to_columns",
        "code_chunk": "def convert_struc_col_to_columns(\n    df: SparkDF,\n    convert_nested_structs: bool = False,\n) -> SparkDF:\n    \"\"\"Flatten struct columns in pyspark dataframe to individual columns.\n\n    Parameters\n    ----------\n    df\n        Dataframe that may or may not contain struct type columns.\n    convert_nested_structs\n        If true, function will recursively call until no structs are left.\n        Inversely, when false, only top level structs are flattened; if these\n        contain subsequent structs they would remain.\n\n    Returns\n    -------\n        The input dataframe but with any struct type columns dropped, and in\n        its place the individual fields within the struct column as individual\n        columns.\n    \"\"\"\n    struct_cols = []\n    for field in df.schema.fields:\n        if type(field.dataType) == T.StructType:\n            struct_cols.append(field.name)\n\n    df = df.select(\n        # Select all columns in df not identified as being struct type.\n        *[col for col in df.columns if col not in struct_cols],\n        # All columns identified as being struct type, but expand the struct\n        # to individual columns\u00a0using .* notation.\n        *[f\"{col}.*\" for col in struct_cols],\n    )\n\n    if convert_nested_structs and any(\n        isinstance(field.dataType, T.StructType) for field in df.schema.fields\n    ):\n        df = convert_struc_col_to_columns(df=df)\n\n    return df",
        "variables": [
            "convert_nested_structs",
            "df",
            "col",
            "struct_cols",
            "field"
        ],
        "docstring": "Flatten struct columns in pyspark dataframe to individual columns.\n\nParameters\n----------\ndf\n    Dataframe that may or may not contain struct type columns.\nconvert_nested_structs\n    If true, function will recursively call until no structs are left.\n    Inversely, when false, only top level structs are flattened; if these\n    contain subsequent structs they would remain.\n\nReturns\n-------\n    The input dataframe but with any struct type columns dropped, and in\n    its place the individual fields within the struct column as individual\n    columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "cut_lineage",
        "code_chunk": "def cut_lineage(df: SparkDF) -> SparkDF:\n    \"\"\"Convert the SparkDF to a Java RDD and back again.\n\n    This function is helpful in instances where Catalyst optimizer is causing\n    memory errors or problems, as it only tries to optimize till the conversion\n    point.\n\n    Note: This uses internal members and may break between versions.\n\n    Parameters\n    ----------\n    df\n        SparkDF to convert.\n\n    Returns\n    -------\n    SparkDF\n        New SparkDF created from Java RDD.\n\n    Raises\n    ------\n    Exception\n        If any error occurs during the lineage cutting process,\n        particularly during conversion between SparkDF and Java RDD\n        or accessing internal members.\n\n    Examples\n    --------\n    >>> df = rdd.toDF()\n    >>> new_df = cut_lineage(df)\n    >>> new_df.count()\n    3\n    \"\"\"\n    try:\n        logger.info(\"Converting SparkDF to Java RDD.\")\n\n        jrdd = df._jdf.toJavaRDD()\n        jschema = df._jdf.schema()\n        jrdd.cache()\n        spark = df.sparkSession\n        new_java_df = spark._jsparkSession.createDataFrame(jrdd, jschema)\n        new_df = SparkDF(new_java_df, spark)\n        return new_df\n    except Exception as e:\n        logger.error(f\"An error occurred during the lineage cutting process: {e}\")\n        raise",
        "variables": [
            "new_java_df",
            "jschema",
            "df",
            "jrdd",
            "spark",
            "new_df",
            "e"
        ],
        "docstring": "Convert the SparkDF to a Java RDD and back again.\n\nThis function is helpful in instances where Catalyst optimizer is causing\nmemory errors or problems, as it only tries to optimize till the conversion\npoint.\n\nNote: This uses internal members and may break between versions.\n\nParameters\n----------\ndf\n    SparkDF to convert.\n\nReturns\n-------\nSparkDF\n    New SparkDF created from Java RDD.\n\nRaises\n------\nException\n    If any error occurs during the lineage cutting process,\n    particularly during conversion between SparkDF and Java RDD\n    or accessing internal members.\n\nExamples\n--------\n>>> df = rdd.toDF()\n>>> new_df = cut_lineage(df)\n>>> new_df.count()\n3"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "find_spark_dataframes",
        "code_chunk": "def find_spark_dataframes(\n    locals_dict: Dict[str, Union[SparkDF, Dict]],\n) -> Dict[str, Union[SparkDF, Dict]]:\n    \"\"\"Extract SparkDF's objects from a given dictionary.\n\n    This function scans the dictionary and returns another containing only\n    entries where the value is a SparkDF. It also handles dictionaries within\n    the input, including them in the output if their first item is a SparkDF.\n\n    Designed to be used with locals() in Python, allowing extraction of\n    all SparkDF variables in a function's local scope.\n\n    Parameters\n    ----------\n    locals_dict\n        A dictionary usually returned by locals(), with variable names\n        as keys and their corresponding objects as values.\n\n    Returns\n    -------\n    Dict\n        A dictionary with entries from locals_dict where the value is a\n        SparkDF or a dictionary with a SparkDF as its first item.\n\n    Examples\n    --------\n    >>> dfs = find_spark_dataframes(locals())\n    \"\"\"\n    frames = {}\n\n    for key, value in locals_dict.items():\n        if key in [\"_\", \"__\", \"___\"]:\n            continue\n\n        if isinstance(value, SparkDF):\n            frames[key] = value\n            logger.info(f\"SparkDF found: {key}\")\n        elif (\n            isinstance(value, dict)\n            and value\n            and isinstance(next(iter(value.values())), SparkDF)\n        ):\n            frames[key] = value\n            logger.info(f\"Dictionary of SparkDFs found: {key}\")\n        else:\n            logger.debug(\n                f\"Skipping non-SparkDF item: {key}, Type: {type(value)}\",\n            )\n\n    return frames",
        "variables": [
            "locals_dict",
            "value",
            "frames",
            "key"
        ],
        "docstring": "Extract SparkDF's objects from a given dictionary.\n\nThis function scans the dictionary and returns another containing only\nentries where the value is a SparkDF. It also handles dictionaries within\nthe input, including them in the output if their first item is a SparkDF.\n\nDesigned to be used with locals() in Python, allowing extraction of\nall SparkDF variables in a function's local scope.\n\nParameters\n----------\nlocals_dict\n    A dictionary usually returned by locals(), with variable names\n    as keys and their corresponding objects as values.\n\nReturns\n-------\nDict\n    A dictionary with entries from locals_dict where the value is a\n    SparkDF or a dictionary with a SparkDF as its first item.\n\nExamples\n--------\n>>> dfs = find_spark_dataframes(locals())"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "create_spark_session",
        "code_chunk": "def create_spark_session(\n    app_name: Optional[str] = None,\n    size: Optional[Literal[\"small\", \"medium\", \"large\", \"extra-large\"]] = None,\n    extra_configs: Optional[Dict[str, str]] = None,\n) -> SparkSession:\n    \"\"\"Create a PySpark Session based on the specified size.\n\n    This function creates a PySpark session with different configurations\n    based on the size specified.\n\n    The size can be 'default', 'small', 'medium', 'large', or 'extra-large'.\n    Extra Spark configurations can be passed as a dictionary.\n    If no size is given, then a basic Spark session is spun up.\n\n    Parameters\n    ----------\n    app_name\n        The spark session app name.\n    size\n        The size of the spark session to be created. It can be 'default',\n        'small', 'medium', 'large', or 'extra-large'.\n    extra_configs\n        Mapping of additional spark session config settings and the desired\n        value for it. Will override any default settings.\n\n    Returns\n    -------\n    SparkSession\n        The created PySpark session.\n\n    Raises\n    ------\n    ValueError\n        If the specified 'size' parameter is not one of the valid options:\n        'small', 'medium', 'large', or 'extra-large'.\n    Exception\n        If any other error occurs during the Spark session creation process.\n\n    Examples\n    --------\n    >>> spark = create_spark_session('medium', {'spark.ui.enabled': 'false'})\n\n    Session Details:\n    ---------------\n    'small':\n        This is the smallest session that will realistically be used. It uses\n        only 1g of memory and 3 executors, and only 1 core. The number of\n        partitions are limited to 12, which can improve performance with\n        smaller data. It's recommended for simple data exploration of small\n        survey data or for training and demonstrations when several people\n        need to run Spark sessions simultaneously.\n    'medium':\n        A standard session used for analysing survey or synthetic datasets.\n        Also used for some Production pipelines based on survey and/or smaller\n        administrative data.It uses 6g of memory and 3 executors, and 3 cores.\n        The number of partitions are limited to 18, which can improve\n        performance with smaller data.\n    'large':\n        Session designed for running Production pipelines on large\n        administrative data, rather than just survey data. It uses 10g of\n        memory and 5 executors, 1g of memory overhead, and 5 cores. It uses the\n        default number of 200 partitions.\n    'extra-large':\n        Used for the most complex pipelines, with huge administrative\n        data sources and complex calculations. It uses 20g of memory and\n        12 executors, 2g of memory overhead, and 5 cores. It uses 240\n        partitions; not significantly higher than the default of 200,\n        but it is best for these to be a multiple of cores and executors.\n\n    References\n    ----------\n    The session sizes and their details are taken directly\n    from the following resource:\n    \"https://best-practice-and-impact.github.io/ons-spark/spark-overview/example-spark-sessions.html\"\n    \"\"\"\n    try:\n        if size:\n            size = size.lower()\n            valid_sizes = [\"small\", \"medium\", \"large\", \"extra-large\"]\n            if size not in valid_sizes:\n                msg = f\"Invalid '{size=}'. If specified must be one of {valid_sizes}.\"\n                raise ValueError(msg)\n\n        logger.info(\n            (\n                f\"Creating a '{size}' Spark session...\"\n                if size\n                else \"Creating a basic Spark session...\"\n            ),\n        )\n\n        if app_name:\n            builder = SparkSession.builder.appName(f\"{app_name}\")\n        else:\n            builder = SparkSession.builder\n\n        # fmt: off\n        if size == \"small\":\n            builder = (\n                builder.config(\"spark.executor.memory\", \"1g\")\n                .config(\"spark.executor.cores\", 1)\n                .config(\"spark.dynamicAllocation.maxExecutors\", 3)\n                .config(\"spark.sql.shuffle.partitions\", 12)\n            )\n        elif size == \"medium\":\n            builder = (\n                builder.config(\"spark.executor.memory\", \"6g\")\n                .config(\"spark.executor.cores\", 3)\n                .config(\"spark.dynamicAllocation.maxExecutors\", 3)\n                .config(\"spark.sql.shuffle.partitions\", 18)\n            )\n        elif size == \"large\":\n            builder = (\n                builder.config(\"spark.executor.memory\", \"10g\")\n                .config(\"spark.yarn.executor.memoryOverhead\", \"1g\")\n                .config(\"spark.executor.cores\", 5)\n                .config(\"spark.dynamicAllocation.maxExecutors\", 5)\n                .config(\"spark.sql.shuffle.partitions\", 200)\n            )\n        elif size == \"extra-large\":\n            builder = (\n                builder.config(\"spark.executor.memory\", \"20g\")\n                .config(\"spark.yarn.executor.memoryOverhead\", \"2g\")\n                .config(\"spark.executor.cores\", 5)\n                .config(\"spark.dynamicAllocation.maxExecutors\", 12)\n                .config(\"spark.sql.shuffle.partitions\", 240)\n            )\n\n        # Common configurations for all sizes\n        builder = (\n            # Dynamic Allocation\n            builder.config(\"spark.dynamicAllocation.enabled\", \"true\")\n             .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\")\n             # Adaptive Query Execution\n             .config(\"spark.sql.adaptive.enabled\", \"true\")\n             # General\n             .config(\"spark.ui.showConsoleProgress\", \"false\")\n        ).enableHiveSupport()\n        # fmt: on\n\n        # Apply extra configurations\n        if extra_configs:\n            for key, value in extra_configs.items():\n                builder = builder.config(key, value)\n\n        logger.info(\"Spark session created successfully!\")\n        return builder.getOrCreate()\n    except Exception as e:\n        logger.error(f\"An error occurred while creating the Spark session: {e}\")\n        raise",
        "variables": [
            "builder",
            "size",
            "valid_sizes",
            "app_name",
            "value",
            "extra_configs",
            "key",
            "msg",
            "e"
        ],
        "docstring": "Create a PySpark Session based on the specified size.\n\nThis function creates a PySpark session with different configurations\nbased on the size specified.\n\nThe size can be 'default', 'small', 'medium', 'large', or 'extra-large'.\nExtra Spark configurations can be passed as a dictionary.\nIf no size is given, then a basic Spark session is spun up.\n\nParameters\n----------\napp_name\n    The spark session app name.\nsize\n    The size of the spark session to be created. It can be 'default',\n    'small', 'medium', 'large', or 'extra-large'.\nextra_configs\n    Mapping of additional spark session config settings and the desired\n    value for it. Will override any default settings.\n\nReturns\n-------\nSparkSession\n    The created PySpark session.\n\nRaises\n------\nValueError\n    If the specified 'size' parameter is not one of the valid options:\n    'small', 'medium', 'large', or 'extra-large'.\nException\n    If any other error occurs during the Spark session creation process.\n\nExamples\n--------\n>>> spark = create_spark_session('medium', {'spark.ui.enabled': 'false'})\n\nSession Details:\n---------------\n'small':\n    This is the smallest session that will realistically be used. It uses\n    only 1g of memory and 3 executors, and only 1 core. The number of\n    partitions are limited to 12, which can improve performance with\n    smaller data. It's recommended for simple data exploration of small\n    survey data or for training and demonstrations when several people\n    need to run Spark sessions simultaneously.\n'medium':\n    A standard session used for analysing survey or synthetic datasets.\n    Also used for some Production pipelines based on survey and/or smaller\n    administrative data.It uses 6g of memory and 3 executors, and 3 cores.\n    The number of partitions are limited to 18, which can improve\n    performance with smaller data.\n'large':\n    Session designed for running Production pipelines on large\n    administrative data, rather than just survey data. It uses 10g of\n    memory and 5 executors, 1g of memory overhead, and 5 cores. It uses the\n    default number of 200 partitions.\n'extra-large':\n    Used for the most complex pipelines, with huge administrative\n    data sources and complex calculations. It uses 20g of memory and\n    12 executors, 2g of memory overhead, and 5 cores. It uses 240\n    partitions; not significantly higher than the default of 200,\n    but it is best for these to be a multiple of cores and executors.\n\nReferences\n----------\nThe session sizes and their details are taken directly\nfrom the following resource:\n\"https://best-practice-and-impact.github.io/ons-spark/spark-overview/example-spark-sessions.html\""
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "load_csv",
        "code_chunk": "def load_csv(\n    spark: SparkSession,\n    filepath: str,\n    keep_columns: Optional[List[str]] = None,\n    rename_columns: Optional[Dict[str, str]] = None,\n    drop_columns: Optional[List[str]] = None,\n    **kwargs,\n) -> SparkDF:\n    \"\"\"Load a CSV file into a PySpark DataFrame.\n\n    spark\n        Active SparkSession.\n    filepath\n        The full path and filename of the CSV file to load.\n    keep_columns\n        A list of column names to keep in the DataFrame, dropping all others.\n        Default value is None.\n    rename_columns\n        A dictionary to rename columns where keys are existing column\n        names and values are new column names.\n        Default value is None.\n    drop_columns\n        A list of column names to drop from the DataFrame.\n        Default value is None.\n    kwargs\n        Additional keyword arguments to pass to the `spark.read.csv` method.\n\n    Returns\n    -------\n    SparkDF\n        PySpark DataFrame containing the data from the CSV file.\n\n    Notes\n    -----\n    Transformation order:\n    1. Columns are kept according to `keep_columns`.\n    2. Columns are dropped according to `drop_columns`.\n    3. Columns are renamed according to `rename_columns`.\n\n    Raises\n    ------\n    Exception\n        If there is an error loading the file.\n    ValueError\n        If a column specified in rename_columns, drop_columns, or\n        keep_columns is not found in the DataFrame.\n\n    Examples\n    --------\n    Load a CSV file with multiline and rename columns:\n\n    >>> df = load_csv(\n            spark,\n            \"/path/to/file.csv\",\n            multiLine=True,\n            rename_columns={\"old_name\": \"new_name\"}\n        )\n\n    Load a CSV file with a specific encoding:\n\n    >>> df = load_csv(spark, \"/path/to/file.csv\", encoding=\"ISO-8859-1\")\n\n    Load a CSV file and keep only specific columns:\n\n    >>> df = load_csv(spark, \"/path/to/file.csv\", keep_columns=[\"col1\", \"col2\"])\n\n    Load a CSV file and drop specific columns:\n\n    >>> df = load_csv(spark, \"/path/to/file.csv\", drop_columns=[\"col1\", \"col2\"])\n\n    Load a CSV file with custom delimiter and multiline:\n\n    >>> df = load_csv(spark, \"/path/to/file.csv\", sep=\";\", multiLine=True)\n    \"\"\"\n    try:\n        df = spark.read.csv(filepath, header=True, **kwargs)\n        logger.info(f\"Loaded CSV file {filepath} with parameters {kwargs}\")\n    except Exception as e:\n        error_message = f\"Error loading file {filepath}: {e}\"\n        logger.error(error_message)\n        raise Exception(error_message) from e\n\n    columns = [str(col) for col in df.columns]\n\n    # When multi_line is used it adds \\r at the end of the final column\n    if kwargs.get(\"multiLine\", False):\n        columns[-1] = columns[-1].replace(\"\\r\", \"\")\n        df = df.withColumnRenamed(df.columns[-1], columns[-1])\n\n    # Apply column transformations: keep, drop, rename\n    if keep_columns:\n        missing_columns = [col for col in keep_columns if col not in columns]\n        if missing_columns:\n            error_message = (\n                f\"Columns {missing_columns} not found in DataFrame and cannot be kept\"\n            )\n            logger.error(error_message)\n            raise ValueError(error_message)\n        df = df.select(*keep_columns)\n\n    if drop_columns:\n        for col in drop_columns:\n            if col in columns:\n                df = df.drop(col)\n            else:\n                error_message = (\n                    f\"Column '{col}' not found in DataFrame and cannot be dropped\"\n                )\n                logger.error(error_message)\n                raise ValueError(error_message)\n\n    if rename_columns:\n        for old_name, new_name in rename_columns.items():\n            if old_name in columns:\n                df = df.withColumnRenamed(old_name, new_name)\n            else:\n                error_message = (\n                    f\"Column '{old_name}' not found in DataFrame and \"\n                    f\"cannot be renamed to '{new_name}'\"\n                )\n                logger.error(error_message)\n                raise ValueError(error_message)\n\n    return df",
        "variables": [
            "columns",
            "rename_columns",
            "drop_columns",
            "df",
            "error_message",
            "col",
            "filepath",
            "old_name",
            "new_name",
            "missing_columns",
            "keep_columns",
            "spark",
            "kwargs",
            "e"
        ],
        "docstring": "Load a CSV file into a PySpark DataFrame.\n\nspark\n    Active SparkSession.\nfilepath\n    The full path and filename of the CSV file to load.\nkeep_columns\n    A list of column names to keep in the DataFrame, dropping all others.\n    Default value is None.\nrename_columns\n    A dictionary to rename columns where keys are existing column\n    names and values are new column names.\n    Default value is None.\ndrop_columns\n    A list of column names to drop from the DataFrame.\n    Default value is None.\nkwargs\n    Additional keyword arguments to pass to the `spark.read.csv` method.\n\nReturns\n-------\nSparkDF\n    PySpark DataFrame containing the data from the CSV file.\n\nNotes\n-----\nTransformation order:\n1. Columns are kept according to `keep_columns`.\n2. Columns are dropped according to `drop_columns`.\n3. Columns are renamed according to `rename_columns`.\n\nRaises\n------\nException\n    If there is an error loading the file.\nValueError\n    If a column specified in rename_columns, drop_columns, or\n    keep_columns is not found in the DataFrame.\n\nExamples\n--------\nLoad a CSV file with multiline and rename columns:\n\n>>> df = load_csv(\n        spark,\n        \"/path/to/file.csv\",\n        multiLine=True,\n        rename_columns={\"old_name\": \"new_name\"}\n    )\n\nLoad a CSV file with a specific encoding:\n\n>>> df = load_csv(spark, \"/path/to/file.csv\", encoding=\"ISO-8859-1\")\n\nLoad a CSV file and keep only specific columns:\n\n>>> df = load_csv(spark, \"/path/to/file.csv\", keep_columns=[\"col1\", \"col2\"])\n\nLoad a CSV file and drop specific columns:\n\n>>> df = load_csv(spark, \"/path/to/file.csv\", drop_columns=[\"col1\", \"col2\"])\n\nLoad a CSV file with custom delimiter and multiline:\n\n>>> df = load_csv(spark, \"/path/to/file.csv\", sep=\";\", multiLine=True)"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "truncate_external_hive_table",
        "code_chunk": "def truncate_external_hive_table(spark: SparkSession, table_identifier: str) -> None:\n    \"\"\"Truncate an External Hive table stored on S3 or HDFS.\n\n    Parameters\n    ----------\n    spark\n        Active SparkSession.\n    table_identifier\n        The name of the Hive table to truncate. This can either be in the format\n        '<database>.<table>' or simply '<table>' if the current Spark session\n        has a database set.\n\n    Returns\n    -------\n    None\n        This function does not return any value. It performs an action of\n        truncating the table.\n\n    Raises\n    ------\n    ValueError\n        If the table name is incorrectly formatted, the database is not provided\n        when required, or if the table does not exist.\n    AnalysisException\n        If there is an issue with partition operations or SQL queries.\n    Exception\n        If there is a general failure during the truncation process.\n\n    Examples\n    --------\n    Truncate a Hive table named 'my_database.my_table':\n\n    >>> truncate_external_hive_table(spark, 'my_database.my_table')\n\n    Or, if the current Spark session already has a database set:\n\n    >>> spark.catalog.setCurrentDatabase('my_database')\n    >>> truncate_external_hive_table(spark, 'my_table')\n    \"\"\"\n    try:\n        logger.info(f\"Attempting to truncate the table '{table_identifier}'\")\n\n        # Extract database and table name, even if only the table name is provided\n        db_name, table_name = extract_database_name(spark, table_identifier)\n\n        # Set the current database if a database was specified\n        if db_name:\n            spark.catalog.setCurrentDatabase(db_name)\n\n        # Check if the table exists before proceeding\n        if not spark.catalog.tableExists(table_name, db_name):\n            error_msg = f\"Table '{db_name}.{table_name}' does not exist.\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        # Get the list of partitions\n        try:\n            partitions = spark.sql(f\"SHOW PARTITIONS {db_name}.{table_name}\").collect()\n        except Exception as e:\n            logger.warning(\n                f\"Unable to retrieve partitions for '{db_name}.{table_name}': {e}\",\n            )\n            partitions = []\n\n        if partitions:\n            logger.info(\n                f\"Table '{table_identifier}' is partitioned. Dropping all partitions.\",\n            )\n\n            # Drop each partition\n            for partition in partitions:\n                partition_spec = partition[\n                    0\n                ]  # e.g., partition is in format 'year=2023', etc.\n                spark.sql(\n                    f\"ALTER TABLE {db_name}.{table_name} \"\n                    f\"DROP IF EXISTS PARTITION ({partition_spec})\",\n                )\n\n        else:\n            logger.info(\n                f\"Table '{table_identifier}' has no partitions or is not partitioned.\",\n            )\n\n            # Overwrite with an empty DataFrame\n            original_df = spark.table(f\"{db_name}.{table_name}\")\n            schema: T.StructType = original_df.schema\n            empty_df = spark.createDataFrame([], schema)\n            empty_df.write.mode(\"overwrite\").insertInto(f\"{db_name}.{table_name}\")\n\n        logger.info(f\"Table '{table_identifier}' successfully truncated.\")\n\n    except Exception as e:\n        logger.error(\n            f\"An error occurred while truncating the table '{table_identifier}': {e}\",\n        )\n        raise",
        "variables": [
            "error_msg",
            "partition",
            "original_df",
            "partition_spec",
            "partitions",
            "db_name",
            "table_identifier",
            "empty_df",
            "schema",
            "spark",
            "table_name",
            "e"
        ],
        "docstring": "Truncate an External Hive table stored on S3 or HDFS.\n\nParameters\n----------\nspark\n    Active SparkSession.\ntable_identifier\n    The name of the Hive table to truncate. This can either be in the format\n    '<database>.<table>' or simply '<table>' if the current Spark session\n    has a database set.\n\nReturns\n-------\nNone\n    This function does not return any value. It performs an action of\n    truncating the table.\n\nRaises\n------\nValueError\n    If the table name is incorrectly formatted, the database is not provided\n    when required, or if the table does not exist.\nAnalysisException\n    If there is an issue with partition operations or SQL queries.\nException\n    If there is a general failure during the truncation process.\n\nExamples\n--------\nTruncate a Hive table named 'my_database.my_table':\n\n>>> truncate_external_hive_table(spark, 'my_database.my_table')\n\nOr, if the current Spark session already has a database set:\n\n>>> spark.catalog.setCurrentDatabase('my_database')\n>>> truncate_external_hive_table(spark, 'my_table')"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "cache_time_df",
        "code_chunk": "def cache_time_df(df: SparkDF) -> None:\n    \"\"\"Cache a PySpark DataFrame and print the time taken to cache and count it.\n\n    Parameters\n    ----------\n    df\n        The PySpark DataFrame to cache.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if not isinstance(df, SparkDF):\n        msg = \"Input must be a PySpark DataFrame.\"\n        raise TypeError(msg)\n\n    start_time = time.time()\n    df.cache().count()\n    end_time = time.time()\n    elapsed_time = round(end_time - start_time, 2)\n    logger.info(f\"Cached in {elapsed_time} seconds\")",
        "variables": [
            "start_time",
            "end_time",
            "df",
            "elapsed_time",
            "msg"
        ],
        "docstring": "Cache a PySpark DataFrame and print the time taken to cache and count it.\n\nParameters\n----------\ndf\n    The PySpark DataFrame to cache.\n\nReturns\n-------\nNone"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "count_nulls",
        "code_chunk": "def count_nulls(\n    df: SparkDF,\n    subset_cols: Optional[Union[List[str], str]] = None,\n) -> pd.DataFrame:\n    \"\"\"Count the number of null values in the specified columns of a SparkDF.\n\n    Parameters\n    ----------\n    df\n        The PySpark DataFrame to analyze.\n    subset_cols\n        List of column names or a single column name as a string to count\n        null values for. If not provided, counts are calculated for all\n        columns.\n\n    Returns\n    -------\n    pd.DataFrame\n        A Pandas DataFrame with the count of null values per column.\n    \"\"\"\n    if not isinstance(df, SparkDF):\n        msg = \"Input must be a PySpark DataFrame.\"\n        raise TypeError(msg)\n    if isinstance(subset_cols, str):\n        subset_cols = [subset_cols]\n    if subset_cols is not None:\n        if not isinstance(subset_cols, list):\n            msg = \"subset_cols must be a list, a string, or None.\"\n            raise TypeError(msg)\n        if not all(isinstance(col, str) for col in subset_cols):\n            msg = \"All elements of subset_cols must be strings.\"\n            raise TypeError(msg)\n\n    cols = subset_cols if subset_cols else df.columns\n    null_counts = df.select(\n        [F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in cols],\n    ).toPandas()\n    return null_counts",
        "variables": [
            "df",
            "col",
            "cols",
            "subset_cols",
            "c",
            "msg",
            "null_counts"
        ],
        "docstring": "Count the number of null values in the specified columns of a SparkDF.\n\nParameters\n----------\ndf\n    The PySpark DataFrame to analyze.\nsubset_cols\n    List of column names or a single column name as a string to count\n    null values for. If not provided, counts are calculated for all\n    columns.\n\nReturns\n-------\npd.DataFrame\n    A Pandas DataFrame with the count of null values per column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "aggregate_col",
        "code_chunk": "def aggregate_col(df: SparkDF, col: str, operation: str) -> float:\n    \"\"\"Aggregate (sum, max, min, or mean) a numeric PySpark column.\n\n    Parameters\n    ----------\n    df\n        The PySpark DataFrame containing the column.\n    col\n        The name of the numeric column to aggregate.\n    operation\n        The type of aggregation to perform. Must be one of 'sum', 'max',\n        'min', or 'mean'.\n\n    Returns\n    -------\n    float\n        The result of the specified aggregation on the column.\n    \"\"\"\n    if not isinstance(df, SparkDF):\n        msg = \"Input df must be a PySpark DataFrame.\"\n        raise TypeError(msg)\n    if not isinstance(col, str):\n        msg = \"Column name must be a string.\"\n        raise TypeError(msg)\n    valid_operations = [\"sum\", \"max\", \"min\", \"mean\"]\n    if operation not in valid_operations:\n        msg = f\"`operation` must be one of {valid_operations}.\"\n        raise ValueError(msg)\n\n    result = df.agg({col: operation}).collect()[0][0]\n    logger.info(f\"{operation.capitalize()} of values in {col}: {result}\")\n    return result",
        "variables": [
            "valid_operations",
            "df",
            "col",
            "result",
            "msg",
            "operation"
        ],
        "docstring": "Aggregate (sum, max, min, or mean) a numeric PySpark column.\n\nParameters\n----------\ndf\n    The PySpark DataFrame containing the column.\ncol\n    The name of the numeric column to aggregate.\noperation\n    The type of aggregation to perform. Must be one of 'sum', 'max',\n    'min', or 'mean'.\n\nReturns\n-------\nfloat\n    The result of the specified aggregation on the column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "get_unique",
        "code_chunk": "def get_unique(\n    df: SparkDF,\n    col: str,\n    remove_null: bool = False,\n    verbose: bool = True,\n) -> List[Optional[Union[str, int, float]]]:\n    \"\"\"Return a list of unique values in a PySpark DataFrame column.\n\n    Parameters\n    ----------\n    df\n        The PySpark DataFrame containing the column.\n    col\n        The name of the column to analyze.\n    remove_null\n        Whether to remove null values from output. Default is False.\n    verbose\n        Whether to log the number of unique values. Default is True.\n\n    Returns\n    -------\n    List\n        A list of unique values from the specified column.\n    \"\"\"\n    if not isinstance(df, SparkDF):\n        msg = \"Input df must be a PySpark DataFrame.\"\n        raise TypeError(msg)\n    if not isinstance(col, str):\n        msg = \"Column name must be a string.\"\n        raise TypeError(msg)\n    if not isinstance(remove_null, bool):\n        msg = \"remove_null must be a boolean.\"\n        raise TypeError(msg)\n    if not isinstance(verbose, bool):\n        msg = \"verbose must be a boolean.\"\n        raise TypeError(msg)\n\n    unique_vals = df.select(col).distinct().rdd.map(lambda r: r[0]).collect()\n    if remove_null:\n        unique_vals = [c for c in unique_vals if c is not None]\n    unique_vals = sorted(unique_vals, key=lambda x: (x is None, x))\n    if verbose:\n        logger.info(f\"{len(unique_vals)} unique values in {col}\")\n    return unique_vals",
        "variables": [
            "df",
            "col",
            "verbose",
            "remove_null",
            "c",
            "msg",
            "unique_vals"
        ],
        "docstring": "Return a list of unique values in a PySpark DataFrame column.\n\nParameters\n----------\ndf\n    The PySpark DataFrame containing the column.\ncol\n    The name of the column to analyze.\nremove_null\n    Whether to remove null values from output. Default is False.\nverbose\n    Whether to log the number of unique values. Default is True.\n\nReturns\n-------\nList\n    A list of unique values from the specified column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "drop_duplicates_reproducible",
        "code_chunk": "def drop_duplicates_reproducible(\n    df: SparkDF,\n    col: str,\n    id_col: Optional[str] = None,\n) -> SparkDF:\n    \"\"\"Remove duplicates from a PySpark DataFrame in a repeatable manner.\n\n    Parameters\n    ----------\n    df\n        The PySpark DataFrame.\n    col\n        The column to partition by for removing duplicates.\n    id_col\n        The column to use for ordering within each partition. If None, a\n        unique ID column is generated.\n\n    Returns\n    -------\n    SparkDF\n        The SparkDF with duplicates removed.\n    \"\"\"\n    if not isinstance(df, SparkDF):\n        msg = \"df must be a PySpark DataFrame.\"\n        raise TypeError(msg)\n    if not isinstance(col, str):\n        msg = \"col must be a string.\"\n        raise TypeError(msg)\n    if col not in df.columns:\n        msg = f\"{col} does not exist in the SparkDF.\"\n        raise ValueError(msg)\n    if id_col is not None:\n        if not isinstance(id_col, str):\n            msg = \"id_col must be a string or None.\"\n            raise TypeError(msg)\n        if id_col not in df.columns:\n            msg = f\"{id_col} not in the SparkDF.\"\n            raise ValueError(msg)\n\n    if id_col is None:\n        df = df.withColumn(\"dup_id\", F.monotonically_increasing_id())\n        id_col = \"dup_id\"\n\n    window_spec = Window.partitionBy(col).orderBy(id_col)\n    df = df.withColumn(\"rank\", F.rank().over(window_spec))\n    df = df.filter(F.col(\"rank\") == 1)\n    df = df.drop(\"dup_id\", \"rank\")\n    return df",
        "variables": [
            "df",
            "col",
            "id_col",
            "window_spec",
            "msg"
        ],
        "docstring": "Remove duplicates from a PySpark DataFrame in a repeatable manner.\n\nParameters\n----------\ndf\n    The PySpark DataFrame.\ncol\n    The column to partition by for removing duplicates.\nid_col\n    The column to use for ordering within each partition. If None, a\n    unique ID column is generated.\n\nReturns\n-------\nSparkDF\n    The SparkDF with duplicates removed."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "apply_col_func",
        "code_chunk": "def apply_col_func(\n    df: SparkDF,\n    cols: List[str],\n    func: Callable[[SparkDF, str], SparkDF],\n) -> SparkDF:\n    \"\"\"Apply a function to a list of columns in a PySpark DataFrame.\n\n    Parameters\n    ----------\n    df\n        The PySpark DataFrame.\n    cols\n        List of column names to apply the function to.\n    func\n        The function to apply, which should accept two arguments: (df, col).\n\n    Returns\n    -------\n    SparkDF\n        The SparkDF after applying the function to each column.\n    \"\"\"\n    if not isinstance(df, SparkDF):\n        msg = \"df must be a PySpark DataFrame.\"\n        raise TypeError(msg)\n    if not isinstance(cols, list):\n        msg = \"cols must be a list of strings.\"\n        raise TypeError(msg)\n    if not all(isinstance(col, str) for col in cols):\n        msg = \"All elements in cols must be strings.\"\n        raise TypeError(msg)\n    if not all(col in df.columns for col in cols):\n        msg = \"All column names in cols must exist in the SparkDF.\"\n        raise ValueError(msg)\n    if not callable(func):\n        msg = \"func must be a callable function.\"\n        raise TypeError(msg)\n\n    for col in cols:\n        df = func(df, col)\n    return df",
        "variables": [
            "func",
            "df",
            "col",
            "msg",
            "cols"
        ],
        "docstring": "Apply a function to a list of columns in a PySpark DataFrame.\n\nParameters\n----------\ndf\n    The PySpark DataFrame.\ncols\n    List of column names to apply the function to.\nfunc\n    The function to apply, which should accept two arguments: (df, col).\n\nReturns\n-------\nSparkDF\n    The SparkDF after applying the function to each column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "pyspark_random_uniform",
        "code_chunk": "def pyspark_random_uniform(\n    df: SparkDF,\n    output_colname: str,\n    lower_bound: float = 0,\n    upper_bound: float = 1,\n    seed: Optional[int] = None,\n) -> SparkDF:\n    \"\"\"Mimic numpy.random.uniform for PySpark.\n\n    Parameters\n    ----------\n    df\n        The PySpark DataFrame to which the column will be added.\n    output_colname\n        The name of the new column to be created.\n    lower_bound\n        The lower bound of the uniform distribution. Defaults to 0.\n    upper_bound\n        The upper bound of the uniform distribution. Defaults to 1.\n    seed\n        Seed for random number generation. Defaults to None for\n        non-deterministic results.\n\n    Returns\n    -------\n    SparkDF\n        The SparkDF with the new column added.\n    \"\"\"\n    if not isinstance(df, SparkDF):\n        msg = \"df must be a PySpark DataFrame.\"\n        raise TypeError(msg)\n    if not isinstance(output_colname, str):\n        msg = \"output_colname must be a string.\"\n        raise TypeError(msg)\n    if not isinstance(lower_bound, (int, float)):\n        msg = \"lower_bound must be a number.\"\n        raise TypeError(msg)\n    if not isinstance(upper_bound, (int, float)):\n        msg = \"upper_bound must be a number.\"\n        raise TypeError(msg)\n    if not lower_bound < upper_bound:\n        msg = \"lower_bound must be less than upper_bound.\"\n        raise ValueError(msg)\n\n    return df.withColumn(\n        output_colname,\n        F.rand(seed) * (upper_bound - lower_bound) + lower_bound,\n    )",
        "variables": [
            "df",
            "lower_bound",
            "msg",
            "seed",
            "upper_bound",
            "output_colname"
        ],
        "docstring": "Mimic numpy.random.uniform for PySpark.\n\nParameters\n----------\ndf\n    The PySpark DataFrame to which the column will be added.\noutput_colname\n    The name of the new column to be created.\nlower_bound\n    The lower bound of the uniform distribution. Defaults to 0.\nupper_bound\n    The upper bound of the uniform distribution. Defaults to 1.\nseed\n    Seed for random number generation. Defaults to None for\n    non-deterministic results.\n\nReturns\n-------\nSparkDF\n    The SparkDF with the new column added."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "cumulative_array",
        "code_chunk": "def cumulative_array(\n    df: SparkDF,\n    array_col: str,\n    output_colname: str,\n) -> SparkDF:\n    \"\"\"Convert a PySpark array column to a cumulative array column.\n\n    Parameters\n    ----------\n    df\n        The PySpark DataFrame containing the array column.\n    array_col\n        The name of the array column to convert.\n    output_colname\n        The name of the new column to store the cumulative array.\n\n    Returns\n    -------\n    SparkDF\n        The SparkDF with the cumulative array column added.\n    \"\"\"\n    if not isinstance(df, SparkDF):\n        msg = \"df must be a PySpark DataFrame.\"\n        raise TypeError(msg)\n    if not isinstance(array_col, str):\n        msg = \"array_col must be a string.\"\n        raise TypeError(msg)\n    if not isinstance(output_colname, str):\n        msg = \"output_colname must be a string.\"\n        raise TypeError(msg)\n    if array_col not in df.columns:\n        msg = f\"{array_col} not in SparkDF columns.\"\n        raise ValueError(msg)\n\n    return df.withColumn(\n        output_colname,\n        F.expr(\n            f\"\"\"transform({array_col}, (x, i) ->\n            aggregate(slice({array_col}, 1, i), 0D,\n            (acc, y) -> acc + y) + x)\"\"\",\n        ),\n    )",
        "variables": [
            "msg",
            "array_col",
            "output_colname",
            "df"
        ],
        "docstring": "Convert a PySpark array column to a cumulative array column.\n\nParameters\n----------\ndf\n    The PySpark DataFrame containing the array column.\narray_col\n    The name of the array column to convert.\noutput_colname\n    The name of the new column to store the cumulative array.\n\nReturns\n-------\nSparkDF\n    The SparkDF with the cumulative array column added."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "union_mismatched_dfs",
        "code_chunk": "def union_mismatched_dfs(df1: SparkDF, df2: SparkDF) -> SparkDF:\n    \"\"\"Perform a union between PySpark DataFrames with mismatched column names.\n\n    Parameters\n    ----------\n    df1\n        The first PySpark DataFrame.\n    df2\n        The second PySpark DataFrame.\n\n    Returns\n    -------\n    SparkDF\n        A SparkDF resulting from the union of df1 and df2, with missing\n        columns filled with null values.\n    \"\"\"\n    if not isinstance(df1, SparkDF):\n        msg = \"df1 must be a PySpark DataFrame.\"\n        raise TypeError(msg)\n    if not isinstance(df2, SparkDF):\n        msg = \"df2 must be a PySpark DataFrame.\"\n        raise TypeError(msg)\n\n    diff1 = [c for c in df2.columns if c not in df1.columns]\n    diff2 = [c for c in df1.columns if c not in df2.columns]\n\n    df1_expanded = df1.select(\"*\", *[F.lit(None).alias(c) for c in diff1])\n    df2_expanded = df2.select(\"*\", *[F.lit(None).alias(c) for c in diff2])\n\n    return df1_expanded.unionByName(df2_expanded)",
        "variables": [
            "df2_expanded",
            "diff2",
            "diff1",
            "c",
            "df1_expanded",
            "df1",
            "msg",
            "df2"
        ],
        "docstring": "Perform a union between PySpark DataFrames with mismatched column names.\n\nParameters\n----------\ndf1\n    The first PySpark DataFrame.\ndf2\n    The second PySpark DataFrame.\n\nReturns\n-------\nSparkDF\n    A SparkDF resulting from the union of df1 and df2, with missing\n    columns filled with null values."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "sum_columns",
        "code_chunk": "def sum_columns(\n    df: SparkDF,\n    cols_to_sum: List[str],\n    output_col: str,\n) -> SparkDF:\n    \"\"\"Calculate row-wise sum of specified PySpark columns.\n\n    Parameters\n    ----------\n    df\n        The PySpark DataFrame to modify.\n    cols_to_sum\n        List of column names to sum together.\n    output_col\n        The name of the new column to create with the sum.\n\n    Returns\n    -------\n    SparkDF\n        The SparkDF with the new column added.\n    \"\"\"\n    if not isinstance(df, SparkDF):\n        msg = \"df must be a PySpark DataFrame.\"\n        raise TypeError(msg)\n    if not isinstance(cols_to_sum, list):\n        msg = \"cols_to_sum must be a list.\"\n        raise TypeError(msg)\n    if not all(isinstance(col, str) for col in cols_to_sum):\n        msg = \"All elements in cols_to_sum must be strings.\"\n        raise TypeError(msg)\n    if not isinstance(output_col, str):\n        msg = \"output_col must be a string.\"\n        raise TypeError(msg)\n\n    cols_to_sum = [F.col(col) for col in cols_to_sum]\n    df = df.withColumn(\n        output_col,\n        functools.reduce(lambda col1, col2: col1 + col2, cols_to_sum),\n    )\n    return df",
        "variables": [
            "cols_to_sum",
            "output_col",
            "df",
            "col",
            "msg"
        ],
        "docstring": "Calculate row-wise sum of specified PySpark columns.\n\nParameters\n----------\ndf\n    The PySpark DataFrame to modify.\ncols_to_sum\n    List of column names to sum together.\noutput_col\n    The name of the new column to create with the sum.\n\nReturns\n-------\nSparkDF\n    The SparkDF with the new column added."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "set_nulls",
        "code_chunk": "def set_nulls(\n    df: SparkDF,\n    column: str,\n    values: Union[str, List[str]],\n) -> SparkDF:\n    \"\"\"Replace specified values with nulls in given column of PySpark df.\n\n    Parameters\n    ----------\n    df\n        The PySpark DataFrame to modify.\n    column\n        The name of the column in which to replace values.\n    values\n        The value(s) to replace with nulls.\n\n    Returns\n    -------\n    SparkDF\n        The SparkDF with specified values replaced by nulls.\n    \"\"\"\n    if not isinstance(df, SparkDF):\n        msg = \"df must be a PySpark DataFrame.\"\n        raise TypeError(msg)\n    if not isinstance(column, str):\n        msg = \"column must be a string.\"\n        raise TypeError(msg)\n    if isinstance(values, str):\n        values = [values]\n    if not isinstance(values, list):\n        msg = \"values must be a list of strings or a string.\"\n        raise TypeError(msg)\n    if not all(isinstance(value, str) for value in values):\n        msg = \"All elements in values must be strings.\"\n        raise TypeError(msg)\n\n    if isinstance(values, str):\n        values = [values]\n    for value in values:\n        df = df.withColumn(\n            column,\n            F.when(F.col(column) != value, F.col(column)).otherwise(F.lit(None)),\n        )\n    return df",
        "variables": [
            "values",
            "column",
            "df",
            "msg",
            "value"
        ],
        "docstring": "Replace specified values with nulls in given column of PySpark df.\n\nParameters\n----------\ndf\n    The PySpark DataFrame to modify.\ncolumn\n    The name of the column in which to replace values.\nvalues\n    The value(s) to replace with nulls.\n\nReturns\n-------\nSparkDF\n    The SparkDF with specified values replaced by nulls."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "union_multi_dfs",
        "code_chunk": "def union_multi_dfs(df_list: List[SparkDF]) -> SparkDF:\n    \"\"\"Perform a union on all SparkDFs in the provided list.\n\n    Note\n    ----\n    All SparkDFs must have the same columns.\n\n    Parameters\n    ----------\n    df_list\n        List of PySpark DataFrames to union.\n\n    Returns\n    -------\n    SparkDF\n        A SparkDF that is the result of the union of all SparkDFs in the list.\n    \"\"\"\n    if not isinstance(df_list, list):\n        msg = \"df_list must be a list.\"\n        raise TypeError(msg)\n    if not len(df_list) > 0:\n        msg = \"df_list must not be empty\"\n        raise ValueError(msg)\n    if not all(isinstance(df, SparkDF) for df in df_list):\n        msg = \"All elements in df_list must be PySpark DataFrames.\"\n        raise TypeError(msg)\n\n    combined_df = functools.reduce(lambda df1, df2: df1.union(df2), df_list)\n    return combined_df",
        "variables": [
            "msg",
            "combined_df",
            "df",
            "df_list"
        ],
        "docstring": "Perform a union on all SparkDFs in the provided list.\n\nNote\n----\nAll SparkDFs must have the same columns.\n\nParameters\n----------\ndf_list\n    List of PySpark DataFrames to union.\n\nReturns\n-------\nSparkDF\n    A SparkDF that is the result of the union of all SparkDFs in the list."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "join_multi_dfs",
        "code_chunk": "def join_multi_dfs(\n    df_list: List[SparkDF],\n    on: Union[str, List[str]],\n    how: str,\n) -> SparkDF:\n    \"\"\"Join multiple Spark SparkDFs together.\n\n    Parameters\n    ----------\n    df_list\n        List of Spark SparkDFs to join.\n    on\n        Column(s) on which to join the SparkDFs.\n    how\n        Type of join to perform (e.g., 'inner', 'outer', 'left', 'right').\n\n    Returns\n    -------\n    SparkDF\n        A SparkDF that is the result of joining all SparkDFs in the list.\n    \"\"\"\n    if not isinstance(df_list, list):\n        msg = \"df_list must be a list of SparkDFs\"\n        raise TypeError(msg)\n    if not all(isinstance(df, SparkDF) for df in df_list):\n        msg = \"All elements in df_list must be SparkDFs\"\n        raise TypeError(msg)\n    if not isinstance(on, (str, list)):\n        msg = \"'on' must be a string or a list of strings\"\n        raise TypeError(msg)\n    if isinstance(on, list) and not all(isinstance(col, str) for col in on):\n        msg = \"All elements in 'on' must be strings\"\n        raise ValueError(msg)\n    valid_join_types = [\"inner\", \"outer\", \"left\", \"right\"]\n    if how not in valid_join_types:\n        msg = f\"'how' must be one of {valid_join_types}\"\n        raise ValueError(msg)\n\n    joined_df = functools.reduce(lambda df1, df2: df1.join(df2, on, how), df_list)\n    return joined_df",
        "variables": [
            "df",
            "valid_join_types",
            "col",
            "joined_df",
            "msg",
            "on",
            "df_list",
            "how"
        ],
        "docstring": "Join multiple Spark SparkDFs together.\n\nParameters\n----------\ndf_list\n    List of Spark SparkDFs to join.\non\n    Column(s) on which to join the SparkDFs.\nhow\n    Type of join to perform (e.g., 'inner', 'outer', 'left', 'right').\n\nReturns\n-------\nSparkDF\n    A SparkDF that is the result of joining all SparkDFs in the list."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "map_column_values",
        "code_chunk": "def map_column_values(\n    df: SparkDF,\n    dict_: Dict[str, str],\n    input_col: str,\n    output_col: Union[str, None] = None,\n) -> SparkDF:\n    \"\"\"Map PySpark column to dictionary keys.\n\n    Parameters\n    ----------\n    df\n        The PySpark DataFrame to modify.\n    dict_\n        Dictionary for mapping values in input_col to new values.\n    input_col\n        The name of the column to replace values in.\n    output_col\n        The name of the new column with replaced values. Defaults to\n        input_col if not provided.\n\n    Returns\n    -------\n    SparkDF\n        The SparkDF with the new column added.\n    \"\"\"\n    if not isinstance(df, SparkDF):\n        msg = \"df must be a PySpark DataFrame.\"\n        raise TypeError(msg)\n    if not isinstance(dict_, dict):\n        msg = \"dict_ must be a dictionary.\"\n        raise TypeError(msg)\n    if not isinstance(input_col, str):\n        msg = \"input_col must be a string.\"\n        raise TypeError(msg)\n    if output_col is not None:\n        if not isinstance(output_col, str):\n            msg = \"output_col must be a string.\"\n            raise TypeError(msg)\n\n    if output_col is None:\n        output_col = input_col\n\n    mapping_expr = F.create_map([F.lit(x) for x in itertools.chain(*dict_.items())])\n\n    df = df.withColumn(\n        output_col,\n        F.coalesce(mapping_expr.getItem(F.col(input_col)), F.col(input_col)),\n    )\n    return df",
        "variables": [
            "dict_",
            "output_col",
            "df",
            "mapping_expr",
            "input_col",
            "x",
            "msg"
        ],
        "docstring": "Map PySpark column to dictionary keys.\n\nParameters\n----------\ndf\n    The PySpark DataFrame to modify.\ndict_\n    Dictionary for mapping values in input_col to new values.\ninput_col\n    The name of the column to replace values in.\noutput_col\n    The name of the new column with replaced values. Defaults to\n    input_col if not provided.\n\nReturns\n-------\nSparkDF\n    The SparkDF with the new column added."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "caller",
        "code_chunk": "def caller(func: Callable[[Union[str, SparkCol]], SparkCol]):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            varnames = func.__code__.co_varnames\n            if args:\n                args = [\n                    (\n                        _convert_to_spark_col(arg)\n                        if varnames[i] not in exclude\n                        else arg\n                    )  # noqa: E501\n                    for i, arg in enumerate(args)\n                ]\n            if kwargs:\n                kwargs = {\n                    k: (\n                        _convert_to_spark_col(kwarg) if k not in exclude else kwarg\n                    )  # noqa: E501\n                    for k, kwarg in kwargs.items()\n                }\n            return func(*args, **kwargs)\n\n        return wrapper",
        "variables": [
            "func",
            "k",
            "varnames",
            "kwarg",
            "arg",
            "i",
            "args",
            "kwargs"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\pyspark.py",
        "function_name": "wrapper",
        "code_chunk": "def wrapper(*args, **kwargs):\n            varnames = func.__code__.co_varnames\n            if args:\n                args = [\n                    (\n                        _convert_to_spark_col(arg)\n                        if varnames[i] not in exclude\n                        else arg\n                    )  # noqa: E501\n                    for i, arg in enumerate(args)\n                ]\n            if kwargs:\n                kwargs = {\n                    k: (\n                        _convert_to_spark_col(kwarg) if k not in exclude else kwarg\n                    )  # noqa: E501\n                    for k, kwarg in kwargs.items()\n                }\n            return func(*args, **kwargs)",
        "variables": [
            "k",
            "varnames",
            "kwarg",
            "arg",
            "i",
            "args",
            "kwargs"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "always_iterable_local",
        "code_chunk": "def always_iterable_local(obj: Any) -> Callable:\n    \"\"\"Supplement more-itertools `always_iterable` to also exclude dicts.\n\n    By default it would convert a dictionary to an iterable of just its keys,\n    dropping all the values. This change makes it so dictionaries are not\n    altered (similar to how strings aren't broken down).\n    \"\"\"\n    return always_iterable(obj, base_type=(str, bytes, dict))",
        "variables": [
            "obj"
        ],
        "docstring": "Supplement more-itertools `always_iterable` to also exclude dicts.\n\nBy default it would convert a dictionary to an iterable of just its keys,\ndropping all the values. This change makes it so dictionaries are not\naltered (similar to how strings aren't broken down)."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "tuple_convert",
        "code_chunk": "def tuple_convert(obj: Any) -> Tuple[Any]:\n    \"\"\"Convert object to tuple using more-itertools' `always_iterable`.\"\"\"\n    return tuple(always_iterable_local(obj))",
        "variables": [
            "obj"
        ],
        "docstring": "Convert object to tuple using more-itertools' `always_iterable`."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "list_convert",
        "code_chunk": "def list_convert(obj: Any) -> List[Any]:\n    \"\"\"Convert object to list using more-itertools' `always_iterable`.\"\"\"\n    return list(always_iterable_local(obj))",
        "variables": [
            "obj"
        ],
        "docstring": "Convert object to list using more-itertools' `always_iterable`."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "extend_lists",
        "code_chunk": "def extend_lists(\n    sections: List[List[str]],\n    elements_to_add: List[str],\n) -> None:\n    \"\"\"Check list elements are unique then append to existing list.\n\n    Note the `.extend` method in Python overwrites each section. There\n    is no need to assign a variable to this function, the section will\n    update automatically.\n\n    The function can be used with the `load_config` function to extend a\n    value list in a config yaml file. For example with a `config.yaml`\n    file as per below:\n\n    ```\n    input_columns\n        - col_a\n        - col_b\n\n    output_columns\n        - col_b\n    ```\n\n    To add column `col_c` the function can be utilised as follows:\n\n    ```\n    config = load_config(\"config.yaml\")\n\n    sections = [config['input_columns'], config['output_columns']]\n    elements_to_add = ['col_c']\n\n    extend_lists(sections, elements_to_add)\n    ```\n\n    The output will be as follows.\n\n    ```\n    input_columns\n        - col_a\n        - col_b\n        - col_c\n\n    output_columns\n        - col_b\n        - col_c\n    ```\n\n    Parameters\n    ----------\n    sections\n        The section to be updated with the extra elements.\n    elements_to_add\n        The new elements to add to the specified sections.\n\n    Returns\n    -------\n    None\n        Note the `.extend` method in Python overwrites the sections.\n        There is no need to assign a variable to this function, the\n        section will update automatically.\n    \"\"\"\n    for section in sections:\n        missing_elements = [\n            element for element in elements_to_add if element not in section\n        ]\n        section.extend(missing_elements)\n\n    return None",
        "variables": [
            "elements_to_add",
            "section",
            "missing_elements",
            "sections",
            "element"
        ],
        "docstring": "Check list elements are unique then append to existing list.\n\nNote the `.extend` method in Python overwrites each section. There\nis no need to assign a variable to this function, the section will\nupdate automatically.\n\nThe function can be used with the `load_config` function to extend a\nvalue list in a config yaml file. For example with a `config.yaml`\nfile as per below:\n\n```\ninput_columns\n    - col_a\n    - col_b\n\noutput_columns\n    - col_b\n```\n\nTo add column `col_c` the function can be utilised as follows:\n\n```\nconfig = load_config(\"config.yaml\")\n\nsections = [config['input_columns'], config['output_columns']]\nelements_to_add = ['col_c']\n\nextend_lists(sections, elements_to_add)\n```\n\nThe output will be as follows.\n\n```\ninput_columns\n    - col_a\n    - col_b\n    - col_c\n\noutput_columns\n    - col_b\n    - col_c\n```\n\nParameters\n----------\nsections\n    The section to be updated with the extra elements.\nelements_to_add\n    The new elements to add to the specified sections.\n\nReturns\n-------\nNone\n    Note the `.extend` method in Python overwrites the sections.\n    There is no need to assign a variable to this function, the\n    section will update automatically."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "overwrite_dictionary",
        "code_chunk": "def overwrite_dictionary(\n    base_dict: Mapping[str, Any],\n    override_dict: Mapping[str, Any],\n) -> Dict[str, Any]:\n    \"\"\"Overwrite dictionary values with user defined values.\n\n    The following restrictions are in place:\n    * base_dict and override_dict have the same value which is not dictionary\n      then override_dict has priority.\n    * If base_dict contains dictionary and override_dict contains a value (e.g.\n      string or list) with the same key, priority is upon base_dict and\n      the override_dict value is ignored.\n    * If key is in override_dict but not in base_dict then an Exception is\n      raised and code stops.\n    * Any other restrictions will require code changes.\n\n    Parameters\n    ----------\n    base_dict\n        Dictionary containing existing key value pairs.\n    override_dict\n        Dictionary containing new keys/values to inset into base_dict.\n\n    Returns\n    -------\n    Dict[str, Any]\n        The base_dict with any keys matching the override_dict being replaced.\n        Any keys not present in base_dict are appended.\n\n    Example\n    -------\n    >>> dic1 = {\"var1\": \"value1\", \"var2\": {\"var3\": 1.1, \"var4\": 4.4}, \"var5\": [1, 2, 3]}\n    >>> dic2 = {\"var2\": {\"var3\": 9.9}}\n\n    >>> overwrite_dictionary(dic1, dic2)\n    {'var1': 'value1', 'var2': {'var3': 9.9, 'var4': 4.4}, 'var5': [1, 2, 3]}\n\n    >>> dic3 = {\"var2\": {\"var3\": 9.9}, \"var6\": -1}\n    >>> overwrite_dictionary(dic1, dic3)\n    ERROR __main__: ('var6', -1) not in base_dict\n\n    Notes\n    -----\n    Modified from: https://stackoverflow.com/a/58742155\n\n    Warning\n    -------\n    Due to recursive nature of function, the function will overwrite the\n    base_dict object that is passed into the original function.\n\n    Raises\n    ------\n    ValueError\n        If a key is present in override_dict but not base_dict.\n    \"\"\"  # noqa: E501\n    for key, val in base_dict.items():\n        if type(val) == dict:\n            if key in override_dict and type(override_dict[key]) == dict:\n                overwrite_dictionary(base_dict[key], override_dict[key])\n            elif key in override_dict and type(override_dict[key]) != dict:\n                logger.warning(\n                    f\"\"\"\n                Not overriding key: {key} in base dictionary as the value type\n                for the base dictionary are of higher priority than the\n                override.\n\n                Base dictionary values for key are of type:\n                {type(base_dict[key])}\n                and have values:\n                {base_dict[key]}\n\n                Override dictionary values for key are of type:\n                {type(override_dict[key])}\n                and have values:\n                {override_dict[key]}\n                \"\"\",\n                )\n            else:\n                # Key in base_dict not present in override_dict so do nothing.\n                pass\n        else:\n            if key in override_dict:\n                base_dict[key] = override_dict[key]\n            else:\n                # Key in base_dict not present in override_dict so do nothing.\n                pass\n\n    for key, val in override_dict.items():\n        if key not in base_dict:\n            msg = f\"\"\"\n            The key, value pair:\n            {key, val}\n            is not in the base dictionary\n            {json.dumps(base_dict, indent=4)}\n            \"\"\"\n            logger.error(msg)\n            raise ValueError(msg)\n\n    return base_dict",
        "variables": [
            "val",
            "override_dict",
            "key",
            "msg",
            "base_dict"
        ],
        "docstring": "Overwrite dictionary values with user defined values.\n\nThe following restrictions are in place:\n* base_dict and override_dict have the same value which is not dictionary\n  then override_dict has priority.\n* If base_dict contains dictionary and override_dict contains a value (e.g.\n  string or list) with the same key, priority is upon base_dict and\n  the override_dict value is ignored.\n* If key is in override_dict but not in base_dict then an Exception is\n  raised and code stops.\n* Any other restrictions will require code changes.\n\nParameters\n----------\nbase_dict\n    Dictionary containing existing key value pairs.\noverride_dict\n    Dictionary containing new keys/values to inset into base_dict.\n\nReturns\n-------\nDict[str, Any]\n    The base_dict with any keys matching the override_dict being replaced.\n    Any keys not present in base_dict are appended.\n\nExample\n-------\n>>> dic1 = {\"var1\": \"value1\", \"var2\": {\"var3\": 1.1, \"var4\": 4.4}, \"var5\": [1, 2, 3]}\n>>> dic2 = {\"var2\": {\"var3\": 9.9}}\n\n>>> overwrite_dictionary(dic1, dic2)\n{'var1': 'value1', 'var2': {'var3': 9.9, 'var4': 4.4}, 'var5': [1, 2, 3]}\n\n>>> dic3 = {\"var2\": {\"var3\": 9.9}, \"var6\": -1}\n>>> overwrite_dictionary(dic1, dic3)\nERROR __main__: ('var6', -1) not in base_dict\n\nNotes\n-----\nModified from: https://stackoverflow.com/a/58742155\n\nWarning\n-------\nDue to recursive nature of function, the function will overwrite the\nbase_dict object that is passed into the original function.\n\nRaises\n------\nValueError\n    If a key is present in override_dict but not base_dict."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "calc_product_of_dict_values",
        "code_chunk": "def calc_product_of_dict_values(\n    **kwargs: Mapping[str, Union[str, float, Iterable]],\n) -> Mapping[str, any]:\n    \"\"\"Create cartesian product of values for each kwarg.\n\n    In order to create product of values, the values are converted to\n    a list so that product of values can be derived.\n\n    Yields\n    ------\n        Next result of cartesian product of kwargs values.\n\n    Example\n    -------\n    my_dict = {\n        'key1': 1,\n        'key2': [2, 3, 4]\n    }\n\n    list(calc_product_of_dict_values(**my_dict))\n    >>> [{'key1': 1, 'key2': 2}, {'key1': 1, 'key2': 3}, {'key1': 1, 'key2': 4}]\n\n    Notes\n    -----\n    Modified from: https://stackoverflow.com/a/5228294\n    \"\"\"\n    # noqa: E501\n    kwargs = {key: list_convert(value) for key, value in kwargs.items()}\n\n    keys = kwargs.keys()\n    vals = kwargs.values()\n\n    for instance in itertools.product(*vals):\n        yield dict(zip(keys, instance))",
        "variables": [
            "instance",
            "vals",
            "key",
            "value",
            "kwargs",
            "keys"
        ],
        "docstring": "Create cartesian product of values for each kwarg.\n\nIn order to create product of values, the values are converted to\na list so that product of values can be derived.\n\nYields\n------\n    Next result of cartesian product of kwargs values.\n\nExample\n-------\nmy_dict = {\n    'key1': 1,\n    'key2': [2, 3, 4]\n}\n\nlist(calc_product_of_dict_values(**my_dict))\n>>> [{'key1': 1, 'key2': 2}, {'key1': 1, 'key2': 3}, {'key1': 1, 'key2': 4}]\n\nNotes\n-----\nModified from: https://stackoverflow.com/a/5228294"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "convert_date_strings_to_datetimes",
        "code_chunk": "def convert_date_strings_to_datetimes(\n    start_date: str,\n    end_date: str,\n) -> Tuple[pd.Timestamp, pd.Timestamp]:\n    \"\"\"Convert start and end dates from strings to timestamps.\n\n    Parameters\n    ----------\n    start_date\n        Datetime like object which is used to define the start date for filter.\n        Acceptable string formats include (but not limited to): MMMM YYYY,\n        YYYY-MM, YYYY-MM-DD, DD MMM YYYY etc. If only month and year specified\n        the start_date is set as first day of month.\n    end_date\n        Datetime like object which is used to define the start date for filter.\n        Acceptable string formats include (but not limited to): MMMM YYYY,\n        YYYY-MM, YYYY-MM-DD, DD MMM YYYY etc. If only month and year specified\n        the end_date is set as final day of month.\n\n    Returns\n    -------\n    tuple[pd.Timestamp, pd.Timestamp]\n        Tuple where the first value is the start date and the second the end\n        date.\n    \"\"\"\n    shift_end_date_to_month_end = False\n\n    year_month_formats = [\n        \"%B %Y\",  # January 2020\n        \"%b %Y\",  # Jan 2020\n        \"%Y %B\",  # 2020 January\n        \"%Y %b\",  # 2020 Jan\n        # '%Y-%m',  # 2020-01 - also matches 2020-01-01\n        # '%Y-%-m',  # 2020-1 - also matches 2020-01-01\n        # '%Y %m',  # 2020 01 - also matches 2020-01-01\n        # '%Y %-m',  # 2020 1 - also matches 2020-01-01\n        \"%m-%Y\",  # 01-2020\n        \"%-m-%Y\",  # 1-2020\n        \"%m %Y\",  # 01 2020\n        \"%-m %Y\",  # 1 2020\n    ]\n\n    # if the end_date format matches one of the above then it is assumed the\n    # used wants to use all days in that month.\n    for date_format in year_month_formats:\n        try:\n            pd.to_datetime(end_date, format=date_format)\n            shift_end_date_to_month_end = True\n\n        except ValueError:\n            pass\n\n    if shift_end_date_to_month_end:\n        end_date = pd.to_datetime(end_date) + MonthEnd(0)\n\n    # Obtain the last \"moment\" of the end_date to ensure any hourly data for\n    # the date is included\n    # https://medium.com/@jorlugaqui/how-to-get-the-latest-earliest-moment-from-a-day-in-python-aa8999bea945  # noqa: E501\n    end_date = datetime.combine(pd.to_datetime(end_date), time.max)\n\n    # Ensure dates are timestamp to enable inclusive filtering of provided end\n    # date, see https://stackoverflow.com/a/43403904 for info.\n    return (pd.Timestamp(start_date), pd.Timestamp(end_date))",
        "variables": [
            "year_month_formats",
            "end_date",
            "shift_end_date_to_month_end",
            "date_format",
            "start_date"
        ],
        "docstring": "Convert start and end dates from strings to timestamps.\n\nParameters\n----------\nstart_date\n    Datetime like object which is used to define the start date for filter.\n    Acceptable string formats include (but not limited to): MMMM YYYY,\n    YYYY-MM, YYYY-MM-DD, DD MMM YYYY etc. If only month and year specified\n    the start_date is set as first day of month.\nend_date\n    Datetime like object which is used to define the start date for filter.\n    Acceptable string formats include (but not limited to): MMMM YYYY,\n    YYYY-MM, YYYY-MM-DD, DD MMM YYYY etc. If only month and year specified\n    the end_date is set as final day of month.\n\nReturns\n-------\ntuple[pd.Timestamp, pd.Timestamp]\n    Tuple where the first value is the start date and the second the end\n    date."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "time_it",
        "code_chunk": "def time_it(*timer_args, **timer_kwargs) -> Callable:\n    \"\"\"Measure the execution time of a function, with options to configure Timer.\n\n    Parameters\n    ----------\n    timer_args\n        Positional arguments to pass to the Timer object.\n    timer_kwargs\n        Keyword arguments to pass to the Timer object.\n\n    Returns\n    -------\n    Callable\n        A wrapped function that includes timing measurement.\n\n    Example\n    -------\n    @time_it()\n    def example_function():\n        # Function implementation\n    \"\"\"\n\n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        def wrap(*args, **kwargs) -> Any:\n            with Timer(*timer_args, **timer_kwargs) as t:\n                result = func(*args, **kwargs)\n            logger.info(f\"<Executed {func.__name__} in {t.last:.2f} seconds>\")\n            return result\n\n        return wrap\n\n    return decorator",
        "variables": [
            "result",
            "timer_kwargs",
            "timer_args",
            "t"
        ],
        "docstring": "Measure the execution time of a function, with options to configure Timer.\n\nParameters\n----------\ntimer_args\n    Positional arguments to pass to the Timer object.\ntimer_kwargs\n    Keyword arguments to pass to the Timer object.\n\nReturns\n-------\nCallable\n    A wrapped function that includes timing measurement.\n\nExample\n-------\n@time_it()\ndef example_function():\n    # Function implementation"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "setdiff",
        "code_chunk": "def setdiff(a: Iterable, b: Iterable) -> List[Any]:\n    \"\"\"Return a list of elements that are present in `a` but not in `b`.\n\n    Parameters\n    ----------\n    a\n        The first iterable from which elements are to be selected.\n    b\n        The second iterable containing elements to be excluded.\n\n    Returns\n    -------\n    list\n        A list of elements that are in `a` but not in `b`.\n\n    Examples\n    --------\n    >>> setdiff([1, 2, 3, 4], [3, 4, 5, 6])\n    [1, 2]\n    >>> setdiff('abcdef', 'bdf')\n    ['a', 'c', 'e']\n    >>> setdiff({1, 2, 3}, {2, 3, 4})\n    [1]\n    >>> setdiff(range(5), range(2, 7))\n    [0, 1]\n    \"\"\"\n    if not isinstance(a, Iterable) or not isinstance(b, Iterable):\n        msg = \"Both inputs must be iterable.\"\n        raise TypeError(msg)\n\n    return list(set(a) - set(b))",
        "variables": [
            "b",
            "a",
            "msg"
        ],
        "docstring": "Return a list of elements that are present in `a` but not in `b`.\n\nParameters\n----------\na\n    The first iterable from which elements are to be selected.\nb\n    The second iterable containing elements to be excluded.\n\nReturns\n-------\nlist\n    A list of elements that are in `a` but not in `b`.\n\nExamples\n--------\n>>> setdiff([1, 2, 3, 4], [3, 4, 5, 6])\n[1, 2]\n>>> setdiff('abcdef', 'bdf')\n['a', 'c', 'e']\n>>> setdiff({1, 2, 3}, {2, 3, 4})\n[1]\n>>> setdiff(range(5), range(2, 7))\n[0, 1]"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "flatten_iterable",
        "code_chunk": "def flatten_iterable(\n    iterable: Iterable,\n    types_to_flatten: Union[type, Tuple] = (list, tuple),\n) -> List:\n    \"\"\"Flatten an iterable.\n\n    Parameters\n    ----------\n    iterable\n        An iterable that may contain elements of various types.\n    types_to_flatten\n        Data type(s) that should be flattened. Defaults to (list, tuple).\n\n    Returns\n    -------\n    list\n        A flattened list with all elements from the input iterable, with\n        specified types unpacked.\n\n    Examples\n    --------\n    >>> flatten_iterable([1, [2, 3], (4, 5), 'abc'])\n    [1, 2, 3, 4, 5, 'abc']\n    >>> flatten_iterable([1, [2, 3], (4, 5), 'abc'], types_to_flatten=list)\n    [1, 2, 3, (4, 5), 'abc']\n    >>> flatten_iterable(['a', 'bc', ['d', 'e']], types_to_flatten=str)\n    ['a', 'b', 'c', 'd', 'e']\n    >>> flatten_iterable((1, [2, 3], (4, 5), 'abc'), types_to_flatten=(list, tuple))\n    (1, 2, 3, 4, 5, 'abc')\n    \"\"\"\n    if not hasattr(iterable, \"__iter__\"):\n        msg = \"`iterable` must be an iterable.\"\n        raise TypeError(msg)\n\n    if not isinstance(types_to_flatten, (type, tuple)):\n        msg = \"`types_to_flatten` must be a type or a tuple of types.\"\n        raise TypeError(msg)\n\n    if isinstance(types_to_flatten, tuple):\n        if not all(isinstance(t, type) for t in types_to_flatten):\n            msg = \"All elements in `types_to_flatten` must be types.\"\n            raise ValueError(msg)\n    else:\n        if not isinstance(types_to_flatten, type):\n            msg = \"`types_to_flatten` must be a type or a tuple of types.\"\n            raise TypeError(msg)\n\n    flattened = []\n    for item in iterable:\n        if isinstance(item, types_to_flatten):\n            flattened.extend(item)\n        else:\n            flattened.append(item)\n    return flattened",
        "variables": [
            "item",
            "iterable",
            "types_to_flatten",
            "flattened",
            "t",
            "msg"
        ],
        "docstring": "Flatten an iterable.\n\nParameters\n----------\niterable\n    An iterable that may contain elements of various types.\ntypes_to_flatten\n    Data type(s) that should be flattened. Defaults to (list, tuple).\n\nReturns\n-------\nlist\n    A flattened list with all elements from the input iterable, with\n    specified types unpacked.\n\nExamples\n--------\n>>> flatten_iterable([1, [2, 3], (4, 5), 'abc'])\n[1, 2, 3, 4, 5, 'abc']\n>>> flatten_iterable([1, [2, 3], (4, 5), 'abc'], types_to_flatten=list)\n[1, 2, 3, (4, 5), 'abc']\n>>> flatten_iterable(['a', 'bc', ['d', 'e']], types_to_flatten=str)\n['a', 'b', 'c', 'd', 'e']\n>>> flatten_iterable((1, [2, 3], (4, 5), 'abc'), types_to_flatten=(list, tuple))\n(1, 2, 3, 4, 5, 'abc')"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "convert_types_iterable",
        "code_chunk": "def convert_types_iterable(lst: Iterable, dtype: type = float) -> List:\n    \"\"\"Convert the data type of elements in an iterable.\n\n    Parameters\n    ----------\n    lst\n        The iterable whose elements are to be converted.\n    dtype\n        The target data type to which elements in the iterable should be\n        converted. Defaults to `float`.\n\n    Returns\n    -------\n    list\n        A new list with elements converted to the specified data type.\n\n    Examples\n    --------\n    >>> convert_types_iterable([1, 2, 3])\n    [1.0, 2.0, 3.0]\n\n    >>> convert_types_iterable((10, 20, 30), dtype=str)\n    ['10', '20', '30']\n\n    >>> convert_types_iterable({'a', 'b', 'c'}, dtype=ord)\n    [97, 98, 99]\n\n    >>> convert_types_iterable(['10', '20', '30'], dtype=int)\n    [10, 20, 30]\n    \"\"\"\n    if not isinstance(lst, (list, tuple, set, frozenset, range)):\n        msg = (\n            \"Input must be an iterable type such as list, tuple, set, \"\n            \"frozenset, or range.\"\n        )\n        raise TypeError(msg)\n\n    if not isinstance(dtype, type):\n        msg = \"`dtype` must be a valid type.\"\n        raise TypeError(msg)\n\n    return list(map(dtype, lst))",
        "variables": [
            "msg",
            "dtype",
            "lst"
        ],
        "docstring": "Convert the data type of elements in an iterable.\n\nParameters\n----------\nlst\n    The iterable whose elements are to be converted.\ndtype\n    The target data type to which elements in the iterable should be\n    converted. Defaults to `float`.\n\nReturns\n-------\nlist\n    A new list with elements converted to the specified data type.\n\nExamples\n--------\n>>> convert_types_iterable([1, 2, 3])\n[1.0, 2.0, 3.0]\n\n>>> convert_types_iterable((10, 20, 30), dtype=str)\n['10', '20', '30']\n\n>>> convert_types_iterable({'a', 'b', 'c'}, dtype=ord)\n[97, 98, 99]\n\n>>> convert_types_iterable(['10', '20', '30'], dtype=int)\n[10, 20, 30]"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "interleave_iterables",
        "code_chunk": "def interleave_iterables(iterable1: Iterable, iterable2: Iterable) -> List:\n    \"\"\"Interleave two iterables element by element.\n\n    Parameters\n    ----------\n    iterable1\n        The first iterable to interleave.\n    iterable2\n        The second iterable to interleave.\n\n    Returns\n    -------\n    list\n        A new list with elements from `iterable1` and `iterable2` interleaved.\n\n    Raises\n    ------\n    TypeError\n        If either of the inputs is not an iterable of types: list, tuple,\n        string, or range.\n    ValueError\n        If the lengths of the two iterables do not match.\n\n    Examples\n    --------\n    >>> interleave_iterables([1, 2, 3], [4, 5, 6])\n    [1, 4, 2, 5, 3, 6]\n\n    >>> interleave_iterables((1, 2, 3), ('a', 'b', 'c'))\n    [1, 'a', 2, 'b', 3, 'c']\n\n    >>> interleave_iterables('ABC', '123')\n    ['A', '1', 'B', '2', 'C', '3']\n\n    >>> interleave_iterables(range(3), range(10, 13))\n    [0, 10, 1, 11, 2, 12]\n    \"\"\"\n    if not isinstance(iterable1, (list, tuple, str, range)) or not isinstance(\n        iterable2,\n        (list, tuple, str, range),\n    ):\n        msg = (\n            \"Both inputs must be iterable types such as list, tuple,\"\n            \"string, or range.\"\n        )\n        raise TypeError(msg)\n\n    if len(iterable1) != len(iterable2):\n        msg = \"Both iterables must have the same length.\"\n        raise ValueError(msg)\n\n    result = [None] * (len(iterable1) + len(iterable2))\n    result[::2] = iterable1\n    result[1::2] = iterable2\n\n    return result",
        "variables": [
            "iterable2",
            "msg",
            "iterable1",
            "result"
        ],
        "docstring": "Interleave two iterables element by element.\n\nParameters\n----------\niterable1\n    The first iterable to interleave.\niterable2\n    The second iterable to interleave.\n\nReturns\n-------\nlist\n    A new list with elements from `iterable1` and `iterable2` interleaved.\n\nRaises\n------\nTypeError\n    If either of the inputs is not an iterable of types: list, tuple,\n    string, or range.\nValueError\n    If the lengths of the two iterables do not match.\n\nExamples\n--------\n>>> interleave_iterables([1, 2, 3], [4, 5, 6])\n[1, 4, 2, 5, 3, 6]\n\n>>> interleave_iterables((1, 2, 3), ('a', 'b', 'c'))\n[1, 'a', 2, 'b', 3, 'c']\n\n>>> interleave_iterables('ABC', '123')\n['A', '1', 'B', '2', 'C', '3']\n\n>>> interleave_iterables(range(3), range(10, 13))\n[0, 10, 1, 11, 2, 12]"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "pairwise_iterable",
        "code_chunk": "def pairwise_iterable(iterable: Iterable) -> zip:\n    \"\"\"Return pairs of adjacent values from the input iterable.\n\n    Parameters\n    ----------\n    iterable\n        An iterable object (e.g., list, tuple, string) from which pairs of\n        adjacent values will be generated.\n\n    Returns\n    -------\n    zip\n        An iterator of tuples, each containing a pair of adjacent values\n        from the input iterable.\n\n    Raises\n    ------\n    TypeError\n        If the input is not an iterable.\n\n    Examples\n    --------\n    >>> list(pairwise_iterable([1, 2, 3, 4]))\n    [(1, 2), (2, 3), (3, 4)]\n\n    >>> list(pairwise_iterable('abcde'))\n    [('a', 'b'), ('b', 'c'), ('c', 'd'), ('d', 'e')]\n\n    >>> list(pairwise_iterable((10, 20, 30)))\n    [(10, 20), (20, 30)]\n    \"\"\"\n    if not hasattr(iterable, \"__iter__\"):\n        msg = \"Input must be an iterable.\"\n        raise TypeError(msg)\n\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)",
        "variables": [
            "b",
            "msg",
            "iterable",
            "a"
        ],
        "docstring": "Return pairs of adjacent values from the input iterable.\n\nParameters\n----------\niterable\n    An iterable object (e.g., list, tuple, string) from which pairs of\n    adjacent values will be generated.\n\nReturns\n-------\nzip\n    An iterator of tuples, each containing a pair of adjacent values\n    from the input iterable.\n\nRaises\n------\nTypeError\n    If the input is not an iterable.\n\nExamples\n--------\n>>> list(pairwise_iterable([1, 2, 3, 4]))\n[(1, 2), (2, 3), (3, 4)]\n\n>>> list(pairwise_iterable('abcde'))\n[('a', 'b'), ('b', 'c'), ('c', 'd'), ('d', 'e')]\n\n>>> list(pairwise_iterable((10, 20, 30)))\n[(10, 20), (20, 30)]"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "merge_multi_dfs",
        "code_chunk": "def merge_multi_dfs(\n    df_list: list,\n    on: Union[str, list],\n    how: str,\n    fillna_val: Union[None, object] = None,\n) -> pd.DataFrame:\n    \"\"\"Perform consecutive merges on a list of pandas DataFrames.\n\n    Parameters\n    ----------\n    df_list\n        A list of DataFrames to be merged.\n    on\n        Column name(s) to merge on.\n    how\n        Type of merge to be performed. Must be one of 'left', 'right', 'outer',\n        'inner'.\n    fillna_val\n        Value to replace missing values with. Default is None.\n\n    Returns\n    -------\n    pd.DataFrame\n        The resulting DataFrame after merging and optional filling of missing\n        values.\n\n    Raises\n    ------\n    TypeError\n        If `df_list` is not a list of pandas DataFrames, or `on` is not a string\n        or list of strings, or `how` is not a string.\n    ValueError\n        If the `how` argument is not one of 'left', 'right', 'outer', or 'inner'.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value1': [1, 2, 3]})\n    >>> df2 = pd.DataFrame({'key': ['A', 'B'], 'value2': [4, 5]})\n    >>> df3 = pd.DataFrame({'key': ['A'], 'value3': [6]})\n    >>> merge_multi_dfs([df1, df2, df3], on='key', how='inner')\n      key  value1  value2  value3\n    0   A       1       4       6\n\n    >>> df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value1': [1, 2, 3]})\n    >>> df2 = pd.DataFrame({'key': ['A', 'B'], 'value2': [4, 5]})\n    >>> merge_multi_dfs([df1, df2], on='key', how='outer',  fillna_val=0)\n      key  value1  value2\n    0   A        1        4\n    1   B        2        5\n    2   C        3        0\n    \"\"\"\n    if not isinstance(df_list, list) or not all(\n        isinstance(df, pd.DataFrame) for df in df_list\n    ):\n        msg = \"`df_list` must be a list of pandas DataFrames.\"\n        raise TypeError(msg)\n\n    if not isinstance(on, (str, list)):\n        msg = \"`on` must be a string or a list of strings.\"\n        raise TypeError(msg)\n\n    if not isinstance(how, str):\n        msg = \"`how` must be a string.\"\n        raise TypeError(msg)\n\n    valid_how_options = [\"left\", \"right\", \"outer\", \"inner\"]\n    if how not in valid_how_options:\n        msg = f\"`how` Must be one of {valid_how_options}.\"\n        raise ValueError(msg)\n\n    merged_df = reduce(\n        lambda left, right: left.merge(right, on=on, how=how),\n        df_list,\n    )\n\n    if fillna_val is not None:\n        merged_df = merged_df.fillna(fillna_val)\n\n    return merged_df",
        "variables": [
            "fillna_val",
            "df",
            "valid_how_options",
            "merged_df",
            "msg",
            "on",
            "df_list",
            "how"
        ],
        "docstring": "Perform consecutive merges on a list of pandas DataFrames.\n\nParameters\n----------\ndf_list\n    A list of DataFrames to be merged.\non\n    Column name(s) to merge on.\nhow\n    Type of merge to be performed. Must be one of 'left', 'right', 'outer',\n    'inner'.\nfillna_val\n    Value to replace missing values with. Default is None.\n\nReturns\n-------\npd.DataFrame\n    The resulting DataFrame after merging and optional filling of missing\n    values.\n\nRaises\n------\nTypeError\n    If `df_list` is not a list of pandas DataFrames, or `on` is not a string\n    or list of strings, or `how` is not a string.\nValueError\n    If the `how` argument is not one of 'left', 'right', 'outer', or 'inner'.\n\nExamples\n--------\n>>> import pandas as pd\n>>> df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value1': [1, 2, 3]})\n>>> df2 = pd.DataFrame({'key': ['A', 'B'], 'value2': [4, 5]})\n>>> df3 = pd.DataFrame({'key': ['A'], 'value3': [6]})\n>>> merge_multi_dfs([df1, df2, df3], on='key', how='inner')\n  key  value1  value2  value3\n0   A       1       4       6\n\n>>> df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value1': [1, 2, 3]})\n>>> df2 = pd.DataFrame({'key': ['A', 'B'], 'value2': [4, 5]})\n>>> merge_multi_dfs([df1, df2], on='key', how='outer',  fillna_val=0)\n  key  value1  value2\n0   A        1        4\n1   B        2        5\n2   C        3        0"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "decorator",
        "code_chunk": "def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        def wrap(*args, **kwargs) -> Any:\n            with Timer(*timer_args, **timer_kwargs) as t:\n                result = func(*args, **kwargs)\n            logger.info(f\"<Executed {func.__name__} in {t.last:.2f} seconds>\")\n            return result\n\n        return wrap",
        "variables": [
            "func",
            "result",
            "t"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\helpers\\python.py",
        "function_name": "wrap",
        "code_chunk": "def wrap(*args, **kwargs) -> Any:\n            with Timer(*timer_args, **timer_kwargs) as t:\n                result = func(*args, **kwargs)\n            logger.info(f\"<Executed {func.__name__} in {t.last:.2f} seconds>\")\n            return result",
        "variables": [
            "args",
            "kwargs",
            "result",
            "t"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\io\\config.py",
        "function_name": "__init__",
        "code_chunk": "def __init__(\n        self: \"LoadConfig\",\n        config_path: Union[CloudPath, Path],\n        config_overrides: Optional[Config] = None,\n        config_type: Optional[Literal[\"json\", \"toml\", \"yaml\"]] = None,\n        config_validators: Optional[Dict[str, BaseModel]] = None,\n    ) -> None:\n        \"\"\"Init method.\n\n        Parameters\n        ----------\n        config_path\n            The path of the config file to be loaded.\n        config_overrides, optional\n            A dictionary containing a subset of the keys and values of the\n            config file that is initially loaded, by default None. If values\n            are provided that are not in the initial config then a ConfigError\n            is raised.\n        config_type, optional\n            The file type of the config file being loaded, by default None. If\n            not specified then this is inferred from the `config_path`.\n        config_validators, optional\n            A dictionary made up of key, value pairs where the keys refer to\n            the top level sections of the loaded config, and the values are a\n            pydantic validation class for the section, by default None. If only\n            some of the keys are specified with validators, warnings are raised\n            to alert that they have not been validated.\n        \"\"\"\n        self.config_path = config_path\n        self.config_file = config_path.name\n        self.config_dir = config_path.parent\n\n        self.config_overrides = config_overrides\n        self.config_type = config_type\n        self.config_validators = config_validators\n\n        if not self.config_type:\n            self.config_type = self.config_path.suffix.lstrip(\".\")\n\n        logger.info(f\"Loading config from file: {self.config_path}\")\n        self._config_contents = read_file(self.config_path)\n\n        self.config = self._get_config_parser()(self._config_contents)\n\n        # Save the original config prior to mutating it.\n        self.config_original = copy.deepcopy(self.config)\n        logger.debug(\n            self._print_config_string(\n                \"loaded config\",\n                self.config_original,\n            ),\n        )\n\n        if self.config_overrides:\n            try:\n                self.config = overwrite_dictionary(\n                    self.config,\n                    self.config_overrides,\n                )\n            except ValueError as e:\n                raise ConfigError(e) from e\n\n            logger.debug(\n                self._print_config_string(\n                    \"config after applying overrides\",\n                    self.config,\n                ),\n            )\n\n        if self.config_validators:\n            self._apply_config_validators()\n            logger.debug(\n                self._print_config_string(\n                    \"config after applying validation\",\n                    self.config,\n                ),\n            )\n\n        logger.info(self._print_config_string(\"using config\", self.config))\n\n        # Assign every key in the config as an attribute in the class. This\n        # means all config sections can be accessed via Config.<key>.\n        for key, value in self.config.items():\n            setattr(self, key, value)",
        "variables": [
            "self",
            "config_validators",
            "config_path",
            "config_type",
            "key",
            "value",
            "config_overrides",
            "e"
        ],
        "docstring": "Init method.\n\nParameters\n----------\nconfig_path\n    The path of the config file to be loaded.\nconfig_overrides, optional\n    A dictionary containing a subset of the keys and values of the\n    config file that is initially loaded, by default None. If values\n    are provided that are not in the initial config then a ConfigError\n    is raised.\nconfig_type, optional\n    The file type of the config file being loaded, by default None. If\n    not specified then this is inferred from the `config_path`.\nconfig_validators, optional\n    A dictionary made up of key, value pairs where the keys refer to\n    the top level sections of the loaded config, and the values are a\n    pydantic validation class for the section, by default None. If only\n    some of the keys are specified with validators, warnings are raised\n    to alert that they have not been validated."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\io\\config.py",
        "function_name": "_get_config_parser",
        "code_chunk": "def _get_config_parser(self: \"LoadConfig\") -> Callable[[str], Config]:\n        \"\"\"Return the appropriate config parsing function.\n\n        Returns\n        -------\n        Callable[[str], Config]\n            Function that will parse a string into a config object.\n\n        Raises\n        ------\n        ConfigError\n            If the specified config file type does not have a config parser\n            implemented.\n        \"\"\"\n        readers = {\n            \"json\": parse_json,\n            \"toml\": parse_toml,\n            \"yaml\": parse_yaml,\n        }\n\n        if config := readers.get(self.config_type):\n            return config\n        else:\n            msg = f\"No config parser present for file type = {self.config_type}\"\n\n            logger.error(msg)\n            raise ConfigError(msg)",
        "variables": [
            "readers",
            "self",
            "msg",
            "config"
        ],
        "docstring": "Return the appropriate config parsing function.\n\nReturns\n-------\nCallable[[str], Config]\n    Function that will parse a string into a config object.\n\nRaises\n------\nConfigError\n    If the specified config file type does not have a config parser\n    implemented."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\io\\config.py",
        "function_name": "_apply_config_validators",
        "code_chunk": "def _apply_config_validators(self: \"LoadConfig\") -> None:\n        \"\"\"Apply validators to every key in config.\n\n        Warnings are raised if a key doesn't have a validator.\n        \"\"\"\n        for key, values in self.config.items():\n            self.config[key] = apply_validation(\n                config=values,\n                Validator=self.config_validators.get(key),\n            )",
        "variables": [
            "values",
            "self",
            "key"
        ],
        "docstring": "Apply validators to every key in config.\n\nWarnings are raised if a key doesn't have a validator."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\io\\config.py",
        "function_name": "_print_config_string",
        "code_chunk": "def _print_config_string(\n        self: \"LoadConfig\",\n        text: str,\n        data: Config,\n    ) -> str:\n        \"\"\"Build string to use for printing config dictionary.\n\n        Parameters\n        ----------\n        text\n            Any text to be printed before the config.\n        data\n            The config to be printed\n\n        Returns\n        -------\n        str\n            String with nicely formatted dictionary.\n        \"\"\"\n        return f\"\\n{text}\\n{json.dumps(data, indent=4)}\"",
        "variables": [
            "text",
            "self",
            "data"
        ],
        "docstring": "Build string to use for printing config dictionary.\n\nParameters\n----------\ntext\n    Any text to be printed before the config.\ndata\n    The config to be printed\n\nReturns\n-------\nstr\n    String with nicely formatted dictionary."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\io\\input.py",
        "function_name": "parse_json",
        "code_chunk": "def parse_json(data: str) -> Config:\n    \"\"\"Parse JSON formatted string into a dictionary.\n\n    Parameters\n    ----------\n    data\n        String containing standard JSON-formatted data.\n\n    Returns\n    -------\n    Config\n        A dictionary containing the parsed data.\n\n    Raises\n    ------\n    json.decoder.JSONDecodeError\n        If the string format of config_overrides cannot be decoded by\n        json.loads (i.e. converted to a dictionary).\n    \"\"\"\n    # Attempt to convert string to dictionary using json module. If this cannot\n    # be done, capture error and log a useful description on what needs to be\n    # changed before raising the error\n    try:\n        return json.loads(data)\n\n    except json.decoder.JSONDecodeError:\n        msg = \"\"\"\n        Cannot convert config_overrides parameter to a dictionary.\n\n        Ensure that argument input is of form:\n\n        '{\"var1\": \"value1\", \"var2\": {\"var3\": 1.1}, \"var4\": [1, 2, 3], ... }'\n\n        where single quote is used around entire entry and double quotes are\n        used for any string values within the argument.\n        \"\"\"\n        logger.error(msg)\n        raise json.decoder.JSONDecodeError(msg)",
        "variables": [
            "msg",
            "data"
        ],
        "docstring": "Parse JSON formatted string into a dictionary.\n\nParameters\n----------\ndata\n    String containing standard JSON-formatted data.\n\nReturns\n-------\nConfig\n    A dictionary containing the parsed data.\n\nRaises\n------\njson.decoder.JSONDecodeError\n    If the string format of config_overrides cannot be decoded by\n    json.loads (i.e. converted to a dictionary)."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\io\\input.py",
        "function_name": "parse_toml",
        "code_chunk": "def parse_toml(data: str) -> Config:\n    \"\"\"Parse TOML formatted string into a dictionary.\n\n    Parameters\n    ----------\n    data\n        String containing standard TOML-formatted data.\n\n    Returns\n    -------\n    Config\n        A dictionary containing the parsed data.\n    \"\"\"\n    return tomli.loads(data)",
        "variables": [
            "data"
        ],
        "docstring": "Parse TOML formatted string into a dictionary.\n\nParameters\n----------\ndata\n    String containing standard TOML-formatted data.\n\nReturns\n-------\nConfig\n    A dictionary containing the parsed data."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\io\\input.py",
        "function_name": "parse_yaml",
        "code_chunk": "def parse_yaml(data: str) -> Config:\n    \"\"\"Parse YAML formatted string into a dictionary.\n\n    Parameters\n    ----------\n    data\n        String containing standard YAML-formatted data.\n\n    Returns\n    -------\n    Config\n        A dictionary containing the parsed data.\n    \"\"\"\n    return yaml.safe_load(data)",
        "variables": [
            "data"
        ],
        "docstring": "Parse YAML formatted string into a dictionary.\n\nParameters\n----------\ndata\n    String containing standard YAML-formatted data.\n\nReturns\n-------\nConfig\n    A dictionary containing the parsed data."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\io\\input.py",
        "function_name": "read_file",
        "code_chunk": "def read_file(file: Union[CloudPath, Path]) -> str:\n    \"\"\"Load contents of specified file.\n\n    Parameters\n    ----------\n    file\n        The absolute file path of the file to be read.\n\n    Returns\n    -------\n    str\n        The contents of the provided file.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the provided file does not exist.\n    \"\"\"\n    if file.exists():\n        return file.read_text()\n    else:\n        msg = f\"{file=} cannot be found.\"\n        logger.error(msg)\n        raise FileNotFoundError(msg)",
        "variables": [
            "msg",
            "file"
        ],
        "docstring": "Load contents of specified file.\n\nParameters\n----------\nfile\n    The absolute file path of the file to be read.\n\nReturns\n-------\nstr\n    The contents of the provided file.\n\nRaises\n------\nFileNotFoundError\n    If the provided file does not exist."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\io\\output.py",
        "function_name": "zip_folder",
        "code_chunk": "def zip_folder(source_dir: str, output_filename: str, overwrite: bool = False) -> bool:\n    \"\"\"Zip the contents of the specified directory.\n\n    Parameters\n    ----------\n    source_dir\n        The directory whose contents are to be zipped.\n    output_filename\n        The output zip file name. It must end with '.zip'.\n    overwrite\n        If True, overwrite the existing zip file if it exists.\n        Default is False.\n\n    Returns\n    -------\n    bool\n        True if the directory was zipped successfully, False otherwise.\n\n    Examples\n    --------\n    >>> zip_folder('/path/to/source_dir', 'output.zip', overwrite=True)\n    True\n    \"\"\"\n    source_dir_path = Path(source_dir)\n    output_filename_path = Path(output_filename)\n\n    if not source_dir_path.exists() or not source_dir_path.is_dir():\n        logger.error(\n            f\"Source directory {source_dir} does not exist or is not a directory.\",\n        )\n        return False\n\n    if output_filename_path.suffix != \".zip\":\n        logger.error(f\"Output filename {output_filename} must have a .zip extension.\")\n        return False\n\n    if output_filename_path.exists() and not overwrite:\n        logger.error(\n            f\"Output file {output_filename} already exists and \"\n            \"overwrite is set to False.\",\n        )\n        return False\n\n    try:\n        # Create the zip file in the same directory as the output_filename\n        temp_zip_path = output_filename_path.with_suffix(\"\")\n        shutil.make_archive(str(temp_zip_path), \"zip\", root_dir=source_dir_path)\n\n        # Move the created zip file to the correct output filename\n        temp_zip_path_with_extension = temp_zip_path.with_suffix(\".zip\")\n        temp_zip_path_with_extension.rename(output_filename_path)\n\n        logger.info(f\"Directory {source_dir} zipped as {output_filename_path}.\")\n        return True\n    except Exception as e:\n        logger.error(f\"Failed to zip the directory {source_dir}: {e}\")\n        return False",
        "variables": [
            "temp_zip_path",
            "overwrite",
            "source_dir_path",
            "output_filename_path",
            "source_dir",
            "temp_zip_path_with_extension",
            "output_filename",
            "e"
        ],
        "docstring": "Zip the contents of the specified directory.\n\nParameters\n----------\nsource_dir\n    The directory whose contents are to be zipped.\noutput_filename\n    The output zip file name. It must end with '.zip'.\noverwrite\n    If True, overwrite the existing zip file if it exists.\n    Default is False.\n\nReturns\n-------\nbool\n    True if the directory was zipped successfully, False otherwise.\n\nExamples\n--------\n>>> zip_folder('/path/to/source_dir', 'output.zip', overwrite=True)\nTrue"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\methods\\averaging_methods.py",
        "function_name": "weighted_arithmetic_average",
        "code_chunk": "def weighted_arithmetic_average(val: str, weight: str) -> SparkCol:\n    \"\"\"Calculate the weighted arithmetic average.\"\"\"\n    return F.sum(F.col(val) * F.col(weight))",
        "variables": [
            "weight",
            "val"
        ],
        "docstring": "Calculate the weighted arithmetic average."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\methods\\averaging_methods.py",
        "function_name": "weighted_geometric_average",
        "code_chunk": "def weighted_geometric_average(val: str, weight: str) -> SparkCol:\n    \"\"\"Calculate the weighted geometric average.\"\"\"\n    return F.exp(F.sum(F.log(F.col(val)) * F.col(weight)))",
        "variables": [
            "weight",
            "val"
        ],
        "docstring": "Calculate the weighted geometric average."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\methods\\averaging_methods.py",
        "function_name": "unweighted_arithmetic_average",
        "code_chunk": "def unweighted_arithmetic_average(val: str) -> SparkCol:\n    \"\"\"Calculate the unweighted arithmetic average.\"\"\"\n    return F.mean(val)",
        "variables": [
            "val"
        ],
        "docstring": "Calculate the unweighted arithmetic average."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\methods\\averaging_methods.py",
        "function_name": "unweighted_geometric_average",
        "code_chunk": "def unweighted_geometric_average(val: str) -> SparkCol:\n    \"\"\"Calculate the unweighted geometric average.\"\"\"\n    return F.exp(F.mean(F.log(val)))",
        "variables": [
            "val"
        ],
        "docstring": "Calculate the unweighted geometric average."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\rdsa_utils\\methods\\averaging_methods.py",
        "function_name": "get_weight_shares",
        "code_chunk": "def get_weight_shares(\n    weights: str,\n    levels: Optional[Union[str, Sequence[str]]] = None,\n) -> SparkCol:\n    \"\"\"Divide weights by sum of weights for each group.\"\"\"\n    return F.col(weights) / F.sum(F.col(weights)).over(get_window_spec(levels))",
        "variables": [
            "levels",
            "weights"
        ],
        "docstring": "Divide weights by sum of weights for each group."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_logging.py",
        "function_name": "test_init",
        "code_chunk": "def test_init(self):\n        \"\"\"Test for log_dev.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test for log_dev."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_logging.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected behaviour.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected behaviour."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_logging.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self, logger, expected):\n        \"\"\"Test expected outputs.\"\"\"\n        actual = timer_args(name=\"load configs\", logger=logger)\n        # Need to mock as function will be timer_args vs TestTimerArgs and Running load configs vs Running {name} will fail.\n        with mock.patch.dict(\n            actual,\n            {\"text\": \"test\", \"initial_text\": \"Running load configs\"},\n        ):\n            assert actual == expected",
        "variables": [
            "logger",
            "self",
            "actual",
            "expected"
        ],
        "docstring": "Test expected outputs."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_logging.py",
        "function_name": "input_df",
        "code_chunk": "def input_df(self):\n        \"\"\"Input pandas dataframe.\"\"\"\n        return create_dataframe(\n            [\n                (\"shop\", \"code\", \"product_name\"),\n                (\"shop_1\", \"111\", \"lemonade 200ml\"),\n                (\"shop_1\", \"222\", \"royal gala 4 pack\"),\n            ],\n        )",
        "variables": [
            "self"
        ],
        "docstring": "Input pandas dataframe."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_logging.py",
        "function_name": "test_raises_error",
        "code_chunk": "def test_raises_error(self, input_df, stop_pipeline, show_records):\n        \"\"\"Tests that an error is raised.\"\"\"\n        message = \"This causes an error\"\n        with pytest.raises(ValueError):\n            print_full_table_and_raise_error(\n                input_df,\n                message,\n                stop_pipeline,\n                show_records,\n            )",
        "variables": [
            "show_records",
            "self",
            "input_df",
            "stop_pipeline",
            "message"
        ],
        "docstring": "Tests that an error is raised."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_logging.py",
        "function_name": "test_expected_with_no_errors_show_records_true",
        "code_chunk": "def test_expected_with_no_errors_show_records_true(self, caplog, input_df):\n        \"\"\"Tests that the correct logger infomation is displayed.\"\"\"\n        stop_pipeline = False\n        show_records = True\n        message = \"Info\"\n        caplog.set_level(logging.INFO)\n        print_full_table_and_raise_error(input_df, message, stop_pipeline, show_records)\n        assert input_df.to_string() in caplog.text\n        assert message in caplog.text",
        "variables": [
            "show_records",
            "self",
            "input_df",
            "caplog",
            "stop_pipeline",
            "message"
        ],
        "docstring": "Tests that the correct logger infomation is displayed."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_logging.py",
        "function_name": "test_expected_with_no_errors_all_args_false",
        "code_chunk": "def test_expected_with_no_errors_all_args_false(self, caplog, input_df):\n        \"\"\"Tests that the correct logger infomation is displayed.\"\"\"\n        stop_pipeline = False\n        show_records = False\n        message = \"Info\"\n        caplog.set_level(logging.INFO)\n        print_full_table_and_raise_error(input_df, message, stop_pipeline, show_records)\n        assert message in caplog.text",
        "variables": [
            "show_records",
            "self",
            "input_df",
            "caplog",
            "stop_pipeline",
            "message"
        ],
        "docstring": "Tests that the correct logger infomation is displayed."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_logging.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected behaviour.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected behaviour."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_logging.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected behaviour.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected behaviour."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_logging.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected behaviour.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected behaviour."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_logging.py",
        "function_name": "test_logger_with_no_handler",
        "code_chunk": "def test_logger_with_no_handler(self, caplog):\n        \"\"\"Test whether a logger is properly initialized with no handlers.\"\"\"\n        caplog.set_level(logging.DEBUG)\n        init_logger_advanced(logging.DEBUG)\n        assert caplog.records[0].levelname == \"DEBUG\"",
        "variables": [
            "self",
            "caplog"
        ],
        "docstring": "Test whether a logger is properly initialized with no handlers."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_logging.py",
        "function_name": "test_logger_with_handlers",
        "code_chunk": "def test_logger_with_handlers(self, caplog):\n        \"\"\"Test whether a logger is properly initialized with a valid handler.\"\"\"\n        caplog.set_level(logging.DEBUG)\n        handler = logging.FileHandler(\"logfile.log\")\n        handlers = [handler]\n        init_logger_advanced(logging.DEBUG, handlers)\n\n        logger = logging.getLogger(\"rdsa_utils.logging\")\n\n        assert caplog.records[0].levelname == \"DEBUG\"\n        assert any(isinstance(h, type(handler)) for h in logger.handlers)",
        "variables": [
            "h",
            "self",
            "handlers",
            "caplog",
            "logger",
            "handler"
        ],
        "docstring": "Test whether a logger is properly initialized with a valid handler."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_logging.py",
        "function_name": "test_logger_with_invalid_handler",
        "code_chunk": "def test_logger_with_invalid_handler(self):\n        \"\"\"Test whether a ValueError is raised when an invalid handler is provided.\"\"\"\n        log_level = logging.DEBUG\n        invalid_handler = \"I am not a handler\"\n        handlers = [invalid_handler]\n        with pytest.raises(ValueError) as exc_info:\n            init_logger_advanced(log_level, handlers)\n        assert (\n            str(exc_info.value)\n            == f\"Handler {invalid_handler} is not an instance of logging.Handler or its subclasses\"\n        )",
        "variables": [
            "self",
            "log_level",
            "exc_info",
            "handlers",
            "invalid_handler"
        ],
        "docstring": "Test whether a ValueError is raised when an invalid handler is provided."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_validation.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_validation.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\test_validation.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        date = \"Feb 2022\"\n\n        actual = allowed_date_format(date)\n\n        assert actual == date",
        "variables": [
            "date",
            "self",
            "actual"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "mock_subprocess_popen",
        "code_chunk": "def mock_subprocess_popen(self, monkeypatch):  # noqa: PT004\n        \"\"\"Fixture to mock the subprocess.Popen function.\n\n        This fixture replaces the subprocess.Popen function with a mock implementation.\n        The mock implementation returns a MagicMock object that simulates the behavior\n        of the Popen object. It sets the returncode to 0 and configures the communicate\n        method to return empty byte strings (b\"\") for stdout and stderr.\n\n        Parameters\n        ----------\n        monkeypatch : pytest.MonkeyPatch\n            The monkeypatch object provided by the pytest framework.\n\n        Returns\n        -------\n        None\n            This fixture does not return any value, but it patches the\n            subprocess.Popen function.\n        \"\"\"\n\n        def mock_popen(*args, **kwargs):\n            process_mock = MagicMock()\n            process_mock.returncode = 0\n            process_mock.communicate.return_value = (b\"\", b\"\")\n\n            return process_mock\n\n        monkeypatch.setattr(subprocess, \"Popen\", mock_popen)",
        "variables": [
            "monkeypatch",
            "self",
            "process_mock"
        ],
        "docstring": "Fixture to mock the subprocess.Popen function.\n\nThis fixture replaces the subprocess.Popen function with a mock implementation.\nThe mock implementation returns a MagicMock object that simulates the behavior\nof the Popen object. It sets the returncode to 0 and configures the communicate\nmethod to return empty byte strings (b\"\") for stdout and stderr.\n\nParameters\n----------\nmonkeypatch : pytest.MonkeyPatch\n    The monkeypatch object provided by the pytest framework.\n\nReturns\n-------\nNone\n    This fixture does not return any value, but it patches the\n    subprocess.Popen function."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "mock_subprocess_popen_date_modifed",
        "code_chunk": "def mock_subprocess_popen_date_modifed(self):\n        \"\"\"Fixture to mock subprocess.Popen for testing get_date_modified.\n\n        This fixture patches the subprocess.Popen function using the patch decorator\n        from the unittest.mock module.\n\n        It configures the mock implementation to return a MagicMock object for\n        mocking the Popen object.\n\n        The MagicMock object is configured to simulate the behavior of stdout.read()\n        method by returning a byte string representing the date \"2023-05-25\" when\n        called with decode method.\n\n        Yields\n        ------\n        MagicMock\n            The MagicMock object that simulates the behavior of subprocess.Popen.\n        \"\"\"\n        with patch(\"subprocess.Popen\") as mock_popen:\n            mock_stdout = MagicMock()\n            mock_stdout.read.return_value.decode.return_value = \"2023-05-25\"\n            mock_popen.return_value.communicate.return_value = (mock_stdout, None)\n            yield mock_popen",
        "variables": [
            "self",
            "mock_popen",
            "mock_stdout"
        ],
        "docstring": "Fixture to mock subprocess.Popen for testing get_date_modified.\n\nThis fixture patches the subprocess.Popen function using the patch decorator\nfrom the unittest.mock module.\n\nIt configures the mock implementation to return a MagicMock object for\nmocking the Popen object.\n\nThe MagicMock object is configured to simulate the behavior of stdout.read()\nmethod by returning a byte string representing the date \"2023-05-25\" when\ncalled with decode method.\n\nYields\n------\nMagicMock\n    The MagicMock object that simulates the behavior of subprocess.Popen."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test__perform",
        "code_chunk": "def test__perform(self, mock_subprocess_popen):\n        \"\"\"Check _perform function behavior on successful command execution.\"\"\"\n        command = [\"ls\", \"-l\", \"/home/user\"]\n        assert _perform(command) is True",
        "variables": [
            "self",
            "command",
            "mock_subprocess_popen"
        ],
        "docstring": "Check _perform function behavior on successful command execution."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_change_permissions",
        "code_chunk": "def test_change_permissions(self, mock_subprocess_popen):\n        \"\"\"Verify proper execution of 'hadoop fs -chmod' command by the change_permissions function.\n\n        Checks if the command is correctly constructed based on the provided arguments.\n        \"\"\"\n        # Test case 1: Test change_permissions without recursive option\n        path = \"/user/example\"\n        permission = \"go+rwx\"\n        command = [\"hadoop\", \"fs\", \"-chmod\", permission, path]\n        assert change_permissions(path, permission) == _perform(command)\n\n        # Test case 2: Test change_permissions with recursive option\n        recursive_path = \"/user/example\"\n        recursive_permission = \"777\"\n        recursive_command = [\n            \"hadoop\",\n            \"fs\",\n            \"-chmod\",\n            \"-R\",\n            recursive_permission,\n            recursive_path,\n        ]\n        assert change_permissions(\n            recursive_path,\n            recursive_permission,\n            recursive=True,\n        ) == _perform(recursive_command)",
        "variables": [
            "recursive_permission",
            "recursive_path",
            "self",
            "command",
            "path",
            "recursive_command",
            "permission",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper execution of 'hadoop fs -chmod' command by the change_permissions function.\n\nChecks if the command is correctly constructed based on the provided arguments."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_copy",
        "code_chunk": "def test_copy(self, mock_subprocess_popen):\n        \"\"\"Verify proper execution of 'hadoop fs -cp' command by the copy function.\n\n        Checks if the command is correctly constructed based on the provided arguments.\n        \"\"\"\n        # Test case 1: Test copy without overwrite option\n        from_path = \"/user/example/file.txt\"\n        to_path = \"/user/backup/file.txt\"\n        command = [\"hadoop\", \"fs\", \"-cp\", from_path, to_path]\n        assert copy(from_path, to_path) == _perform(command)\n\n        # Test case 2: Test copy with overwrite option\n        overwrite_from_path = \"/user/example/file.txt\"\n        overwrite_to_path = \"/user/backup/file.txt\"\n        overwrite_command = [\n            \"hadoop\",\n            \"fs\",\n            \"-cp\",\n            \"-f\",\n            overwrite_from_path,\n            overwrite_to_path,\n        ]\n        assert copy(overwrite_from_path, overwrite_to_path, overwrite=True) == _perform(\n            overwrite_command,\n        )",
        "variables": [
            "overwrite_from_path",
            "self",
            "overwrite_command",
            "command",
            "overwrite_to_path",
            "from_path",
            "to_path",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper execution of 'hadoop fs -cp' command by the copy function.\n\nChecks if the command is correctly constructed based on the provided arguments."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_copy_local_to_hdfs",
        "code_chunk": "def test_copy_local_to_hdfs(self, mock_subprocess_popen):\n        \"\"\"Verify 'hadoop fs -copyFromLocal' command execution by copy_local_to_hdfs.\n\n        Checks if the command is correctly constructed based on the provided arguments.\n        \"\"\"\n        # Test case 1: Test copy_local_to_hdfs without overwrite option\n        from_path = \"/local/path/file.txt\"\n        to_path = \"/user/hdfs/path/file.txt\"\n        command = [\"hadoop\", \"fs\", \"-copyFromLocal\", from_path, to_path]\n        assert copy_local_to_hdfs(from_path, to_path) == _perform(command)\n\n        # Test case 2: Test copy_local_to_hdfs with overwrite option\n        overwrite_from_path = \"/local/path/file.txt\"\n        overwrite_to_path = \"/user/hdfs/path/file.txt\"\n        overwrite_command = [\n            \"hadoop\",\n            \"fs\",\n            \"-copyFromLocal\",\n            \"-f\",\n            overwrite_from_path,\n            overwrite_to_path,\n        ]\n        assert copy_local_to_hdfs(overwrite_from_path, overwrite_to_path) == _perform(\n            overwrite_command,\n        )",
        "variables": [
            "overwrite_from_path",
            "self",
            "overwrite_command",
            "command",
            "overwrite_to_path",
            "from_path",
            "to_path",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify 'hadoop fs -copyFromLocal' command execution by copy_local_to_hdfs.\n\nChecks if the command is correctly constructed based on the provided arguments."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_create_dir",
        "code_chunk": "def test_create_dir(self, mock_subprocess_popen):\n        \"\"\"Verify proper execution of 'hadoop fs -mkdir' command by the create_dir function.\n\n        Checks if the command is correctly constructed based on the provided path.\n        \"\"\"\n        # Test case 1: Test create_dir with a valid path\n        path = \"/user/new_directory\"\n        command = [\"hadoop\", \"fs\", \"-mkdir\", path]\n        assert create_dir(path) == _perform(command)",
        "variables": [
            "command",
            "self",
            "path",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper execution of 'hadoop fs -mkdir' command by the create_dir function.\n\nChecks if the command is correctly constructed based on the provided path."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_create_txt_from_string",
        "code_chunk": "def test_create_txt_from_string(\n        self,\n        path,\n        string_to_write,\n        replace,\n        expected_call,\n    ):\n        \"\"\"Verify 'echo | hadoop fs -put -' command execution by create_txt_from_string.\"\"\"\n        with patch(\"subprocess.call\") as subprocess_mock, patch(\n            \"rdsa_utils.cdp.helpers.hdfs_utils.file_exists\",\n        ) as file_exists_mock, patch(\n            \"rdsa_utils.cdp.helpers.hdfs_utils.delete_file\",\n        ) as delete_file_mock:\n            file_exists_mock.return_value = (\n                replace  # Assume file exists if replace is True\n            )\n\n            if expected_call:\n                # Test if subprocess.call is called correctly\n                create_txt_from_string(path, string_to_write, replace)\n                subprocess_mock.assert_called_with(expected_call, shell=True)\n            else:\n                # Test if FileNotFoundError is raised\n                with pytest.raises(FileNotFoundError) as excinfo:\n                    create_txt_from_string(path, string_to_write, replace)\n                assert (\n                    str(excinfo.value)\n                    == f\"File {path} already exists and replace is set to False.\"\n                )\n\n            # Verify if file_exists mock is called correctly\n            file_exists_mock.assert_called_with(path)\n\n            if replace and file_exists_mock.return_value:\n                # Verify if delete_file mock is called when replace is True\n                # and file exists\n                delete_file_mock.assert_called_with(path)\n            else:\n                # Verify delete_file mock is not called in other cases\n                delete_file_mock.assert_not_called()",
        "variables": [
            "replace",
            "delete_file_mock",
            "self",
            "string_to_write",
            "file_exists_mock",
            "path",
            "excinfo",
            "subprocess_mock",
            "expected_call"
        ],
        "docstring": "Verify 'echo | hadoop fs -put -' command execution by create_txt_from_string."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_delete_dir",
        "code_chunk": "def test_delete_dir(self, mock_subprocess_popen):\n        \"\"\"Verify proper execution of 'hadoop fs -rmdir' command by the delete_dir function.\n\n        Checks if the command is correctly constructed based on the provided path.\n        \"\"\"\n        # Test case 1: Test delete_dir with a valid path\n        path = \"/user/directory\"\n        command = [\"hadoop\", \"fs\", \"-rmdir\", path]\n        assert delete_dir(path) == _perform(command)",
        "variables": [
            "command",
            "self",
            "path",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper execution of 'hadoop fs -rmdir' command by the delete_dir function.\n\nChecks if the command is correctly constructed based on the provided path."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_delete_file",
        "code_chunk": "def test_delete_file(self, mock_subprocess_popen):\n        \"\"\"Verify proper execution of 'hadoop fs -rm' command by the delete_file function.\n\n        Checks if the command is correctly constructed based on the provided path.\n        \"\"\"\n        # Test case 1: Test delete_file with a valid path\n        path = \"/user/file.txt\"\n        command = [\"hadoop\", \"fs\", \"-rm\", path]\n        assert delete_file(path) == _perform(command)",
        "variables": [
            "command",
            "self",
            "path",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper execution of 'hadoop fs -rm' command by the delete_file function.\n\nChecks if the command is correctly constructed based on the provided path."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_delete_file_with_delete_path",
        "code_chunk": "def test_delete_file_with_delete_path(self, mock_subprocess_popen):\n        \"\"\"Verify proper execution of 'hadoop fs -rm -r' command by the delete_path function for a file.\n\n        Checks if the command is correctly constructed based on the provided file path.\n        \"\"\"\n        # Test case: Test delete_path with a valid file path\n        file_path = \"/user/testfile.txt\"\n        command = [\"hadoop\", \"fs\", \"-rm\", \"-r\", file_path]\n        assert delete_path(file_path) == _perform(command)",
        "variables": [
            "file_path",
            "self",
            "command",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper execution of 'hadoop fs -rm -r' command by the delete_path function for a file.\n\nChecks if the command is correctly constructed based on the provided file path."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_delete_directory_with_delete_path",
        "code_chunk": "def test_delete_directory_with_delete_path(self, mock_subprocess_popen):\n        \"\"\"Verify proper execution of 'hadoop fs -rm -r' command by the delete_path function for a directory.\n\n        Checks if the command is correctly constructed based on the provided directory path.\n        \"\"\"\n        # Test case: Test delete_path with a valid directory path\n        dir_path = \"/user/testdir\"\n        command = [\"hadoop\", \"fs\", \"-rm\", \"-r\", dir_path]\n        assert delete_path(dir_path) == _perform(command)",
        "variables": [
            "self",
            "command",
            "dir_path",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper execution of 'hadoop fs -rm -r' command by the delete_path function for a directory.\n\nChecks if the command is correctly constructed based on the provided directory path."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_file_exists",
        "code_chunk": "def test_file_exists(self, mock_subprocess_popen):\n        \"\"\"Verify proper execution of 'hadoop fs -test -e' command by the file_exists function.\n\n        Checks if the command is correctly constructed based on the provided path.\n        \"\"\"\n        # Test case 1: Test file_exists with an existing file\n        path = \"/user/file.txt\"\n        command = [\"hadoop\", \"fs\", \"-test\", \"-e\", path]\n        assert file_exists(path) == _perform(command)\n\n        # Test case 2: Test file_exists with a non-existing file\n        non_existing_path = \"/user/non_existing_file.txt\"\n        non_existing_command = [\"hadoop\", \"fs\", \"-test\", \"-e\", non_existing_path]\n        assert file_exists(non_existing_path) == _perform(non_existing_command)",
        "variables": [
            "self",
            "non_existing_path",
            "non_existing_command",
            "command",
            "path",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper execution of 'hadoop fs -test -e' command by the file_exists function.\n\nChecks if the command is correctly constructed based on the provided path."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_get_date_modified",
        "code_chunk": "def test_get_date_modified(self, mock_subprocess_popen_date_modifed):\n        \"\"\"Verify proper execution of 'hadoop fs -stat %y' command by the get_date_modified function.\n\n        Checks if the command is correctly constructed based on the provided path.\n        \"\"\"\n        # Test case: Test get_date_modified with a valid path\n        filepath = \"/user/file.txt\"\n        command_mock = mock_subprocess_popen_date_modifed.return_value\n        stdout_mock = command_mock.stdout\n        stdout_mock.read.return_value.decode.return_value.__getitem__.return_value = (\n            \"2023-05-25\"\n        )\n        expected_output = \"2023-05-25\"\n        assert get_date_modified(filepath) == expected_output",
        "variables": [
            "mock_subprocess_popen_date_modifed",
            "self",
            "expected_output",
            "filepath",
            "stdout_mock",
            "command_mock"
        ],
        "docstring": "Verify proper execution of 'hadoop fs -stat %y' command by the get_date_modified function.\n\nChecks if the command is correctly constructed based on the provided path."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_is_dir",
        "code_chunk": "def test_is_dir(self, mock_subprocess_popen):\n        \"\"\"Verify proper execution of 'hadoop fs -test -d' command by the is_dir function.\n\n        Checks if the command is correctly constructed based on the provided path.\n        \"\"\"\n        # Test case 1: Test is_dir with an existing directory\n        path = \"/user/directory\"\n        command = [\"hadoop\", \"fs\", \"-test\", \"-d\", path]\n        assert is_dir(path) == _perform(command)\n\n        # Test case 2: Test is_dir with a non-existing directory\n        non_existing_path = \"/user/non_existing_directory\"\n        non_existing_command = [\"hadoop\", \"fs\", \"-test\", \"-d\", non_existing_path]\n        assert is_dir(non_existing_path) == _perform(non_existing_command)",
        "variables": [
            "self",
            "non_existing_path",
            "non_existing_command",
            "command",
            "path",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper execution of 'hadoop fs -test -d' command by the is_dir function.\n\nChecks if the command is correctly constructed based on the provided path."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_move_local_to_hdfs",
        "code_chunk": "def test_move_local_to_hdfs(self, mock_subprocess_popen):\n        \"\"\"Verify proper execution of 'hadoop fs -moveFromLocal' command by move_local_to_hdfs.\n\n        Checks if the command is correctly constructed based on the provided arguments.\n        \"\"\"\n        # Test case 1: Test move_local_to_hdfs without overwrite option\n        from_path = \"/local/path/file.txt\"\n        to_path = \"/user/hdfs/path/file.txt\"\n        command = [\"hadoop\", \"fs\", \"-moveFromLocal\", from_path, to_path]\n        assert move_local_to_hdfs(from_path, to_path) == _perform(command)\n\n        # Test case 2: Test move_local_to_hdfs with overwrite option\n        overwrite_from_path = \"/local/path/file.txt\"\n        overwrite_to_path = \"/user/hdfs/path/file.txt\"\n        overwrite_command = [\n            \"hadoop\",\n            \"fs\",\n            \"-moveFromLocal\",\n            overwrite_from_path,\n            overwrite_to_path,\n        ]\n        assert move_local_to_hdfs(overwrite_from_path, overwrite_to_path) == _perform(\n            overwrite_command,\n        )",
        "variables": [
            "overwrite_from_path",
            "self",
            "overwrite_command",
            "command",
            "overwrite_to_path",
            "from_path",
            "to_path",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper execution of 'hadoop fs -moveFromLocal' command by move_local_to_hdfs.\n\nChecks if the command is correctly constructed based on the provided arguments."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_read_dir",
        "code_chunk": "def test_read_dir(self, mock_subprocess_popen):\n        \"\"\"Verify proper execution of 'hadoop fs -ls' command by the read_dir function.\n\n        Checks if the command is correctly constructed based on the provided path.\n        \"\"\"\n        # Test case 1: Test read_dir with a valid path\n        path = \"/user/directory\"\n        ls = subprocess.Popen([\"hadoop\", \"fs\", \"-ls\", path], stdout=subprocess.PIPE)\n        expected_files = [\n            line.decode(\"utf-8\").split()[-1]\n            for line in ls.stdout\n            if \"Found\" not in line.decode(\"utf-8\")\n        ]\n        assert read_dir(path) == expected_files",
        "variables": [
            "self",
            "ls",
            "expected_files",
            "path",
            "line",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper execution of 'hadoop fs -ls' command by the read_dir function.\n\nChecks if the command is correctly constructed based on the provided path."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_read_dir_files",
        "code_chunk": "def test_read_dir_files(self, mock_subprocess_popen):\n        \"\"\"Verify proper extraction of filenames from paths by the read_dir_files function.\"\"\"\n        # Test case 1: Test read_dir_files with a valid path\n        path = \"/user/directory\"\n        expected_files = [Path(p).name for p in read_dir(path)]\n        assert read_dir_files(path) == expected_files",
        "variables": [
            "self",
            "expected_files",
            "path",
            "p",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper extraction of filenames from paths by the read_dir_files function."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_read_dir_files_recursive",
        "code_chunk": "def test_read_dir_files_recursive(self, mock_subprocess_popen):\n        \"\"\"Verify proper execution of 'hadoop fs -ls -R' command by read_dir_files_recursive.\n\n        Checks if the command is correctly constructed based on the provided path.\n        \"\"\"\n        # Test case 1: Test read_dir_files_recursive without return_path option\n        path = \"/user/directory\"\n        command = subprocess.Popen(\n            f\"hadoop fs -ls -R {path} | grep -v ^d | tr -s ' ' | cut -d ' ' -f 8-\",\n            stdout=subprocess.PIPE,\n            shell=True,\n        )\n        expected_files = [\n            obj.decode(\"utf-8\") for obj in command.stdout.read().splitlines()\n        ]\n        assert read_dir_files_recursive(path) == expected_files\n\n        # Test case 2: Test read_dir_files_recursive with return_path option\n        return_path = True\n        return_path_files = [\n            obj.decode(\"utf-8\") for obj in command.stdout.read().splitlines()\n        ]\n        assert read_dir_files_recursive(path, return_path) == return_path_files",
        "variables": [
            "return_path_files",
            "self",
            "return_path",
            "expected_files",
            "obj",
            "command",
            "path",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper execution of 'hadoop fs -ls -R' command by read_dir_files_recursive.\n\nChecks if the command is correctly constructed based on the provided path."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "test_rename",
        "code_chunk": "def test_rename(self, mock_subprocess_popen):\n        \"\"\"Verify proper execution of the 'hadoop fs -mv' command by the rename function.\n\n        Checks if the command is correctly constructed based on the provided arguments.\n        \"\"\"\n        # Test case 1: Test rename without overwrite option\n        from_path = \"/user/old_file.txt\"\n        to_path = \"/user/new_file.txt\"\n        command = [\"hadoop\", \"fs\", \"-mv\", from_path, to_path]\n        assert rename(from_path, to_path) == _perform(command)\n\n        # Test case 2: Test rename with overwrite option\n        overwrite_from_path = \"/user/old_file.txt\"\n        overwrite_to_path = \"/user/new_file.txt\"\n        overwrite_command = [\n            \"hadoop\",\n            \"fs\",\n            \"-mv\",\n            overwrite_from_path,\n            overwrite_to_path,\n        ]\n        assert rename(\n            overwrite_from_path,\n            overwrite_to_path,\n            overwrite=True,\n        ) == _perform(overwrite_command)",
        "variables": [
            "overwrite_from_path",
            "self",
            "overwrite_command",
            "command",
            "overwrite_to_path",
            "from_path",
            "to_path",
            "mock_subprocess_popen"
        ],
        "docstring": "Verify proper execution of the 'hadoop fs -mv' command by the rename function.\n\nChecks if the command is correctly constructed based on the provided arguments."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_hdfs_utils.py",
        "function_name": "mock_popen",
        "code_chunk": "def mock_popen(*args, **kwargs):\n            process_mock = MagicMock()\n            process_mock.returncode = 0\n            process_mock.communicate.return_value = (b\"\", b\"\")\n\n            return process_mock",
        "variables": [
            "process_mock",
            "args",
            "kwargs"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_impala.py",
        "function_name": "test_invalidate_impala_metadata",
        "code_chunk": "def test_invalidate_impala_metadata(self, mocker):\n        \"\"\"Test the invalidate_impala_metadata function.\n\n        Parameters\n        ----------\n        mocker : pytest_mock.MockFixture\n            Pytest's MockFixture object to mock subprocess.run().\n\n        Notes\n        -----\n        This test verifies the following:\n        1. The correct impala-shell command is executed with\n           the correct arguments.\n        2. The function does not raise any exceptions.\n        3. The function correctly handles and logs the stderr output\n           when keep_stderr is True.\n        \"\"\"\n        # Mock the subprocess.run() call\n        mock_subprocess_run = mocker.patch(\"subprocess.run\")\n\n        # Set up test parameters\n        table = \"test_table\"\n        impalad_address_port = \"localhost:21050\"\n        impalad_ca_cert = \"/path/to/ca_cert.pem\"\n\n        # Mock logger.info\n        mock_logger_info = mocker.patch(\"logging.Logger.info\")\n\n        # Call the function without keep_stderr\n        invalidate_impala_metadata(table, impalad_address_port, impalad_ca_cert)\n\n        # Check that subprocess.run() was called with the correct arguments\n        mock_subprocess_run.assert_called_with(\n            [\n                \"impala-shell\",\n                \"-k\",\n                \"--ssl\",\n                \"-i\",\n                impalad_address_port,\n                \"--ca_cert\",\n                impalad_ca_cert,\n                \"-q\",\n                f\"invalidate metadata {table};\",\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n\n        # Reset the mock\n        mock_subprocess_run.reset_mock()\n\n        # Call the function with keep_stderr\n        result = subprocess.CompletedProcess(\n            args=[\"dummy\"],\n            returncode=0,\n            stdout=b\"\",\n            stderr=b\"Test Error\",\n        )\n        mock_subprocess_run.return_value = result\n\n        invalidate_impala_metadata(\n            table,\n            impalad_address_port,\n            impalad_ca_cert,\n            keep_stderr=True,\n        )\n\n        # Check that subprocess.run() was called with the correct arguments\n        # and logger.info() was called with the expected error message.\n        mock_subprocess_run.assert_called_with(\n            [\n                \"impala-shell\",\n                \"-k\",\n                \"--ssl\",\n                \"-i\",\n                impalad_address_port,\n                \"--ca_cert\",\n                impalad_ca_cert,\n                \"-q\",\n                f\"invalidate metadata {table};\",\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        mock_logger_info.assert_called_once_with(\"Test Error\")",
        "variables": [
            "self",
            "mocker",
            "result",
            "table",
            "mock_subprocess_run",
            "mock_logger_info",
            "impalad_ca_cert",
            "impalad_address_port"
        ],
        "docstring": "Test the invalidate_impala_metadata function.\n\nParameters\n----------\nmocker : pytest_mock.MockFixture\n    Pytest's MockFixture object to mock subprocess.run().\n\nNotes\n-----\nThis test verifies the following:\n1. The correct impala-shell command is executed with\n   the correct arguments.\n2. The function does not raise any exceptions.\n3. The function correctly handles and logs the stderr output\n   when keep_stderr is True."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "_aws_credentials",
        "code_chunk": "def _aws_credentials():\n    \"\"\"Mock AWS Credentials for moto.\"\"\"\n    boto3.setup_default_session(\n        aws_access_key_id=\"testing\",\n        aws_secret_access_key=\"testing\",\n        aws_session_token=\"testing\",\n    )",
        "variables": [],
        "docstring": "Mock AWS Credentials for moto."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "s3_client",
        "code_chunk": "def s3_client(_aws_credentials):\n    \"\"\"Provide a mocked AWS S3 client for testing\n    using moto with temporary credentials.\n    \"\"\"\n    with mock_aws():\n        client = boto3.client(\"s3\", region_name=\"us-east-1\")\n        client.create_bucket(Bucket=\"test-bucket\")\n        yield client",
        "variables": [
            "client",
            "_aws_credentials"
        ],
        "docstring": "Provide a mocked AWS S3 client for testing\nusing moto with temporary credentials."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "setup_files",
        "code_chunk": "def setup_files(tmp_path):\n    \"\"\"\n    Set up local files for upload and download tests.\n\n    Creates a temporary file with content 'Hello, world!'\n    for testing upload and download functionality.\n\n    Returns the path of the created local file, which is\n    provided to the upload_file and download_file functions.\n    \"\"\"\n    local_file = tmp_path / \"test_file.txt\"\n    local_file.write_text(\"Hello, world!\")\n    return local_file",
        "variables": [
            "local_file",
            "tmp_path"
        ],
        "docstring": "Set up local files for upload and download tests.\n\nCreates a temporary file with content 'Hello, world!'\nfor testing upload and download functionality.\n\nReturns the path of the created local file, which is\nprovided to the upload_file and download_file functions."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "setup_folder",
        "code_chunk": "def setup_folder(tmp_path):\n    \"\"\"\n    Set up local folder and files for upload tests.\n\n    Creates a temporary folder with files and subfolders\n    for testing folder upload functionality.\n\n    Returns the path of the created local folder, which\n    is provided to the create_folder_on_s3 and upload_folder function.\n    \"\"\"\n    folder_path = tmp_path / \"test_folder\"\n    folder_path.mkdir()\n    (folder_path / \"file1.txt\").write_text(\"Content of file 1\")\n    (folder_path / \"file2.txt\").write_text(\"Content of file 2\")\n    subfolder = folder_path / \"subfolder\"\n    subfolder.mkdir()\n    (subfolder / \"file3.txt\").write_text(\"Content of subfolder file\")\n    return folder_path",
        "variables": [
            "folder_path",
            "subfolder",
            "tmp_path"
        ],
        "docstring": "Set up local folder and files for upload tests.\n\nCreates a temporary folder with files and subfolders\nfor testing folder upload functionality.\n\nReturns the path of the created local folder, which\nis provided to the create_folder_on_s3 and upload_folder function."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "s3_client_for_list_files",
        "code_chunk": "def s3_client_for_list_files(_aws_credentials):\n    \"\"\"\n    Provide a mocked AWS S3 client with temporary\n    credentials for testing list_files function.\n\n    Creates a temporary S3 bucket and sets up some\n    objects within it for testing.\n\n    Yields the S3 client for use in the test functions.\n    \"\"\"\n    with mock_aws():\n        client = boto3.client(\"s3\", region_name=\"us-east-1\")\n        client.create_bucket(Bucket=\"test-bucket\")\n        # Set up some objects in S3 for testing\n        objects = [\n            \"file1.txt\",\n            \"folder/file2.txt\",\n            \"folder/file3.txt\",\n            \"another_folder/file4.txt\",\n            \"file1.txt.bak\",  # To test filter precision\n        ]\n        for obj in objects:\n            client.put_object(\n                Bucket=\"test-bucket\",\n                Key=obj,\n                Body=b\"Test content\",\n            )\n        yield client",
        "variables": [
            "client",
            "_aws_credentials",
            "objects",
            "obj"
        ],
        "docstring": "Provide a mocked AWS S3 client with temporary\ncredentials for testing list_files function.\n\nCreates a temporary S3 bucket and sets up some\nobjects within it for testing.\n\nYields the S3 client for use in the test functions."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "s3_client_for_delete_and_copy",
        "code_chunk": "def s3_client_for_delete_and_copy(_aws_credentials):\n    \"\"\"\n    Provide a mocked AWS S3 client with temporary\n    credentials for testing delete_file and copy_file functions.\n\n    Creates two temporary S3 buckets\n    ('source-bucket' and 'destination-bucket')\n    and sets up some objects within them for testing.\n\n    Yields the S3 client for use in the test functions.\n    \"\"\"\n    with mock_aws():\n        client = boto3.client(\"s3\", region_name=\"us-east-1\")\n        client.create_bucket(Bucket=\"source-bucket\")\n        client.create_bucket(Bucket=\"destination-bucket\")\n        # Set up some objects in S3 for testing\n        objects = [\n            (\"source-bucket\", \"source_file.txt\"),\n            (\"destination-bucket\", \"dest_file.txt\"),\n        ]\n        for bucket, obj in objects:\n            client.put_object(Bucket=bucket, Key=obj, Body=b\"Test content\")\n        yield client",
        "variables": [
            "_aws_credentials",
            "bucket",
            "obj",
            "client",
            "objects"
        ],
        "docstring": "Provide a mocked AWS S3 client with temporary\ncredentials for testing delete_file and copy_file functions.\n\nCreates two temporary S3 buckets\n('source-bucket' and 'destination-bucket')\nand sets up some objects within them for testing.\n\nYields the S3 client for use in the test functions."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_remove_leading_slash_no_slash",
        "code_chunk": "def test_remove_leading_slash_no_slash(self):\n        \"\"\"Test remove_leading_slash with a string without leading slash.\"\"\"\n        assert remove_leading_slash(\"example/path\") == \"example/path\"",
        "variables": [
            "self"
        ],
        "docstring": "Test remove_leading_slash with a string without leading slash."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_remove_leading_slash_with_slash",
        "code_chunk": "def test_remove_leading_slash_with_slash(self):\n        \"\"\"Test remove_leading_slash with a string with leading slash.\"\"\"\n        assert remove_leading_slash(\"/example/path\") == \"example/path\"",
        "variables": [
            "self"
        ],
        "docstring": "Test remove_leading_slash with a string with leading slash."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_validate_bucket_name_valid",
        "code_chunk": "def test_validate_bucket_name_valid(self):\n        \"\"\"Test validate_bucket_name with a valid bucket name.\"\"\"\n        assert validate_bucket_name(\"valid-bucket-name\") == \"valid-bucket-name\"",
        "variables": [
            "self"
        ],
        "docstring": "Test validate_bucket_name with a valid bucket name."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_validate_bucket_name_invalid_underscore",
        "code_chunk": "def test_validate_bucket_name_invalid_underscore(self):\n        \"\"\"Test validate_bucket_name with an invalid\n        bucket name containing underscore.\n        \"\"\"\n        with pytest.raises(\n            InvalidBucketNameError,\n            match=\"Bucket name must not contain underscores.\",\n        ):\n            validate_bucket_name(\"invalid_bucket_name\")",
        "variables": [
            "self"
        ],
        "docstring": "Test validate_bucket_name with an invalid\nbucket name containing underscore."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_validate_bucket_name_too_short",
        "code_chunk": "def test_validate_bucket_name_too_short(self):\n        \"\"\"Test validate_bucket_name with a bucket name that is too short.\"\"\"\n        with pytest.raises(\n            InvalidBucketNameError,\n            match=\"Bucket name must be between 3 and 63 characters long.\",\n        ):\n            validate_bucket_name(\"ab\")",
        "variables": [
            "self"
        ],
        "docstring": "Test validate_bucket_name with a bucket name that is too short."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_validate_bucket_name_too_long",
        "code_chunk": "def test_validate_bucket_name_too_long(self):\n        \"\"\"Test validate_bucket_name with a bucket name that is too long.\"\"\"\n        with pytest.raises(\n            InvalidBucketNameError,\n            match=\"Bucket name must be between 3 and 63 characters long.\",\n        ):\n            validate_bucket_name(\"a\" * 64)",
        "variables": [
            "self"
        ],
        "docstring": "Test validate_bucket_name with a bucket name that is too long."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_validate_bucket_name_contains_uppercase",
        "code_chunk": "def test_validate_bucket_name_contains_uppercase(self):\n        \"\"\"Test validate_bucket_name with a bucket name\n        containing uppercase letters.\n        \"\"\"\n        with pytest.raises(\n            InvalidBucketNameError,\n            match=\"Bucket name must not contain uppercase letters.\",\n        ):\n            validate_bucket_name(\"InvalidBucketName\")",
        "variables": [
            "self"
        ],
        "docstring": "Test validate_bucket_name with a bucket name\ncontaining uppercase letters."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_validate_bucket_name_starts_with_non_alnum",
        "code_chunk": "def test_validate_bucket_name_starts_with_non_alnum(self):\n        \"\"\"Test validate_bucket_name with a bucket name starting\n        with non-alphanumeric character.\n        \"\"\"\n        with pytest.raises(\n            InvalidBucketNameError,\n            match=\"Bucket name must start and end with a lowercase letter or number.\",\n        ):\n            validate_bucket_name(\"-invalidname\")",
        "variables": [
            "self"
        ],
        "docstring": "Test validate_bucket_name with a bucket name starting\nwith non-alphanumeric character."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_validate_bucket_name_ends_with_non_alnum",
        "code_chunk": "def test_validate_bucket_name_ends_with_non_alnum(self):\n        \"\"\"Test validate_bucket_name with a bucket name ending\n        with non-alphanumeric character.\n        \"\"\"\n        with pytest.raises(\n            InvalidBucketNameError,\n            match=\"Bucket name must start and end with a lowercase letter or number.\",\n        ):\n            validate_bucket_name(\"invalidname-\")",
        "variables": [
            "self"
        ],
        "docstring": "Test validate_bucket_name with a bucket name ending\nwith non-alphanumeric character."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_validate_bucket_name_contains_slash",
        "code_chunk": "def test_validate_bucket_name_contains_slash(self):\n        \"\"\"Test validate_bucket_name with a bucket name\n        containing forward slash.\n        \"\"\"\n        with pytest.raises(\n            InvalidBucketNameError,\n            match=\"Bucket name must not contain forward slashes.\",\n        ):\n            validate_bucket_name(\"invalid/name\")",
        "variables": [
            "self"
        ],
        "docstring": "Test validate_bucket_name with a bucket name\ncontaining forward slash."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_valid_non_s3_path",
        "code_chunk": "def test_valid_non_s3_path(self):\n        \"\"\"Test that non-S3 paths are valid when S3 schemes are not allowed.\"\"\"\n        assert (\n            validate_s3_file_path(\"data_folder/data.csv\", allow_s3_scheme=False)\n            == \"data_folder/data.csv\"\n        )",
        "variables": [
            "self"
        ],
        "docstring": "Test that non-S3 paths are valid when S3 schemes are not allowed."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_invalid_s3_path_when_not_allowed",
        "code_chunk": "def test_invalid_s3_path_when_not_allowed(self):\n        \"\"\"Test that S3 paths raise an error when S3 schemes are not allowed.\"\"\"\n        with pytest.raises(\n            InvalidS3FilePathError,\n            match=\"should not contain an S3 URI scheme\",\n        ):\n            validate_s3_file_path(\"s3a://bucket-name/data.csv\", allow_s3_scheme=False)",
        "variables": [
            "self"
        ],
        "docstring": "Test that S3 paths raise an error when S3 schemes are not allowed."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_valid_s3_path_when_allowed",
        "code_chunk": "def test_valid_s3_path_when_allowed(self):\n        \"\"\"Test that S3 paths are valid when S3 schemes are allowed.\"\"\"\n        assert (\n            validate_s3_file_path(\"s3a://bucket-name/data.csv\", allow_s3_scheme=True)\n            == \"s3a://bucket-name/data.csv\"\n        )",
        "variables": [
            "self"
        ],
        "docstring": "Test that S3 paths are valid when S3 schemes are allowed."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_invalid_non_s3_path_when_s3_required",
        "code_chunk": "def test_invalid_non_s3_path_when_s3_required(self):\n        \"\"\"Test that non-S3 paths raise an error when S3 schemes are required.\"\"\"\n        with pytest.raises(\n            InvalidS3FilePathError,\n            match=\"must contain an S3 URI scheme\",\n        ):\n            validate_s3_file_path(\"data_folder/data.csv\", allow_s3_scheme=True)",
        "variables": [
            "self"
        ],
        "docstring": "Test that non-S3 paths raise an error when S3 schemes are required."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_valid_s3_path_with_s3_scheme",
        "code_chunk": "def test_valid_s3_path_with_s3_scheme(self):\n        \"\"\"Test that paths with 's3://' scheme are valid when S3 schemes are allowed.\"\"\"\n        assert (\n            validate_s3_file_path(\"s3://bucket-name/data.csv\", allow_s3_scheme=True)\n            == \"s3://bucket-name/data.csv\"\n        )",
        "variables": [
            "self"
        ],
        "docstring": "Test that paths with 's3://' scheme are valid when S3 schemes are allowed."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_valid_path_without_bucket_name",
        "code_chunk": "def test_valid_path_without_bucket_name(self):\n        \"\"\"Test that paths without bucket names are valid when no S3 scheme is present.\"\"\"\n        assert (\n            validate_s3_file_path(\"my_folder/data.csv\", allow_s3_scheme=False)\n            == \"my_folder/data.csv\"\n        )",
        "variables": [
            "self"
        ],
        "docstring": "Test that paths without bucket names are valid when no S3 scheme is present."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_invalid_empty_path",
        "code_chunk": "def test_invalid_empty_path(self):\n        \"\"\"Test that an empty path raises an error.\"\"\"\n        with pytest.raises(\n            InvalidS3FilePathError,\n            match=\"The file path cannot be empty.\",\n        ):\n            validate_s3_file_path(\"\", allow_s3_scheme=False)",
        "variables": [
            "self"
        ],
        "docstring": "Test that an empty path raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_valid_s3_path_with_longer_structure",
        "code_chunk": "def test_valid_s3_path_with_longer_structure(self):\n        \"\"\"Test that a longer S3 path structure is valid when S3 scheme is allowed.\"\"\"\n        assert (\n            validate_s3_file_path(\n                \"s3a://bucket-name/folder/subfolder/data.csv\",\n                allow_s3_scheme=True,\n            )\n            == \"s3a://bucket-name/folder/subfolder/data.csv\"\n        )",
        "variables": [
            "self"
        ],
        "docstring": "Test that a longer S3 path structure is valid when S3 scheme is allowed."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_valid_path_without_s3_scheme_with_dots_in_name",
        "code_chunk": "def test_valid_path_without_s3_scheme_with_dots_in_name(self):\n        \"\"\"Test that a path containing dots but without S3 scheme is valid.\"\"\"\n        assert (\n            validate_s3_file_path(\"my.bucket/folder/data.csv\", allow_s3_scheme=False)\n            == \"my.bucket/folder/data.csv\"\n        )",
        "variables": [
            "self"
        ],
        "docstring": "Test that a path containing dots but without S3 scheme is valid."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_invalid_non_s3_path_with_invalid_characters",
        "code_chunk": "def test_invalid_non_s3_path_with_invalid_characters(self):\n        \"\"\"Test that non-S3 paths with invalid characters still pass if no scheme is present.\"\"\"\n        assert (\n            validate_s3_file_path(\"invalid!@#$%^&*/data.csv\", allow_s3_scheme=False)\n            == \"invalid!@#$%^&*/data.csv\"\n        )",
        "variables": [
            "self"
        ],
        "docstring": "Test that non-S3 paths with invalid characters still pass if no scheme is present."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_file_exists_true",
        "code_chunk": "def test_file_exists_true(self, s3_client):\n        \"\"\"Test file_exists returns True when the file is in the bucket.\"\"\"\n        s3_client.put_object(\n            Bucket=\"test-bucket\",\n            Key=\"test-file.txt\",\n            Body=b\"content\",\n        )\n        assert file_exists(s3_client, \"test-bucket\", \"test-file.txt\") is True",
        "variables": [
            "self",
            "s3_client"
        ],
        "docstring": "Test file_exists returns True when the file is in the bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_file_exists_false",
        "code_chunk": "def test_file_exists_false(self, s3_client):\n        \"\"\"Test file_exists returns False when the file is not in the bucket.\"\"\"\n        assert file_exists(s3_client, \"test-bucket\", \"nonexistent.txt\") is False",
        "variables": [
            "self",
            "s3_client"
        ],
        "docstring": "Test file_exists returns False when the file is not in the bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_upload_success",
        "code_chunk": "def test_upload_success(self, s3_client, setup_files):\n        \"\"\"Test file is uploaded successfully.\"\"\"\n        assert (\n            upload_file(\n                s3_client,\n                \"test-bucket\",\n                str(setup_files),\n                \"uploaded.txt\",\n            )\n            is True\n        )",
        "variables": [
            "self",
            "setup_files",
            "s3_client"
        ],
        "docstring": "Test file is uploaded successfully."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_upload_failure_file_not_found",
        "code_chunk": "def test_upload_failure_file_not_found(self, s3_client, setup_files):\n        \"\"\"Test upload fails if the local file does not exist.\"\"\"\n        assert (\n            upload_file(\n                s3_client,\n                \"test-bucket\",\n                str(setup_files.parent / \"nonexistent.txt\"),\n            )\n            is False\n        )",
        "variables": [
            "self",
            "setup_files",
            "s3_client"
        ],
        "docstring": "Test upload fails if the local file does not exist."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_upload_no_overwrite",
        "code_chunk": "def test_upload_no_overwrite(self, s3_client, setup_files):\n        \"\"\"Test no overwrite existing file without permission.\"\"\"\n        upload_file(s3_client, \"test-bucket\", str(setup_files), \"uploaded.txt\")\n        assert (\n            upload_file(\n                s3_client,\n                \"test-bucket\",\n                str(setup_files),\n                \"uploaded.txt\",\n            )\n            is False\n        )",
        "variables": [
            "self",
            "setup_files",
            "s3_client"
        ],
        "docstring": "Test no overwrite existing file without permission."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_download_success",
        "code_chunk": "def test_download_success(self, s3_client, setup_files):\n        \"\"\"Test file is downloaded successfully.\"\"\"\n        s3_client.upload_file(\n            str(setup_files),\n            \"test-bucket\",\n            \"file_to_download.txt\",\n        )\n        download_path = setup_files.parent / \"downloaded.txt\"\n        assert (\n            download_file(\n                s3_client,\n                \"test-bucket\",\n                \"file_to_download.txt\",\n                str(download_path),\n            )\n            is True\n        )",
        "variables": [
            "self",
            "download_path",
            "setup_files",
            "s3_client"
        ],
        "docstring": "Test file is downloaded successfully."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_download_file_not_found",
        "code_chunk": "def test_download_file_not_found(self, s3_client, setup_files):\n        \"\"\"Test download fails if the S3 file does not exist.\"\"\"\n        download_path = setup_files.parent / \"downloaded.txt\"\n        assert (\n            download_file(\n                s3_client,\n                \"test-bucket\",\n                \"nonexistent.txt\",\n                str(download_path),\n            )\n            is False\n        )",
        "variables": [
            "self",
            "download_path",
            "setup_files",
            "s3_client"
        ],
        "docstring": "Test download fails if the S3 file does not exist."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_download_no_overwrite_local_file",
        "code_chunk": "def test_download_no_overwrite_local_file(self, s3_client, setup_files):\n        \"\"\"Test no overwrite existing local file without permission.\"\"\"\n        download_path = setup_files  # Use the same path as the setup file\n        s3_client.upload_file(\n            str(setup_files),\n            \"test-bucket\",\n            \"file_to_download.txt\",\n        )\n        # First download\n        download_file(\n            s3_client,\n            \"test-bucket\",\n            \"file_to_download.txt\",\n            str(download_path),\n        )\n        # Attempt to download again without overwrite permission\n        assert (\n            download_file(\n                s3_client,\n                \"test-bucket\",\n                \"file_to_download.txt\",\n                str(download_path),\n            )\n            is False\n        )",
        "variables": [
            "self",
            "download_path",
            "setup_files",
            "s3_client"
        ],
        "docstring": "Test no overwrite existing local file without permission."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_create_new_folder",
        "code_chunk": "def test_create_new_folder(self, s3_client):\n        \"\"\"Test creation of a new folder on S3.\"\"\"\n        assert create_folder_on_s3(s3_client, \"test-bucket\", \"new_folder/\") is True",
        "variables": [
            "self",
            "s3_client"
        ],
        "docstring": "Test creation of a new folder on S3."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_folder_already_exists",
        "code_chunk": "def test_folder_already_exists(self, s3_client):\n        \"\"\"Test handling when the folder already exists on S3.\"\"\"\n        s3_client.put_object(Bucket=\"test-bucket\", Key=\"existing_folder/\")\n        assert create_folder_on_s3(s3_client, \"test-bucket\", \"existing_folder/\") is True",
        "variables": [
            "self",
            "s3_client"
        ],
        "docstring": "Test handling when the folder already exists on S3."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_upload_folder_success",
        "code_chunk": "def test_upload_folder_success(self, s3_client, setup_folder):\n        \"\"\"Test successful upload of a folder to S3.\"\"\"\n        assert (\n            upload_folder(\n                s3_client,\n                \"test-bucket\",\n                str(setup_folder),\n                \"test_prefix\",\n                overwrite=True,\n            )\n            is True\n        )",
        "variables": [
            "self",
            "setup_folder",
            "s3_client"
        ],
        "docstring": "Test successful upload of a folder to S3."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_upload_folder_nonexistent_local",
        "code_chunk": "def test_upload_folder_nonexistent_local(self, s3_client):\n        \"\"\"Test handling when the local folder does not exist.\"\"\"\n        assert (\n            upload_folder(\n                s3_client,\n                \"test-bucket\",\n                \"nonexistent_folder\",\n                \"test_prefix\",\n            )\n            is False\n        )",
        "variables": [
            "self",
            "s3_client"
        ],
        "docstring": "Test handling when the local folder does not exist."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_upload_folder_no_overwrite_existing_files",
        "code_chunk": "def test_upload_folder_no_overwrite_existing_files(\n        self,\n        s3_client,\n        setup_folder,\n    ):\n        \"\"\"Test not overwriting existing files without permission.\"\"\"\n        s3_client.put_object(\n            Bucket=\"test-bucket\",\n            Key=\"test_prefix/file1.txt\",\n            Body=b\"Old content\",\n        )\n        assert (\n            upload_folder(\n                s3_client,\n                \"test-bucket\",\n                str(setup_folder),\n                \"test_prefix\",\n            )\n            is False\n        )",
        "variables": [
            "self",
            "setup_folder",
            "s3_client"
        ],
        "docstring": "Test not overwriting existing files without permission."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_list_all_files",
        "code_chunk": "def test_list_all_files(self, s3_client_for_list_files):\n        \"\"\"Test listing all files in the bucket.\"\"\"\n        files = list_files(s3_client_for_list_files, \"test-bucket\")\n        assert len(files) == 5",
        "variables": [
            "files",
            "self",
            "s3_client_for_list_files"
        ],
        "docstring": "Test listing all files in the bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_list_files_with_prefix",
        "code_chunk": "def test_list_files_with_prefix(self, s3_client_for_list_files):\n        \"\"\"Test listing files filtered by a specific prefix.\"\"\"\n        files = list_files(s3_client_for_list_files, \"test-bucket\", \"folder/\")\n        assert len(files) == 2\n        assert \"folder/file2.txt\" in files\n        assert \"folder/file3.txt\" in files",
        "variables": [
            "files",
            "self",
            "s3_client_for_list_files"
        ],
        "docstring": "Test listing files filtered by a specific prefix."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_list_files_empty_prefix",
        "code_chunk": "def test_list_files_empty_prefix(self, s3_client_for_list_files):\n        \"\"\"Test listing files with an empty prefix returns all files.\"\"\"\n        files = list_files(s3_client_for_list_files, \"test-bucket\", \"\")\n        assert len(files) == 5",
        "variables": [
            "files",
            "self",
            "s3_client_for_list_files"
        ],
        "docstring": "Test listing files with an empty prefix returns all files."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_list_files_no_match",
        "code_chunk": "def test_list_files_no_match(self, s3_client_for_list_files):\n        \"\"\"Test listing files with a prefix that matches no files.\"\"\"\n        files = list_files(\n            s3_client_for_list_files,\n            \"test-bucket\",\n            \"nonexistent_prefix/\",\n        )\n        assert len(files) == 0",
        "variables": [
            "files",
            "self",
            "s3_client_for_list_files"
        ],
        "docstring": "Test listing files with a prefix that matches no files."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_list_files_pagination",
        "code_chunk": "def test_list_files_pagination(self, s3_client_for_list_files):\n        \"\"\"Test listing >1000 files to verify pagination works correctly.\"\"\"\n        for i in range(1001):\n            s3_client_for_list_files.put_object(\n                Bucket=\"test-bucket\",\n                Key=f\"paginated/file_{i:04d}.txt\",\n                Body=b\"Test content\",\n            )\n\n        files = list_files(s3_client_for_list_files, \"test-bucket\")\n        assert len(files) == 1006\n        assert \"paginated/file_0000.txt\" in files\n        assert \"paginated/file_1000.txt\" in files",
        "variables": [
            "i",
            "self",
            "s3_client_for_list_files",
            "files"
        ],
        "docstring": "Test listing >1000 files to verify pagination works correctly."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_delete_file_success",
        "code_chunk": "def test_delete_file_success(self, s3_client_for_delete_and_copy):\n        \"\"\"Test successful file deletion.\"\"\"\n        assert (\n            delete_file(\n                s3_client_for_delete_and_copy,\n                \"source-bucket\",\n                \"source_file.txt\",\n            )\n            is True\n        )",
        "variables": [
            "self",
            "s3_client_for_delete_and_copy"
        ],
        "docstring": "Test successful file deletion."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_delete_file_nonexistent",
        "code_chunk": "def test_delete_file_nonexistent(self, s3_client_for_delete_and_copy):\n        \"\"\"Test failure when trying to delete a nonexistent\n        file without overwrite enabled.\n        \"\"\"\n        assert (\n            delete_file(\n                s3_client_for_delete_and_copy,\n                \"source-bucket\",\n                \"nonexistent.txt\",\n            )\n            is False\n        )",
        "variables": [
            "self",
            "s3_client_for_delete_and_copy"
        ],
        "docstring": "Test failure when trying to delete a nonexistent\nfile without overwrite enabled."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_delete_file_nonexistent_with_overwrite",
        "code_chunk": "def test_delete_file_nonexistent_with_overwrite(\n        self,\n        s3_client_for_delete_and_copy,\n    ):\n        \"\"\"Test handling when overwrite is True for a nonexistent file.\"\"\"\n        assert (\n            delete_file(\n                s3_client_for_delete_and_copy,\n                \"source-bucket\",\n                \"nonexistent.txt\",\n                overwrite=True,\n            )\n            is True\n        )",
        "variables": [
            "self",
            "s3_client_for_delete_and_copy"
        ],
        "docstring": "Test handling when overwrite is True for a nonexistent file."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_copy_file_success",
        "code_chunk": "def test_copy_file_success(self, s3_client_for_delete_and_copy):\n        \"\"\"Test successful file copying.\"\"\"\n        assert (\n            copy_file(\n                s3_client_for_delete_and_copy,\n                \"source-bucket\",\n                \"source_file.txt\",\n                \"destination-bucket\",\n                \"new_dest_file.txt\",\n            )\n            is True\n        )",
        "variables": [
            "self",
            "s3_client_for_delete_and_copy"
        ],
        "docstring": "Test successful file copying."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_copy_file_failure_existing_destination",
        "code_chunk": "def test_copy_file_failure_existing_destination(\n        self,\n        s3_client_for_delete_and_copy,\n    ):\n        \"\"\"Test failure when the destination file\n        exists and overwrite is False.\n        \"\"\"\n        assert (\n            copy_file(\n                s3_client_for_delete_and_copy,\n                \"source-bucket\",\n                \"source_file.txt\",\n                \"destination-bucket\",\n                \"dest_file.txt\",\n            )\n            is False\n        )",
        "variables": [
            "self",
            "s3_client_for_delete_and_copy"
        ],
        "docstring": "Test failure when the destination file\nexists and overwrite is False."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_copy_file_overwrite_success",
        "code_chunk": "def test_copy_file_overwrite_success(self, s3_client_for_delete_and_copy):\n        \"\"\"Test successful overwrite of an existing destination file.\"\"\"\n        assert (\n            copy_file(\n                s3_client_for_delete_and_copy,\n                \"source-bucket\",\n                \"source_file.txt\",\n                \"destination-bucket\",\n                \"dest_file.txt\",\n                overwrite=True,\n            )\n            is True\n        )",
        "variables": [
            "self",
            "s3_client_for_delete_and_copy"
        ],
        "docstring": "Test successful overwrite of an existing destination file."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_is_s3_directory_true",
        "code_chunk": "def test_is_s3_directory_true(self, s3_client):\n        \"\"\"Test is_s3_directory returns True when the key is a directory.\"\"\"\n        s3_client.put_object(Bucket=\"test-bucket\", Key=\"test-folder/\")\n        assert is_s3_directory(s3_client, \"test-bucket\", \"test-folder/\") is True",
        "variables": [
            "self",
            "s3_client"
        ],
        "docstring": "Test is_s3_directory returns True when the key is a directory."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_is_s3_directory_false",
        "code_chunk": "def test_is_s3_directory_false(self, s3_client):\n        \"\"\"Test is_s3_directory returns False when the key is not a directory.\"\"\"\n        s3_client.put_object(\n            Bucket=\"test-bucket\",\n            Key=\"test-file.txt\",\n            Body=b\"content\",\n        )\n        assert is_s3_directory(s3_client, \"test-bucket\", \"test-file.txt\") is False",
        "variables": [
            "self",
            "s3_client"
        ],
        "docstring": "Test is_s3_directory returns False when the key is not a directory."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_download_folder_success",
        "code_chunk": "def test_download_folder_success(self, s3_client, tmp_path):\n        \"\"\"Test download_folder successfully downloads a folder.\"\"\"\n        s3_client.put_object(\n            Bucket=\"test-bucket\",\n            Key=\"test-folder/file1.txt\",\n            Body=b\"content1\",\n        )\n        s3_client.put_object(\n            Bucket=\"test-bucket\",\n            Key=\"test-folder/file2.txt\",\n            Body=b\"content2\",\n        )\n\n        local_path = tmp_path / \"local-folder\"\n        success = download_folder(\n            s3_client,\n            \"test-bucket\",\n            \"test-folder/\",\n            str(local_path),\n            overwrite=True,\n        )\n\n        assert success is True\n        assert (local_path / \"file1.txt\").exists()\n        assert (local_path / \"file2.txt\").exists()",
        "variables": [
            "tmp_path",
            "self",
            "local_path",
            "success",
            "s3_client"
        ],
        "docstring": "Test download_folder successfully downloads a folder."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_download_folder_no_overwrite",
        "code_chunk": "def test_download_folder_no_overwrite(self, s3_client, tmp_path):\n        \"\"\"Test download_folder does not overwrite existing\n        files if overwrite is False.\n        \"\"\"\n        s3_client.put_object(\n            Bucket=\"test-bucket\",\n            Key=\"test-folder/file1.txt\",\n            Body=b\"content1\",\n        )\n\n        local_path = tmp_path / \"local-folder\"\n        local_path.mkdir(parents=True, exist_ok=True)\n        (local_path / \"file1.txt\").write_text(\"existing content\")\n\n        success = download_folder(\n            s3_client,\n            \"test-bucket\",\n            \"test-folder/\",\n            str(local_path),\n            overwrite=False,\n        )\n\n        assert success is True\n        assert (local_path / \"file1.txt\").read_text() == \"existing content\"",
        "variables": [
            "tmp_path",
            "self",
            "local_path",
            "success",
            "s3_client"
        ],
        "docstring": "Test download_folder does not overwrite existing\nfiles if overwrite is False."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_download_folder_not_directory",
        "code_chunk": "def test_download_folder_not_directory(self, s3_client, tmp_path):\n        \"\"\"Test download_folder returns False when\n        the prefix is not a directory.\n        \"\"\"\n        s3_client.put_object(\n            Bucket=\"test-bucket\",\n            Key=\"not-a-folder.txt\",\n            Body=b\"content\",\n        )\n\n        local_path = tmp_path / \"local-folder\"\n        success = download_folder(\n            s3_client,\n            \"test-bucket\",\n            \"not-a-folder.txt\",\n            str(local_path),\n            overwrite=True,\n        )\n\n        assert success is False",
        "variables": [
            "tmp_path",
            "self",
            "local_path",
            "success",
            "s3_client"
        ],
        "docstring": "Test download_folder returns False when\nthe prefix is not a directory."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_move_file_success",
        "code_chunk": "def test_move_file_success(self, s3_client):\n        \"\"\"Test move_file successfully moves a file between buckets.\"\"\"\n        s3_client.create_bucket(Bucket=\"dest-bucket\")\n        s3_client.put_object(\n            Bucket=\"test-bucket\",\n            Key=\"test-file.txt\",\n            Body=b\"content\",\n        )\n\n        success = move_file(\n            s3_client,\n            \"test-bucket\",\n            \"test-file.txt\",\n            \"dest-bucket\",\n            \"moved-file.txt\",\n        )\n\n        assert success is True\n        assert (\n            s3_client.get_object(Bucket=\"dest-bucket\", Key=\"moved-file.txt\")[\n                \"Body\"\n            ].read()\n            == b\"content\"\n        )\n        assert \"Contents\" not in s3_client.list_objects_v2(\n            Bucket=\"test-bucket\",\n            Prefix=\"test-file.txt\",\n        )",
        "variables": [
            "self",
            "success",
            "s3_client"
        ],
        "docstring": "Test move_file successfully moves a file between buckets."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_move_file_source_not_exist",
        "code_chunk": "def test_move_file_source_not_exist(self, s3_client):\n        \"\"\"Test move_file returns False when the source file does not exist.\"\"\"\n        s3_client.create_bucket(Bucket=\"dest-bucket\")\n\n        success = move_file(\n            s3_client,\n            \"test-bucket\",\n            \"nonexistent.txt\",\n            \"dest-bucket\",\n            \"moved-file.txt\",\n        )\n\n        assert success is False",
        "variables": [
            "self",
            "success",
            "s3_client"
        ],
        "docstring": "Test move_file returns False when the source file does not exist."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_delete_folder_success",
        "code_chunk": "def test_delete_folder_success(self, s3_client):\n        \"\"\"Test delete_folder deletes all objects within a folder.\"\"\"\n        s3_client.put_object(\n            Bucket=\"test-bucket\",\n            Key=\"folder/test-file1.txt\",\n            Body=b\"content1\",\n        )\n        s3_client.put_object(\n            Bucket=\"test-bucket\",\n            Key=\"folder/test-file2.txt\",\n            Body=b\"content2\",\n        )\n\n        result = delete_folder(s3_client, \"test-bucket\", \"folder/\")\n        assert result is True\n\n        # Verify that the folder is empty\n        objects = s3_client.list_objects_v2(\n            Bucket=\"test-bucket\",\n            Prefix=\"folder/\",\n        )\n        assert \"Contents\" not in objects",
        "variables": [
            "self",
            "objects",
            "result",
            "s3_client"
        ],
        "docstring": "Test delete_folder deletes all objects within a folder."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_delete_folder_nonexistent",
        "code_chunk": "def test_delete_folder_nonexistent(self, s3_client):\n        \"\"\"Test delete_folder when the folder does not exist.\"\"\"\n        result = delete_folder(s3_client, \"test-bucket\", \"nonexistent-folder/\")\n        assert result is False",
        "variables": [
            "self",
            "result",
            "s3_client"
        ],
        "docstring": "Test delete_folder when the folder does not exist."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "s3_client",
        "code_chunk": "def s3_client(self):\n        \"\"\"Boto3 S3 client fixture for this test class.\"\"\"\n        with mock_aws():\n            s3 = boto3.client(\"s3\", region_name=\"us-east-1\")\n            s3.create_bucket(Bucket=\"test-bucket\")\n            yield s3",
        "variables": [
            "self",
            "s3"
        ],
        "docstring": "Boto3 S3 client fixture for this test class."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "upload_to_s3",
        "code_chunk": "def upload_to_s3(self, s3_client, bucket_name, key, data):\n        \"\"\"Upload a string as a CSV file to S3.\"\"\"\n        s3_client.put_object(Bucket=bucket_name, Key=key, Body=data)",
        "variables": [
            "data",
            "self",
            "key",
            "bucket_name",
            "s3_client"
        ],
        "docstring": "Upload a string as a CSV file to S3."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_csv_basic",
        "code_chunk": "def test_load_csv_basic(self, s3_client):\n        \"\"\"Test loading CSV file.\"\"\"\n        self.upload_to_s3(s3_client, \"test-bucket\", \"test_basic.csv\", self.data_basic)\n        df = load_csv(s3_client, \"test-bucket\", \"test_basic.csv\")\n        assert len(df) == 3\n        assert len(df.columns) == 3",
        "variables": [
            "self",
            "df",
            "s3_client"
        ],
        "docstring": "Test loading CSV file."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_csv_multiline",
        "code_chunk": "def test_load_csv_multiline(self, s3_client):\n        \"\"\"Test loading multiline CSV file.\"\"\"\n        self.upload_to_s3(\n            s3_client,\n            \"test-bucket\",\n            \"test_multiline.csv\",\n            self.data_multiline,\n        )\n        df = load_csv(s3_client, \"test-bucket\", \"test_multiline.csv\")\n        assert len(df) == 2\n        assert len(df.columns) == 3",
        "variables": [
            "self",
            "df",
            "s3_client"
        ],
        "docstring": "Test loading multiline CSV file."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_csv_keep_columns",
        "code_chunk": "def test_load_csv_keep_columns(self, s3_client):\n        \"\"\"Test keeping specific columns.\"\"\"\n        self.upload_to_s3(\n            s3_client,\n            \"test-bucket\",\n            \"test_keep_columns.csv\",\n            self.data_basic,\n        )\n        df = load_csv(\n            s3_client,\n            \"test-bucket\",\n            \"test_keep_columns.csv\",\n            keep_columns=[\"col1\", \"col2\"],\n        )\n        assert len(df) == 3\n        assert len(df.columns) == 2\n        assert \"col1\" in df.columns\n        assert \"col2\" in df.columns\n        assert \"col3\" not in df.columns",
        "variables": [
            "self",
            "df",
            "s3_client"
        ],
        "docstring": "Test keeping specific columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_csv_drop_columns",
        "code_chunk": "def test_load_csv_drop_columns(self, s3_client):\n        \"\"\"Test dropping specific columns.\"\"\"\n        self.upload_to_s3(\n            s3_client,\n            \"test-bucket\",\n            \"test_drop_columns.csv\",\n            self.data_basic,\n        )\n        df = load_csv(\n            s3_client,\n            \"test-bucket\",\n            \"test_drop_columns.csv\",\n            drop_columns=[\"col2\"],\n        )\n        assert len(df) == 3\n        assert len(df.columns) == 2\n        assert \"col1\" in df.columns\n        assert \"col3\" in df.columns\n        assert \"col2\" not in df.columns",
        "variables": [
            "self",
            "df",
            "s3_client"
        ],
        "docstring": "Test dropping specific columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_csv_rename_columns",
        "code_chunk": "def test_load_csv_rename_columns(self, s3_client):\n        \"\"\"Test renaming columns.\"\"\"\n        self.upload_to_s3(\n            s3_client,\n            \"test-bucket\",\n            \"test_rename_columns.csv\",\n            self.data_basic,\n        )\n        df = load_csv(\n            s3_client,\n            \"test-bucket\",\n            \"test_rename_columns.csv\",\n            rename_columns={\"col1\": \"new_col1\", \"col3\": \"new_col3\"},\n        )\n        assert len(df) == 3\n        assert len(df.columns) == 3\n        assert \"new_col1\" in df.columns\n        assert \"col1\" not in df.columns\n        assert \"new_col3\" in df.columns\n        assert \"col3\" not in df.columns",
        "variables": [
            "self",
            "df",
            "s3_client"
        ],
        "docstring": "Test renaming columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_csv_missing_keep_column",
        "code_chunk": "def test_load_csv_missing_keep_column(self, s3_client):\n        \"\"\"Test error when keep column is missing.\"\"\"\n        self.upload_to_s3(\n            s3_client,\n            \"test-bucket\",\n            \"test_missing_keep_column.csv\",\n            self.data_basic,\n        )\n        with pytest.raises(ValueError):\n            load_csv(\n                s3_client,\n                \"test-bucket\",\n                \"test_missing_keep_column.csv\",\n                keep_columns=[\"col4\"],\n            )",
        "variables": [
            "self",
            "s3_client"
        ],
        "docstring": "Test error when keep column is missing."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_csv_missing_drop_column",
        "code_chunk": "def test_load_csv_missing_drop_column(self, s3_client):\n        \"\"\"Test error when drop column is missing.\"\"\"\n        self.upload_to_s3(\n            s3_client,\n            \"test-bucket\",\n            \"test_missing_drop_column.csv\",\n            self.data_basic,\n        )\n        with pytest.raises(ValueError):\n            load_csv(\n                s3_client,\n                \"test-bucket\",\n                \"test_missing_drop_column.csv\",\n                drop_columns=[\"col4\"],\n            )",
        "variables": [
            "self",
            "s3_client"
        ],
        "docstring": "Test error when drop column is missing."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_csv_missing_rename_column",
        "code_chunk": "def test_load_csv_missing_rename_column(self, s3_client):\n        \"\"\"Test error when rename column is missing.\"\"\"\n        self.upload_to_s3(\n            s3_client,\n            \"test-bucket\",\n            \"test_missing_rename_column.csv\",\n            self.data_basic,\n        )\n        with pytest.raises(ValueError):\n            load_csv(\n                s3_client,\n                \"test-bucket\",\n                \"test_missing_rename_column.csv\",\n                rename_columns={\"col4\": \"new_col4\"},\n            )",
        "variables": [
            "self",
            "s3_client"
        ],
        "docstring": "Test error when rename column is missing."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_csv_with_encoding",
        "code_chunk": "def test_load_csv_with_encoding(self, s3_client):\n        \"\"\"Test loading CSV with a specific encoding.\"\"\"\n        self.upload_to_s3(\n            s3_client,\n            \"test-bucket\",\n            \"test_encoding.csv\",\n            self.data_basic,\n        )\n        df = load_csv(\n            s3_client,\n            \"test-bucket\",\n            \"test_encoding.csv\",\n            encoding=\"ISO-8859-1\",\n        )\n        assert len(df) == 3\n        assert len(df.columns) == 3",
        "variables": [
            "self",
            "df",
            "s3_client"
        ],
        "docstring": "Test loading CSV with a specific encoding."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_csv_with_custom_delimiter",
        "code_chunk": "def test_load_csv_with_custom_delimiter(self, s3_client):\n        \"\"\"Test loading CSV with a custom delimiter.\"\"\"\n        data_with_semicolon = \"\"\"col1;col2;col3\n1;A;foo\n2;B;bar\n3;C;baz\n\"\"\"\n        self.upload_to_s3(\n            s3_client,\n            \"test-bucket\",\n            \"test_custom_delimiter.csv\",\n            data_with_semicolon,\n        )\n        df = load_csv(s3_client, \"test-bucket\", \"test_custom_delimiter.csv\", sep=\";\")\n        assert len(df) == 3\n        assert len(df.columns) == 3",
        "variables": [
            "df",
            "self",
            "data_with_semicolon",
            "s3_client"
        ],
        "docstring": "Test loading CSV with a custom delimiter."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_csv_with_custom_quote",
        "code_chunk": "def test_load_csv_with_custom_quote(self, s3_client):\n        \"\"\"Test loading CSV with a custom quote character.\"\"\"\n        data_with_custom_quote = \"\"\"col1,col2,col3\n    1,A,foo\n    2,B,'bar'\n    3,C,'baz'\n    \"\"\"\n        self.upload_to_s3(\n            s3_client,\n            \"test-bucket\",\n            \"test_custom_quote.csv\",\n            data_with_custom_quote,\n        )\n        df = load_csv(s3_client, \"test-bucket\", \"test_custom_quote.csv\", quotechar=\"'\")\n        assert len(df) == 3\n        assert len(df.columns) == 3\n        assert df[df[\"col3\"] == \"bar\"].shape[0] == 1\n        assert df[df[\"col3\"] == \"baz\"].shape[0] == 1",
        "variables": [
            "self",
            "data_with_custom_quote",
            "df",
            "s3_client"
        ],
        "docstring": "Test loading CSV with a custom quote character."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "s3_client",
        "code_chunk": "def s3_client(self):\n        \"\"\"Boto3 S3 client fixture for this test class.\"\"\"\n        with mock_aws():\n            s3 = boto3.client(\"s3\", region_name=\"us-east-1\")\n            s3.create_bucket(Bucket=\"test-bucket\")\n            yield s3",
        "variables": [
            "self",
            "s3"
        ],
        "docstring": "Boto3 S3 client fixture for this test class."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "upload_json_to_s3",
        "code_chunk": "def upload_json_to_s3(self, s3_client, bucket_name, key, data):\n        \"\"\"Upload a dictionary as a JSON file to S3.\"\"\"\n        s3_client.put_object(Bucket=bucket_name, Key=key, Body=json.dumps(data))",
        "variables": [
            "data",
            "self",
            "key",
            "bucket_name",
            "s3_client"
        ],
        "docstring": "Upload a dictionary as a JSON file to S3."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_json_success",
        "code_chunk": "def test_load_json_success(self, s3_client):\n        \"\"\"Test load_json successfully reads a JSON file.\"\"\"\n        data = {\"name\": \"John\", \"age\": 30, \"city\": \"Manchester\"}\n        self.upload_json_to_s3(s3_client, \"test-bucket\", \"test-file.json\", data)\n\n        result = load_json(s3_client, \"test-bucket\", \"test-file.json\")\n        assert result == data",
        "variables": [
            "self",
            "data",
            "result",
            "s3_client"
        ],
        "docstring": "Test load_json successfully reads a JSON file."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_json_nonexistent_file",
        "code_chunk": "def test_load_json_nonexistent_file(self, s3_client):\n        \"\"\"Test load_json raises an exception for a nonexistent file.\"\"\"\n        with pytest.raises(Exception):\n            load_json(s3_client, \"test-bucket\", \"nonexistent.json\")",
        "variables": [
            "self",
            "s3_client"
        ],
        "docstring": "Test load_json raises an exception for a nonexistent file."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_json_invalid_json",
        "code_chunk": "def test_load_json_invalid_json(self, s3_client):\n        \"\"\"Test load_json raises an exception when the JSON file is invalid.\"\"\"\n        s3_client.put_object(\n            Bucket=\"test-bucket\",\n            Key=\"invalid.json\",\n            Body=\"not a valid json\",\n        )\n\n        with pytest.raises(Exception):\n            load_json(s3_client, \"test-bucket\", \"invalid.json\")",
        "variables": [
            "self",
            "s3_client"
        ],
        "docstring": "Test load_json raises an exception when the JSON file is invalid."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_json_with_encoding",
        "code_chunk": "def test_load_json_with_encoding(self, s3_client):\n        \"\"\"Test load_json with a specific encoding.\"\"\"\n        data = {\"name\": \"John\", \"age\": 30, \"city\": \"Manchester\"}\n\n        # Convert the dictionary to JSON string and encode it in 'utf-16'\n        json_data = json.dumps(data).encode(\"utf-16\")\n\n        # Upload the utf-16 encoded JSON file to S3\n        s3_client.put_object(\n            Bucket=\"test-bucket\",\n            Key=\"test-file-utf16.json\",\n            Body=json_data,\n        )\n\n        # Read the file back, specifying the 'utf-16' encoding\n        result = load_json(\n            s3_client,\n            \"test-bucket\",\n            \"test-file-utf16.json\",\n            encoding=\"utf-16\",\n        )\n        assert result == data",
        "variables": [
            "json_data",
            "data",
            "self",
            "result",
            "s3_client"
        ],
        "docstring": "Test load_json with a specific encoding."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_load_json_invalid_bucket_name",
        "code_chunk": "def test_load_json_invalid_bucket_name(self, s3_client):\n        \"\"\"Test load_json raises InvalidBucketNameError for invalid bucket name.\"\"\"\n        data = {\"name\": \"John\", \"age\": 30}\n        self.upload_json_to_s3(s3_client, \"test-bucket\", \"test-file.json\", data)\n\n        with pytest.raises(InvalidBucketNameError):\n            load_json(s3_client, \"INVALID_BUCKET\", \"test-file.json\")",
        "variables": [
            "self",
            "data",
            "s3_client"
        ],
        "docstring": "Test load_json raises InvalidBucketNameError for invalid bucket name."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "s3_client",
        "code_chunk": "def s3_client(self):\n        \"\"\"Boto3 S3 client fixture for this test class.\"\"\"\n        with mock_aws():\n            s3 = boto3.client(\"s3\", region_name=\"us-east-1\")\n            s3.create_bucket(Bucket=\"test-bucket\")\n            yield s3",
        "variables": [
            "self",
            "s3"
        ],
        "docstring": "Boto3 S3 client fixture for this test class."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_write_csv_success",
        "code_chunk": "def test_write_csv_success(self, s3_client):\n        \"\"\"Test that write_csv returns True if successful.\"\"\"\n        data = {\"name\": [\"John\"], \"age\": [30], \"city\": [\"Manchester\"]}\n        df = pd.DataFrame(data)\n\n        result = write_csv(s3_client, \"test-bucket\", df, \"test_file.csv\")\n        assert result",
        "variables": [
            "data",
            "df",
            "self",
            "result",
            "s3_client"
        ],
        "docstring": "Test that write_csv returns True if successful."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_write_csv_read_back",
        "code_chunk": "def test_write_csv_read_back(self, s3_client):\n        \"\"\"Test that a file wrtitten by write_csv can be read back and returns\n        the same dataframe as input. Uses kwargs.\n        \"\"\"\n        data = {\"name\": [\"John\"], \"age\": [30], \"city\": [\"Manchester\"]}\n        df = pd.DataFrame(data)\n\n        _ = write_csv(s3_client, \"test-bucket\", df, \"test_file.csv\", index=False)\n        result = load_csv(s3_client, \"test-bucket\", \"test_file.csv\")\n        pd.testing.assert_frame_equal(df, result)",
        "variables": [
            "_",
            "data",
            "df",
            "self",
            "result",
            "s3_client"
        ],
        "docstring": "Test that a file wrtitten by write_csv can be read back and returns\nthe same dataframe as input. Uses kwargs."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_write_csv_failure",
        "code_chunk": "def test_write_csv_failure(self, s3_client):\n        \"\"\"Test that write_csv returns False if unable to write.\n        Dictionary data does not have to_csv method.\n        \"\"\"\n        data = {\"name\": [\"John\"], \"age\": [30], \"city\": [\"Manchester\"]}\n\n        result = write_csv(s3_client, \"test-bucket\", data, \"test_file.csv\", index=False)\n        assert not result",
        "variables": [
            "self",
            "data",
            "result",
            "s3_client"
        ],
        "docstring": "Test that write_csv returns False if unable to write.\nDictionary data does not have to_csv method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "s3_client",
        "code_chunk": "def s3_client(self):\n        \"\"\"Boto3 S3 client fixture for this test class.\"\"\"\n        with mock_aws():\n            s3 = boto3.client(\"s3\", region_name=\"us-east-1\")\n            s3.create_bucket(Bucket=\"test-bucket\")\n            yield s3",
        "variables": [
            "self",
            "s3"
        ],
        "docstring": "Boto3 S3 client fixture for this test class."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_write_excel_success",
        "code_chunk": "def test_write_excel_success(self, s3_client):\n        \"\"\"Test that write_excel returns True if successful.\"\"\"\n        data = {\"name\": [\"John\"], \"age\": [30], \"city\": [\"Manchester\"]}\n        df = pd.DataFrame(data)\n\n        result = write_excel(s3_client, \"test-bucket\", df, \"test_file.xlsx\")\n        assert result",
        "variables": [
            "data",
            "df",
            "self",
            "result",
            "s3_client"
        ],
        "docstring": "Test that write_excel returns True if successful."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_write_excel_read_back",
        "code_chunk": "def test_write_excel_read_back(self, s3_client):\n        \"\"\"Test that a file written by write_excel can be read back and returns\n        the same dataframe as input. Uses kwargs.\n        \"\"\"\n        data = {\"name\": [\"John\"], \"age\": [30], \"city\": [\"Manchester\"]}\n        df = pd.DataFrame(data)\n\n        _ = write_excel(s3_client, \"test-bucket\", df, \"test_file.xlsx\", index=False)\n\n        # Read back the file from S3\n        obj = s3_client.get_object(Bucket=\"test-bucket\", Key=\"test_file.xlsx\")\n        excel_buffer = BytesIO(obj[\"Body\"].read())\n\n        # Load the Excel file into a DataFrame\n        result_df = pd.read_excel(excel_buffer, engine=\"openpyxl\")\n\n        pd.testing.assert_frame_equal(df, result_df)",
        "variables": [
            "_",
            "data",
            "df",
            "self",
            "excel_buffer",
            "obj",
            "result_df",
            "s3_client"
        ],
        "docstring": "Test that a file written by write_excel can be read back and returns\nthe same dataframe as input. Uses kwargs."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\helpers\\test_s3_utils.py",
        "function_name": "test_write_excel_failure",
        "code_chunk": "def test_write_excel_failure(self, s3_client):\n        \"\"\"Test that write_excel returns False if unable to write.\n        Dictionary data does not have a to_excel method.\n        \"\"\"\n        data = {\"name\": [\"John\"], \"age\": [30], \"city\": [\"Manchester\"]}\n\n        result = write_excel(\n            s3_client,\n            \"test-bucket\",\n            data,\n            \"test_file.xlsx\",\n            index=False,\n        )\n        assert not result",
        "variables": [
            "self",
            "data",
            "result",
            "s3_client"
        ],
        "docstring": "Test that write_excel returns False if unable to write.\nDictionary data does not have a to_excel method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "setup_and_teardown_database",
        "code_chunk": "def setup_and_teardown_database(  # noqa: PT004\n        self,\n        spark_session: SparkSession,\n    ) -> Generator[None, None, None]:\n        \"\"\"\n        Fixture that sets up a dummy Spark database for testing.\n\n        This fixture creates a test database named 'temp_test_db'. After the\n        tests using this fixture are completed, it cleans up by dropping the\n        test database.\n\n        Parameters\n        ----------\n        spark_session : SparkSession\n            Active SparkSession to use for creating and deleting the test\n            database.\n\n        Yields\n        ------\n        None\n        \"\"\"\n        spark_session.sql(\"CREATE DATABASE IF NOT EXISTS temp_test_db\")\n        yield\n        spark_session.sql(\"DROP DATABASE IF EXISTS temp_test_db\")",
        "variables": [
            "self",
            "spark_session"
        ],
        "docstring": "Fixture that sets up a dummy Spark database for testing.\n\nThis fixture creates a test database named 'temp_test_db'. After the\ntests using this fixture are completed, it cleans up by dropping the\ntest database.\n\nParameters\n----------\nspark_session : SparkSession\n    Active SparkSession to use for creating and deleting the test\n    database.\n\nYields\n------\nNone"
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_get_current_database_default",
        "code_chunk": "def test_get_current_database_default(\n        self,\n        spark_session: SparkSession,\n    ) -> None:\n        \"\"\"Test that get_current_database returns the default database when no\n        database is explicitly set.\n        \"\"\"\n        current_db = get_current_database(spark_session)\n        default_db = spark_session.sql(\"SELECT current_database()\").collect()[0][\n            \"current_database()\"\n        ]\n        assert current_db == default_db",
        "variables": [
            "self",
            "default_db",
            "spark_session",
            "current_db"
        ],
        "docstring": "Test that get_current_database returns the default database when no\ndatabase is explicitly set."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_get_current_database_after_setting",
        "code_chunk": "def test_get_current_database_after_setting(\n        self,\n        spark_session: SparkSession,\n        setup_and_teardown_database: None,\n    ) -> None:\n        \"\"\"Test that get_current_database returns the correct database after\n        explicitly setting a different active database.\n        \"\"\"\n        spark_session.sql(\"USE temp_test_db\")\n        current_db = get_current_database(spark_session)\n        assert current_db == \"temp_test_db\"",
        "variables": [
            "self",
            "setup_and_teardown_database",
            "spark_session",
            "current_db"
        ],
        "docstring": "Test that get_current_database returns the correct database after\nexplicitly setting a different active database."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "dummy_database_and_table",
        "code_chunk": "def dummy_database_and_table(\n        self,\n        spark_session: SparkSession,\n    ) -> Generator[None, None, None]:\n        \"\"\"Fixture that creates a dummy Spark database and table for testing.\n\n        This fixture creates a test database named 'test_db' and a test table\n        named 'test_table' in that database. The table is simple and contains\n        two columns: 'name' (a string) and 'age' (an integer).\n\n        The name of the table in the form 'database.table' is then\n        yielded for use in the tests.\n\n        After the tests using this fixture are completed, it cleans up by\n        dropping the test table and the test database.\n\n        Parameters\n        ----------\n        spark_session\n            Active SparkSession to use for creating and deleting the test\n            database and table.\n\n        Yields\n        ------\n        str\n            The name of the test table in the form 'database.table'.\n        \"\"\"\n        spark_session.sql(\"CREATE DATABASE IF NOT EXISTS test_db\")\n        spark_session.sql(\n            \"CREATE TABLE IF NOT EXISTS test_db.test_table (name STRING, age INT)\",\n        )\n        yield \"test_db.test_table\"\n        spark_session.sql(\"DROP TABLE IF EXISTS test_db.test_table\")\n        spark_session.sql(\"DROP DATABASE IF EXISTS test_db\")",
        "variables": [
            "self",
            "spark_session"
        ],
        "docstring": "Fixture that creates a dummy Spark database and table for testing.\n\nThis fixture creates a test database named 'test_db' and a test table\nnamed 'test_table' in that database. The table is simple and contains\ntwo columns: 'name' (a string) and 'age' (an integer).\n\nThe name of the table in the form 'database.table' is then\nyielded for use in the tests.\n\nAfter the tests using this fixture are completed, it cleans up by\ndropping the test table and the test database.\n\nParameters\n----------\nspark_session\n    Active SparkSession to use for creating and deleting the test\n    database and table.\n\nYields\n------\nstr\n    The name of the test table in the form 'database.table'."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_extract_database_name_correct_format",
        "code_chunk": "def test_extract_database_name_correct_format(\n        self,\n        spark_session: SparkSession,\n        dummy_database_and_table: str,\n    ) -> None:\n        \"\"\"Test that extract_database_name correctly identifies the database and\n        table name from a correctly formatted input.\n        \"\"\"\n        long_table_name = dummy_database_and_table\n        db_name, table_name = extract_database_name(\n            spark_session,\n            long_table_name,\n        )\n        assert db_name == \"test_db\"\n        assert table_name == \"test_table\"",
        "variables": [
            "spark_session",
            "self",
            "db_name",
            "dummy_database_and_table",
            "table_name",
            "long_table_name"
        ],
        "docstring": "Test that extract_database_name correctly identifies the database and\ntable name from a correctly formatted input."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_extract_database_name_incorrect_format",
        "code_chunk": "def test_extract_database_name_incorrect_format(\n        self,\n        spark_session: SparkSession,\n    ) -> None:\n        \"\"\"Test that extract_database_name raises a ValueError when the input is\n        incorrectly formatted.\n        \"\"\"\n        long_table_name = \"part1.part2.part3.part4\"\n        with pytest.raises(ValueError):\n            db_name, table_name = extract_database_name(\n                spark_session,\n                long_table_name,\n            )",
        "variables": [
            "spark_session",
            "self",
            "db_name",
            "table_name",
            "long_table_name"
        ],
        "docstring": "Test that extract_database_name raises a ValueError when the input is\nincorrectly formatted."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_extract_database_name_gcp_format",
        "code_chunk": "def test_extract_database_name_gcp_format(\n        self,\n        spark_session: SparkSession,\n    ) -> None:\n        \"\"\"Test that extract_database_name correctly identifies the database and\n        table name from the GCP format input.\n        \"\"\"\n        long_table_name = \"project_name.test_db.test_table\"\n        db_name, table_name = extract_database_name(\n            spark_session,\n            long_table_name,\n        )\n        assert db_name == \"test_db\"\n        assert table_name == \"test_table\"",
        "variables": [
            "spark_session",
            "self",
            "db_name",
            "table_name",
            "long_table_name"
        ],
        "docstring": "Test that extract_database_name correctly identifies the database and\ntable name from the GCP format input."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_extract_database_name_no_specified_database",
        "code_chunk": "def test_extract_database_name_no_specified_database(\n        self,\n        spark_session: SparkSession,\n    ) -> None:\n        \"\"\"Test that extract_database_name correctly identifies the current\n        database when no database is specified in the input.\n        \"\"\"\n        long_table_name = \"test_table\"\n        db_name, table_name = extract_database_name(\n            spark_session,\n            long_table_name,\n        )\n        current_db = spark_session.sql(\"SELECT current_database()\").collect()[0][\n            \"current_database()\"\n        ]\n        assert db_name == current_db\n        assert table_name == \"test_table\"",
        "variables": [
            "spark_session",
            "current_db",
            "self",
            "db_name",
            "table_name",
            "long_table_name"
        ],
        "docstring": "Test that extract_database_name correctly identifies the current\ndatabase when no database is specified in the input."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_load_and_validate_table_with_empty_table",
        "code_chunk": "def test_load_and_validate_table_with_empty_table(self) -> None:\n        \"\"\"Test that load_and_validate_table raises a ValueError when the table\n        is empty and skip_validation is False.\n        \"\"\"\n        table_name = \"empty_table\"\n        # Mock SparkSession and DataFrame\n        spark_session = MagicMock(spec=SparkSession)\n        df = MagicMock(spec=SparkDF)\n        df.rdd.isEmpty.return_value = True\n        spark_session.read.table.return_value = df\n        with pytest.raises(DataframeEmptyError):\n            load_and_validate_table(spark_session, table_name)",
        "variables": [
            "self",
            "table_name",
            "spark_session",
            "df"
        ],
        "docstring": "Test that load_and_validate_table raises a ValueError when the table\nis empty and skip_validation is False."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_load_and_validate_table_with_non_existing_table",
        "code_chunk": "def test_load_and_validate_table_with_non_existing_table(self) -> None:\n        \"\"\"Test that load_and_validate_table raises a PermissionError when the\n        table doesn't exist.\n        \"\"\"\n        table_name = \"non_existing_table\"\n        # Mock SparkSession\n        spark_session = MagicMock(spec=SparkSession)\n        spark_session.read.table.side_effect = Exception(\"Table not found.\")\n        with pytest.raises(PermissionError):\n            load_and_validate_table(spark_session, table_name)",
        "variables": [
            "self",
            "table_name",
            "spark_session"
        ],
        "docstring": "Test that load_and_validate_table raises a PermissionError when the\ntable doesn't exist."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_load_and_validate_table_with_filter",
        "code_chunk": "def test_load_and_validate_table_with_filter(self) -> None:\n        \"\"\"Test that load_and_validate_table applies the filter condition and\n        raises a ValueError when the DataFrame is empty after filtering.\n        \"\"\"\n        table_name = \"test_table\"\n        filter_cond = \"age > 30\"\n        # Mock SparkSession and DataFrame\n        spark_session = MagicMock(spec=SparkSession)\n        df = MagicMock(spec=SparkDF)\n        df.rdd.isEmpty.side_effect = [False, True]\n        spark_session.read.table.return_value = df\n        with pytest.raises(DataframeEmptyError):\n            load_and_validate_table(\n                spark_session,\n                table_name,\n                filter_cond=filter_cond,\n            )",
        "variables": [
            "spark_session",
            "df",
            "self",
            "filter_cond",
            "table_name"
        ],
        "docstring": "Test that load_and_validate_table applies the filter condition and\nraises a ValueError when the DataFrame is empty after filtering."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_load_and_validate_table_with_skip_validation",
        "code_chunk": "def test_load_and_validate_table_with_skip_validation(self) -> None:\n        \"\"\"Test that load_and_validate_table doesn't raise any exceptions when\n        skip_validation is True even if the table is empty.\n        \"\"\"\n        table_name = \"empty_table\"\n        # Mock SparkSession and DataFrame\n        spark_session = MagicMock(spec=SparkSession)\n        df = MagicMock(spec=SparkDF)\n        df.rdd.isEmpty.return_value = True\n        spark_session.read.table.return_value = df\n        # No exception is expected to be raised here\n        load_and_validate_table(spark_session, table_name, skip_validation=True)",
        "variables": [
            "self",
            "table_name",
            "spark_session",
            "df"
        ],
        "docstring": "Test that load_and_validate_table doesn't raise any exceptions when\nskip_validation is True even if the table is empty."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_load_and_validate_table_with_normal_table",
        "code_chunk": "def test_load_and_validate_table_with_normal_table(self) -> None:\n        \"\"\"Test that load_and_validate_table works correctly when the table\n        exists, is not empty, and doesn't need a filter.\n        \"\"\"\n        table_name = \"normal_table\"\n        # Mock SparkSession and DataFrame\n        spark_session = MagicMock(spec=SparkSession)\n        df = MagicMock(spec=SparkDF)\n        df.rdd.isEmpty.return_value = False\n        spark_session.read.table.return_value = df\n        # No exception is expected to be raised here\n        result = load_and_validate_table(spark_session, table_name)\n        # Check that the returned DataFrame is our mock DataFrame\n        assert result == df",
        "variables": [
            "spark_session",
            "df",
            "self",
            "result",
            "table_name"
        ],
        "docstring": "Test that load_and_validate_table works correctly when the table\nexists, is not empty, and doesn't need a filter."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_load_and_validate_table_with_keep_columns",
        "code_chunk": "def test_load_and_validate_table_with_keep_columns(self) -> None:\n        \"\"\"Test that load_and_validate_table keeps only the specified columns.\"\"\"\n        table_name = \"test_table\"\n        keep_columns = [\"name\", \"age\"]\n        # Mock SparkSession and DataFrame\n        spark_session = MagicMock(spec=SparkSession)\n        df = MagicMock(spec=SparkDF)\n        df.columns = [\"name\", \"age\", \"city\"]\n        df.rdd.isEmpty.return_value = False\n        df.select.return_value = df\n        spark_session.read.table.return_value = df\n        # No exception is expected to be raised here\n        result = load_and_validate_table(\n            spark_session,\n            table_name,\n            keep_columns=keep_columns,\n        )\n        df.select.assert_called_once_with(*keep_columns)\n        assert result == df",
        "variables": [
            "spark_session",
            "df",
            "self",
            "result",
            "keep_columns",
            "table_name"
        ],
        "docstring": "Test that load_and_validate_table keeps only the specified columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_load_and_validate_table_with_drop_columns",
        "code_chunk": "def test_load_and_validate_table_with_drop_columns(self) -> None:\n        \"\"\"Test that load_and_validate_table drops the specified columns.\"\"\"\n        table_name = \"test_table\"\n        drop_columns = [\"city\"]\n        # Mock SparkSession and DataFrame\n        spark_session = MagicMock(spec=SparkSession)\n        df = MagicMock(spec=SparkDF)\n        df.columns = [\"name\", \"age\", \"city\"]\n        df.rdd.isEmpty.return_value = False\n        df.drop.return_value = df\n        spark_session.read.table.return_value = df\n        # No exception is expected to be raised here\n        result = load_and_validate_table(\n            spark_session,\n            table_name,\n            drop_columns=drop_columns,\n        )\n        df.drop.assert_called_once_with(\"city\")\n        assert result == df",
        "variables": [
            "spark_session",
            "drop_columns",
            "df",
            "self",
            "result",
            "table_name"
        ],
        "docstring": "Test that load_and_validate_table drops the specified columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_load_and_validate_table_with_rename_columns",
        "code_chunk": "def test_load_and_validate_table_with_rename_columns(self) -> None:\n        \"\"\"Test that load_and_validate_table renames the specified columns.\"\"\"\n        table_name = \"test_table\"\n        rename_columns = {\"name\": \"full_name\"}\n        # Mock SparkSession and DataFrame\n        spark_session = MagicMock(spec=SparkSession)\n        df = MagicMock(spec=SparkDF)\n        df.columns = [\"name\", \"age\", \"city\"]\n        df.rdd.isEmpty.return_value = False\n        df.withColumnRenamed.return_value = df\n        spark_session.read.table.return_value = df\n        # No exception is expected to be raised here\n        result = load_and_validate_table(\n            spark_session,\n            table_name,\n            rename_columns=rename_columns,\n        )\n        df.withColumnRenamed.assert_called_once_with(\"name\", \"full_name\")\n        assert result == df",
        "variables": [
            "spark_session",
            "rename_columns",
            "df",
            "self",
            "result",
            "table_name"
        ],
        "docstring": "Test that load_and_validate_table renames the specified columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_load_and_validate_table_with_combined_transformations",
        "code_chunk": "def test_load_and_validate_table_with_combined_transformations(self) -> None:\n        \"\"\"Test that load_and_validate_table applies keep, drop, and rename\n        transformations in the correct order.\n        \"\"\"\n        table_name = \"test_table\"\n        keep_columns = [\"name\", \"age\", \"city\"]\n        drop_columns = [\"city\"]\n        rename_columns = {\"name\": \"full_name\"}\n        # Mock SparkSession and DataFrame\n        spark_session = MagicMock(spec=SparkSession)\n        df = MagicMock(spec=SparkDF)\n        df.columns = [\"name\", \"age\", \"city\", \"country\"]\n        df.rdd.isEmpty.return_value = False\n        df.select.return_value = df\n        df.drop.return_value = df\n        df.withColumnRenamed.return_value = df\n        spark_session.read.table.return_value = df\n        # No exception is expected to be raised here\n        result = load_and_validate_table(\n            spark_session,\n            table_name,\n            keep_columns=keep_columns,\n            drop_columns=drop_columns,\n            rename_columns=rename_columns,\n        )\n        df.select.assert_called_once_with(*keep_columns)\n        df.drop.assert_called_once_with(\"city\")\n        df.withColumnRenamed.assert_called_once_with(\"name\", \"full_name\")\n        assert result == df",
        "variables": [
            "spark_session",
            "rename_columns",
            "drop_columns",
            "df",
            "self",
            "result",
            "keep_columns",
            "table_name"
        ],
        "docstring": "Test that load_and_validate_table applies keep, drop, and rename\ntransformations in the correct order."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "setup_class",
        "code_chunk": "def setup_class(cls):\n        \"\"\"Set up SparkSession for tests.\"\"\"\n        cls.spark = (\n            SparkSession.builder.master(\"local\")\n            .appName(\"test_get_tables_in_database\")\n            .getOrCreate()\n        )\n        cls.spark.sql(\"CREATE DATABASE IF NOT EXISTS test_db\")\n        cls.spark.sql(\"USE test_db\")\n        cls.spark.sql(\"CREATE TABLE IF NOT EXISTS test_table1 (id INT, name STRING)\")\n        cls.spark.sql(\"CREATE TABLE IF NOT EXISTS test_table2 (id INT, name STRING)\")",
        "variables": [
            "cls"
        ],
        "docstring": "Set up SparkSession for tests."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "teardown_class",
        "code_chunk": "def teardown_class(cls):\n        \"\"\"Tear down SparkSession after tests.\"\"\"\n        cls.spark.sql(\"DROP TABLE IF EXISTS test_db.test_table1\")\n        cls.spark.sql(\"DROP TABLE IF EXISTS test_db.test_table2\")\n        cls.spark.sql(\"DROP DATABASE IF EXISTS test_db\")\n        cls.spark.stop()",
        "variables": [
            "cls"
        ],
        "docstring": "Tear down SparkSession after tests."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_get_tables_in_existing_database",
        "code_chunk": "def test_get_tables_in_existing_database(self):\n        \"\"\"Test with existing database.\"\"\"\n        tables = get_tables_in_database(self.spark, \"test_db\")\n        assert \"test_table1\" in tables\n        assert \"test_table2\" in tables",
        "variables": [
            "self",
            "tables"
        ],
        "docstring": "Test with existing database."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_get_tables_in_non_existing_database",
        "code_chunk": "def test_get_tables_in_non_existing_database(self):\n        \"\"\"Test with non-existing database.\"\"\"\n        with pytest.raises(\n            ValueError,\n            match=\"Error fetching tables from database non_existing_db\",\n        ):\n            get_tables_in_database(self.spark, \"non_existing_db\")",
        "variables": [
            "self"
        ],
        "docstring": "Test with non-existing database."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_get_tables_with_no_tables",
        "code_chunk": "def test_get_tables_with_no_tables(self):\n        \"\"\"Test with database having no tables.\"\"\"\n        self.spark.sql(\"CREATE DATABASE IF NOT EXISTS empty_db\")\n        tables = get_tables_in_database(self.spark, \"empty_db\")\n        assert tables == []\n        self.spark.sql(\"DROP DATABASE IF EXISTS empty_db\")",
        "variables": [
            "self",
            "tables"
        ],
        "docstring": "Test with database having no tables."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "test_get_tables_with_exception",
        "code_chunk": "def test_get_tables_with_exception(self):\n        \"\"\"Test exception handling.\"\"\"\n        original_sql = self.spark.sql\n\n        def mock_sql(query):\n            raise RuntimeError(\"Test exception\")  # noqa: EM101\n\n        self.spark.sql = mock_sql\n\n        try:\n            with pytest.raises(\n                ValueError,\n                match=\"Error fetching tables from database test_db\",\n            ):\n                get_tables_in_database(self.spark, \"test_db\")\n        finally:\n            self.spark.sql = original_sql",
        "variables": [
            "self",
            "original_sql"
        ],
        "docstring": "Test exception handling."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_input.py",
        "function_name": "mock_sql",
        "code_chunk": "def mock_sql(query):\n            raise RuntimeError(\"Test exception\")",
        "variables": [
            "query"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_df",
        "code_chunk": "def test_df(self, spark_session: SparkSession, create_spark_df: Callable):\n        \"\"\"Fixture to create a test DataFrame with the help of `create_spark_df`\n        callable.\n\n        This fixture uses the `create_spark_df` callable to generate\n        a DataFrame for testing.\n\n        The created DataFrame has three columns: 'id' (an integer),\n        'name' (a string), and 'age' (an integer).\n\n        It has two rows of data with values (1, 'Alice', 25)\n        and (2, 'Bob', 30).\n\n        Parameters\n        ----------\n        spark_session\n            Active SparkSession to use for creating the DataFrame.\n        create_spark_df\n            A callable function to create a DataFrame.\n\n        Returns\n        -------\n        SparkDF\n            A DataFrame with specified schema and data.\n        \"\"\"\n        input_schema = T.StructType(\n            [\n                T.StructField(\"id\", T.IntegerType(), True),\n                T.StructField(\"name\", T.StringType(), True),\n                T.StructField(\"age\", T.IntegerType(), True),\n            ],\n        )\n        df = create_spark_df(\n            [\n                (input_schema),\n                (1, \"Alice\", 25),\n                (2, \"Bob\", 30),\n            ],\n        )\n\n        return df",
        "variables": [
            "spark_session",
            "df",
            "self",
            "create_spark_df",
            "input_schema"
        ],
        "docstring": "Fixture to create a test DataFrame with the help of `create_spark_df`\ncallable.\n\nThis fixture uses the `create_spark_df` callable to generate\na DataFrame for testing.\n\nThe created DataFrame has three columns: 'id' (an integer),\n'name' (a string), and 'age' (an integer).\n\nIt has two rows of data with values (1, 'Alice', 25)\nand (2, 'Bob', 30).\n\nParameters\n----------\nspark_session\n    Active SparkSession to use for creating the DataFrame.\ncreate_spark_df\n    A callable function to create a DataFrame.\n\nReturns\n-------\nSparkDF\n    A DataFrame with specified schema and data."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_insert_df_to_hive_table_with_missing_columns",
        "code_chunk": "def test_insert_df_to_hive_table_with_missing_columns(\n        self,\n        mock_lit,\n        mock_table,\n        mock_with_column,\n        spark_session: SparkSession,\n        test_df: SparkDF,\n    ) -> None:\n        \"\"\"Test that insert_df_to_hive_table correctly inserts data into a Hive\n        table when 'fill_missing_cols' is True.\n        \"\"\"\n        table_name = \"test_table\"\n\n        # Mock the table's columns to include 'address'\n        mock_table.return_value.columns = [\"id\", \"name\", \"age\", \"address\"]\n\n        # Mock the Hive table schema\n        mock_table_schema = T.StructType(\n            [\n                T.StructField(\"id\", T.IntegerType()),\n                T.StructField(\"name\", T.StringType()),\n                T.StructField(\"age\", T.IntegerType()),\n                T.StructField(\n                    \"address\",\n                    T.StringType(),\n                ),  # Include 'address' with StringType\n            ],\n        )\n        mock_table.return_value.schema = mock_table_schema\n\n        # Create a mock of the DataFrame (test_df) that does not contain 'address'\n        test_df_mock = MagicMock()\n        test_df_mock.columns = [\n            \"id\",\n            \"name\",\n            \"age\",\n        ]  # Mock that it doesn't have 'address'\n\n        # Mock `withColumn` behavior - ensure it's being called\n        # with correct column expression\n        mock_with_column.return_value = (\n            test_df_mock  # Simulate that `withColumn` returns `test_df_mock`\n        )\n\n        # Mock the return of the `lit` function to simulate the expression\n        mock_lit.return_value = F.lit(None).cast(\n            T.StringType(),\n        )  # We expect the `lit` to return the null expression\n\n        # Call the function to insert data into the table\n        insert_df_to_hive_table(\n            spark_session,\n            test_df,\n            table_name,\n            overwrite=True,\n            fill_missing_cols=True,\n        )\n\n        # Assert that `lit` was called with `None` to create the null expression\n        mock_lit.assert_called_with(None)\n\n        # Since `lit().cast()` returns the same object,\n        # directly assert the final return value\n        expected_column = F.lit(None).cast(T.StringType())\n\n        # Check if withColumn was called with 'address' and the expected expression\n        # Compare the exact column expression (the expected one, not the mock)\n        mock_with_column.assert_any_call(\"address\", expected_column)",
        "variables": [
            "mock_table",
            "spark_session",
            "test_df",
            "test_df_mock",
            "self",
            "mock_lit",
            "expected_column",
            "mock_with_column",
            "mock_table_schema",
            "table_name"
        ],
        "docstring": "Test that insert_df_to_hive_table correctly inserts data into a Hive\ntable when 'fill_missing_cols' is True."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_insert_df_to_hive_table_without_missing_columns",
        "code_chunk": "def test_insert_df_to_hive_table_without_missing_columns(\n        self,\n        mock_table,\n        spark_session: SparkSession,\n        test_df: SparkDF,\n    ) -> None:\n        \"\"\"Test that insert_df_to_hive_table raises a ValueError when\n        'fill_missing_cols' is False and DataFrame schema doesn't match with the\n        table schema.\n        \"\"\"\n        table_name = \"test_table\"\n        # Mock the table columns\n        mock_table.return_value.columns = [\"id\", \"name\", \"age\", \"address\"]\n        with pytest.raises(ValueError):\n            insert_df_to_hive_table(\n                spark_session,\n                test_df,\n                table_name,\n                fill_missing_cols=False,\n            )",
        "variables": [
            "mock_table",
            "spark_session",
            "test_df",
            "self",
            "table_name"
        ],
        "docstring": "Test that insert_df_to_hive_table raises a ValueError when\n'fill_missing_cols' is False and DataFrame schema doesn't match with the\ntable schema."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_insert_df_to_hive_table_insert_into_existing_table",
        "code_chunk": "def test_insert_df_to_hive_table_insert_into_existing_table(\n        self,\n        mock_table,\n        mock_insert_into,\n        spark_session: SparkSession,\n        test_df: SparkDF,\n    ) -> None:\n        \"\"\"Test that insertInto is called when the table already exists in Hive.\"\"\"\n        table_name = \"existing_table\"\n\n        # Mock the table columns to simulate the table already exists\n        mock_table.return_value.columns = [\"id\", \"name\", \"age\"]\n\n        # Simulate a successful call to `insertInto`\n        mock_insert_into.return_value = None\n\n        # Call the function that triggers insertInto when the table exists\n        insert_df_to_hive_table(\n            spark_session,\n            test_df,\n            table_name,\n        )\n\n        # Assert that insertInto was called with the correct table name\n        mock_insert_into.assert_called_once_with(table_name)",
        "variables": [
            "mock_table",
            "spark_session",
            "test_df",
            "self",
            "mock_insert_into",
            "table_name"
        ],
        "docstring": "Test that insertInto is called when the table already exists in Hive."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_insert_df_to_hive_table_save_as_table_when_table_does_not_exist",
        "code_chunk": "def test_insert_df_to_hive_table_save_as_table_when_table_does_not_exist(\n        self,\n        mock_table,\n        mock_save_as_table,\n        spark_session: SparkSession,\n        test_df: SparkDF,\n    ) -> None:\n        \"\"\"Test that saveAsTable is called when the Hive table does not exist.\"\"\"\n        table_name = \"new_table\"\n\n        # Simulate the table not existing by raising an AnalysisException\n        mock_table.side_effect = AnalysisException(f\"Table {table_name} not found.\")\n\n        # Simulate a successful call to `saveAsTable`\n        mock_save_as_table.return_value = None\n\n        # Call the function that triggers saveAsTable when the table does not exist\n        insert_df_to_hive_table(\n            spark_session,\n            test_df,\n            table_name,\n        )\n\n        # Assert that saveAsTable was called with the correct table name\n        mock_save_as_table.assert_called_once_with(table_name)",
        "variables": [
            "mock_table",
            "mock_save_as_table",
            "spark_session",
            "test_df",
            "self",
            "table_name"
        ],
        "docstring": "Test that saveAsTable is called when the Hive table does not exist."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_insert_df_to_hive_table_with_repartition_data_by",
        "code_chunk": "def test_insert_df_to_hive_table_with_repartition_data_by(\n        self,\n        mock_table,\n        mock_repartition,\n        spark_session: SparkSession,\n        test_df: SparkDF,\n    ) -> None:\n        \"\"\"Test that the DataFrame is repartitioned by a specified column.\"\"\"\n        table_name = \"test_table\"\n        mock_table.return_value.columns = [\"id\", \"name\", \"age\"]\n\n        # Ensure that mock_repartition is set to return the same test_df\n        mock_repartition.return_value = test_df\n\n        # Call the function that triggers repartition\n        insert_df_to_hive_table(\n            spark_session,\n            test_df,\n            table_name,\n            repartition_data_by=\"id\",  # We expect \"id\" column to be used for repartitioning\n            overwrite=True,\n        )\n\n        # Assert that repartition was called with the correct argument (the column name)\n        mock_repartition.assert_called_once_with(\"id\")",
        "variables": [
            "mock_table",
            "spark_session",
            "mock_repartition",
            "test_df",
            "self",
            "table_name"
        ],
        "docstring": "Test that the DataFrame is repartitioned by a specified column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_insert_df_to_hive_table_with_repartition_num_partitions",
        "code_chunk": "def test_insert_df_to_hive_table_with_repartition_num_partitions(\n        self,\n        mock_table,\n        mock_repartition,\n        spark_session: SparkSession,\n        test_df: SparkDF,\n    ) -> None:\n        \"\"\"Test that the DataFrame is repartitioned into a specific number of partitions.\"\"\"\n        table_name = \"test_table\"\n        mock_table.return_value.columns = [\"id\", \"name\", \"age\"]\n\n        # Ensure that mock_repartition is set to return the same test_df\n        mock_repartition.return_value = test_df\n\n        # Call the function that triggers repartition\n        insert_df_to_hive_table(\n            spark_session,\n            test_df,\n            table_name,\n            repartition_data_by=5,  # Expecting 5 partitions\n            overwrite=True,\n        )\n\n        # Assert that repartition was called with the number of partitions (5)\n        mock_repartition.assert_called_once_with(5)",
        "variables": [
            "mock_table",
            "spark_session",
            "mock_repartition",
            "test_df",
            "self",
            "table_name"
        ],
        "docstring": "Test that the DataFrame is repartitioned into a specific number of partitions."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_insert_df_to_hive_table_with_empty_dataframe",
        "code_chunk": "def test_insert_df_to_hive_table_with_empty_dataframe(\n        self,\n        spark_session: SparkSession,\n    ) -> None:\n        \"\"\"Test that an empty DataFrame raises DataframeEmptyError.\"\"\"\n        from rdsa_utils.exceptions import DataframeEmptyError\n\n        table_name = \"test_table\"\n        empty_df = spark_session.createDataFrame(\n            [],\n            schema=\"id INT, name STRING, age INT\",\n        )\n        with pytest.raises(DataframeEmptyError):\n            insert_df_to_hive_table(\n                spark_session,\n                empty_df,\n                table_name,\n            )",
        "variables": [
            "self",
            "table_name",
            "spark_session",
            "empty_df"
        ],
        "docstring": "Test that an empty DataFrame raises DataframeEmptyError."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "mock_spark",
        "code_chunk": "def mock_spark(self):\n        \"\"\"Fixture for mocked SparkSession.\"\"\"\n        return Mock(spec=SparkSession)",
        "variables": [
            "self"
        ],
        "docstring": "Fixture for mocked SparkSession."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "mock_df",
        "code_chunk": "def mock_df(self):\n        \"\"\"Fixture for mocked DataFrame with 'run_id' and 'data' columns.\"\"\"\n        mock_df = Mock(spec=SparkDF)\n        mock_df.columns = [\"run_id\", \"data\"]\n        return mock_df",
        "variables": [
            "self",
            "mock_df"
        ],
        "docstring": "Fixture for mocked DataFrame with 'run_id' and 'data' columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_write_and_read_hive_table_success",
        "code_chunk": "def test_write_and_read_hive_table_success(\n        self,\n        mock_insert,\n        mock_load_and_validate,\n        mock_spark,\n        mock_df,\n    ):\n        \"\"\"Test that write_and_read_hive_table function successfully writes\n        SparkDF to the Hive table and reads it back when all arguments are\n        valid.\n        \"\"\"\n        # Mock the functions\n        mock_insert.return_value = None\n        mock_load_and_validate.return_value = mock_df\n\n        # Call the function\n        result_df = write_and_read_hive_table(\n            mock_spark,\n            mock_df,\n            \"test_table\",\n            \"test_database\",\n            \"test_run\",\n        )\n\n        # Verify the calls\n        mock_insert.assert_called_once_with(\n            mock_spark,\n            mock_df,\n            \"test_database.test_table\",\n            fill_missing_cols=False,\n        )\n        mock_load_and_validate.assert_called_once_with(\n            mock_spark,\n            \"test_database.test_table\",\n            skip_validation=False,\n            err_msg=None,\n            filter_cond=\"run_id = 'test_run'\",\n        )\n\n        # Check the result\n        assert result_df == mock_df",
        "variables": [
            "mock_insert",
            "self",
            "mock_df",
            "mock_spark",
            "mock_load_and_validate",
            "result_df"
        ],
        "docstring": "Test that write_and_read_hive_table function successfully writes\nSparkDF to the Hive table and reads it back when all arguments are\nvalid."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_hive_table_does_not_exist",
        "code_chunk": "def test_hive_table_does_not_exist(self, mock_spark, mock_df):\n        \"\"\"Check exception handling when the Hive table does not exist.\"\"\"\n        mock_spark.catalog.tableExists.return_value = False\n        with pytest.raises(\n            TableNotFoundError,\n            match=\"The specified Hive table test_database.test_table does not exist.\",\n        ):\n            write_and_read_hive_table(\n                mock_spark,\n                mock_df,\n                \"test_table\",\n                \"test_database\",\n                \"test_run\",\n            )",
        "variables": [
            "self",
            "mock_spark",
            "mock_df"
        ],
        "docstring": "Check exception handling when the Hive table does not exist."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_df_missing_filter_column",
        "code_chunk": "def test_df_missing_filter_column(self, mock_spark, mock_df):\n        \"\"\"Check exception handling when the DataFrame is missing the filter\n        column.\n        \"\"\"\n        mock_df.columns = [\"col1\", \"col2\"]\n        with pytest.raises(\n            ColumnNotInDataframeError,\n            match=(\n                \"The provided DataFrame doesn't contain the specified \"\n                \"filter column: run_id\"\n            ),\n        ):\n            write_and_read_hive_table(\n                mock_spark,\n                mock_df,\n                \"test_table\",\n                \"test_database\",\n                \"test_run\",\n            )",
        "variables": [
            "self",
            "mock_spark",
            "mock_df"
        ],
        "docstring": "Check exception handling when the DataFrame is missing the filter\ncolumn."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "mock_df",
        "code_chunk": "def mock_df(self) -> Mock:\n        \"\"\"Fixture for mocked Spark DataFrame.\"\"\"\n        return Mock(spec=SparkDF)",
        "variables": [
            "self"
        ],
        "docstring": "Fixture for mocked Spark DataFrame."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_save_csv_to_hdfs_success",
        "code_chunk": "def test_save_csv_to_hdfs_success(\n        self,\n        mock_file_exists,\n        mock_rename,\n        mock_delete_path,\n        mock_logger,\n        mock_df,\n    ):\n        \"\"\"Test successful saving of DataFrame to HDFS as a single CSV file.\"\"\"\n        mock_file_exists.return_value = False\n        mock_rename.return_value = True\n\n        file_name = \"test_output.csv\"\n        file_path = \"/test/hdfs/path\"\n\n        save_csv_to_hdfs(mock_df, file_name, file_path)\n\n        mock_df.coalesce.assert_called_once_with(1)\n        mock_rename.assert_called_once()\n        mock_delete_path.assert_called_once()\n        assert mock_logger.info.call_count > 0",
        "variables": [
            "mock_rename",
            "self",
            "mock_file_exists",
            "mock_logger",
            "mock_df",
            "file_path",
            "mock_delete_path",
            "file_name"
        ],
        "docstring": "Test successful saving of DataFrame to HDFS as a single CSV file."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_overwriting_existing_file",
        "code_chunk": "def test_overwriting_existing_file(\n        self,\n        mock_logger,\n        mock_delete_path,\n        mock_rename,\n        mock_file_exists,\n        mock_df,\n    ):\n        \"\"\"Ensure the function correctly overwrites an existing file when overwrite=True.\"\"\"\n        file_name = \"should_overwrite.csv\"\n        file_path = \"/test/overwrite/path\"\n\n        # Attempt to save, expecting no errors\n        try:\n            save_csv_to_hdfs(mock_df, file_name, file_path, overwrite=True)\n        except Exception as e:\n            pytest.fail(f\"Function raised an unexpected exception: {e}\")\n\n        mock_rename.assert_called_once()",
        "variables": [
            "mock_rename",
            "self",
            "mock_file_exists",
            "mock_logger",
            "mock_df",
            "file_path",
            "mock_delete_path",
            "file_name",
            "e"
        ],
        "docstring": "Ensure the function correctly overwrites an existing file when overwrite=True."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_save_csv_to_hdfs_file_exists_error",
        "code_chunk": "def test_save_csv_to_hdfs_file_exists_error(\n        self,\n        mock_file_exists,\n        mock_df,\n    ):\n        \"\"\"Test error raised when the target file exists and overwrite is False.\"\"\"\n        mock_file_exists.return_value = True\n\n        file_name = \"test_output.csv\"\n        file_path = \"/test/hdfs/path\"\n\n        with pytest.raises(IOError):\n            save_csv_to_hdfs(mock_df, file_name, file_path, overwrite=False)\n\n        mock_file_exists.assert_called_once_with(\n            f\"{file_path.rstrip('/')}/{file_name}\",\n        )",
        "variables": [
            "self",
            "mock_file_exists",
            "mock_df",
            "file_name",
            "file_path"
        ],
        "docstring": "Test error raised when the target file exists and overwrite is False."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_save_csv_to_hdfs_invalid_file_name",
        "code_chunk": "def test_save_csv_to_hdfs_invalid_file_name(self, mock_df):\n        \"\"\"Test error raised when file name does not end with '.csv'.\"\"\"\n        file_name = \"invalid_file_name\"\n        file_path = \"/test/hdfs/path\"\n\n        with pytest.raises(ValueError):\n            save_csv_to_hdfs(mock_df, file_name, file_path)",
        "variables": [
            "file_path",
            "self",
            "file_name",
            "mock_df"
        ],
        "docstring": "Test error raised when file name does not end with '.csv'."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_file_path_schemes",
        "code_chunk": "def test_file_path_schemes(\n        self,\n        mock_logger,\n        mock_delete_path,\n        mock_rename,\n        mock_file_exists,\n        mock_df,\n        file_path,\n        expected_call,\n    ):\n        \"\"\"Test the function with different file path schemes, including S3 and HDFS.\"\"\"\n        file_name = \"should_write.csv\"\n\n        try:\n            save_csv_to_hdfs(mock_df, file_name, file_path)\n        except Exception as e:\n            pytest.fail(\n                f\"Function raised an unexpected exception with file path '{file_path}': {e}\",\n            )\n\n        # We're focusing on path handling, so just ensure it gets to the rename call\n        mock_rename.assert_called_once()",
        "variables": [
            "mock_rename",
            "self",
            "e",
            "mock_file_exists",
            "mock_logger",
            "mock_df",
            "mock_delete_path",
            "file_name",
            "file_path",
            "expected_call"
        ],
        "docstring": "Test the function with different file path schemes, including S3 and HDFS."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "_aws_credentials",
        "code_chunk": "def _aws_credentials(self):\n        \"\"\"Mock AWS Credentials for moto.\"\"\"\n        boto3.setup_default_session(\n            aws_access_key_id=\"testing\",\n            aws_secret_access_key=\"testing\",\n            aws_session_token=\"testing\",\n        )",
        "variables": [
            "self"
        ],
        "docstring": "Mock AWS Credentials for moto."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "s3_client",
        "code_chunk": "def s3_client(self, _aws_credentials):\n        \"\"\"Provide a mocked AWS S3 client for testing\n        using moto with temporary credentials.\n        \"\"\"\n        with mock_aws():\n            client = boto3.client(\"s3\", region_name=\"us-east-1\")\n            client.create_bucket(Bucket=\"test-bucket\")\n            yield client",
        "variables": [
            "client",
            "self",
            "_aws_credentials"
        ],
        "docstring": "Provide a mocked AWS S3 client for testing\nusing moto with temporary credentials."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "dummy_dataframe",
        "code_chunk": "def dummy_dataframe(self, spark_session):\n        \"\"\"Create a dummy PySpark DataFrame for testing.\"\"\"\n        data = [(\"John\", 1), (\"Jane\", 2), (\"Bob\", 3)]\n        columns = [\"name\", \"id\"]\n        return spark_session.createDataFrame(data, columns)",
        "variables": [
            "self",
            "spark_session",
            "data",
            "columns"
        ],
        "docstring": "Create a dummy PySpark DataFrame for testing."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_save_csv_to_s3",
        "code_chunk": "def test_save_csv_to_s3(\n        self,\n        mock_uuid,\n        mock_file_exists,\n        mock_delete_folder,\n        mock_copy_file,\n        mock_list_files,\n        mock_write,\n        dummy_dataframe,\n        s3_client,\n    ):\n        \"\"\"Test saving a PySpark DataFrame to S3 as a CSV file.\"\"\"\n        bucket_name = \"test-bucket\"\n        file_name = \"data_output.csv\"\n        file_path = \"data_folder/\"\n\n        # Mock UUID to return a fixed value\n        mock_uuid.return_value.hex = \"1234\"\n\n        # Mock the relevant methods\n        mock_write.return_value.csv.return_value = None\n        mock_list_files.return_value = [\n            f'{file_path.rstrip(\"/\")}/temp_1234_data_output.csv/part-00000.csv',\n        ]\n        mock_delete_folder.return_value = None\n        mock_file_exists.side_effect = lambda client, bucket, key: (\n            False if key == f\"{file_path.rstrip('/')}/{file_name}\" else False\n        )\n\n        def copy_file_side_effect(\n            s3_client,\n            src_bucket_name,\n            src_key,\n            dest_bucket_name,\n            dest_key,\n            overwrite,\n        ):\n            # Simulate copying by creating an object in the destination\n            copy_source = {\"Bucket\": src_bucket_name, \"Key\": src_key}\n            s3_client.copy_object(\n                CopySource=copy_source,\n                Bucket=dest_bucket_name,\n                Key=dest_key,\n            )\n            return True\n\n        mock_copy_file.side_effect = copy_file_side_effect\n\n        # Create the temporary file in the bucket to simulate the write operation\n        temp_key = f'{file_path.rstrip(\"/\")}/temp_1234_data_output.csv/part-00000.csv'\n        s3_client.put_object(Bucket=bucket_name, Key=temp_key, Body=\"data\")\n\n        # Call the function\n        save_csv_to_s3(\n            df=dummy_dataframe,\n            bucket_name=bucket_name,\n            file_name=file_name,\n            file_path=file_path,\n            s3_client=s3_client,\n            overwrite=True,\n        )\n\n        # Check if the file is saved using boto3\n        destination_path = f\"{file_path.rstrip('/')}/{file_name}\"\n        response = s3_client.list_objects_v2(\n            Bucket=bucket_name,\n            Prefix=file_path,\n        )\n        keys = [obj[\"Key\"] for obj in response.get(\"Contents\", [])]\n        assert destination_path in keys",
        "variables": [
            "keys",
            "temp_key",
            "self",
            "mock_uuid",
            "mock_list_files",
            "mock_file_exists",
            "mock_write",
            "obj",
            "file_path",
            "destination_path",
            "response",
            "mock_delete_folder",
            "mock_copy_file",
            "copy_source",
            "file_name",
            "bucket_name",
            "dummy_dataframe",
            "s3_client"
        ],
        "docstring": "Test saving a PySpark DataFrame to S3 as a CSV file."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_save_csv_to_s3_overwrite_false",
        "code_chunk": "def test_save_csv_to_s3_overwrite_false(\n        self,\n        mock_uuid,\n        mock_file_exists,\n        mock_delete_folder,\n        mock_copy_file,\n        mock_list_files,\n        mock_write,\n        dummy_dataframe,\n        s3_client,\n    ):\n        \"\"\"Test saving a PySpark DataFrame to S3 with overwrite set to False.\"\"\"\n        bucket_name = \"test-bucket\"\n        file_name = \"data_output.csv\"\n        file_path = \"data_folder/\"\n\n        # Mock UUID to return a fixed value\n        mock_uuid.return_value.hex = \"1234\"\n\n        # Mock the relevant methods\n        mock_write.return_value.csv.return_value = None\n        mock_list_files.return_value = [\n            f'{file_path.rstrip(\"/\")}/temp_1234_data_output.csv/part-00000.csv',\n        ]\n        mock_delete_folder.return_value = None\n\n        def copy_file_side_effect(\n            s3_client,\n            src_bucket_name,\n            src_key,\n            dest_bucket_name,\n            dest_key,\n            overwrite,\n        ):\n            # Simulate copying by creating an object in the destination\n            copy_source = {\"Bucket\": src_bucket_name, \"Key\": src_key}\n            s3_client.copy_object(\n                CopySource=copy_source,\n                Bucket=dest_bucket_name,\n                Key=dest_key,\n            )\n            return True\n\n        mock_copy_file.side_effect = copy_file_side_effect\n\n        # Create the temporary file in the bucket to simulate the write operation\n        temp_key = f'{file_path.rstrip(\"/\")}/temp_1234_data_output.csv/part-00000.csv'\n        s3_client.put_object(Bucket=bucket_name, Key=temp_key, Body=\"data\")\n\n        # Set up file_exists to return False initially, then True\n        mock_file_exists.side_effect = lambda client, bucket, key: (\n            True if key == f\"{file_path.rstrip('/')}/{file_name}\" else False\n        )\n\n        # Save the DataFrame once\n        save_csv_to_s3(\n            df=dummy_dataframe,\n            bucket_name=bucket_name,\n            file_name=file_name,\n            file_path=file_path,\n            s3_client=s3_client,\n            overwrite=True,\n        )\n\n        # Try to save the DataFrame again with overwrite set to False,\n        # which should raise IOError\n        with pytest.raises(IOError):\n            save_csv_to_s3(\n                df=dummy_dataframe,\n                bucket_name=bucket_name,\n                file_name=file_name,\n                file_path=file_path,\n                s3_client=s3_client,\n                overwrite=False,\n            )",
        "variables": [
            "temp_key",
            "self",
            "mock_uuid",
            "mock_list_files",
            "mock_file_exists",
            "mock_write",
            "file_path",
            "mock_delete_folder",
            "mock_copy_file",
            "copy_source",
            "file_name",
            "bucket_name",
            "dummy_dataframe",
            "s3_client"
        ],
        "docstring": "Test saving a PySpark DataFrame to S3 with overwrite set to False."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "test_save_csv_to_s3_invalid_extension",
        "code_chunk": "def test_save_csv_to_s3_invalid_extension(\n        self,\n        mock_write,\n        dummy_dataframe,\n        s3_client,\n    ):\n        \"\"\"Test saving a PySpark DataFrame to S3 with an invalid file extension.\"\"\"\n        bucket_name = \"test-bucket\"\n        file_name = \"data_output.txt\"\n        file_path = \"data_folder/\"\n\n        mock_write.return_value.csv.return_value = None\n\n        with pytest.raises(ValueError):\n            save_csv_to_s3(\n                df=dummy_dataframe,\n                bucket_name=bucket_name,\n                file_name=file_name,\n                file_path=file_path,\n                s3_client=s3_client,\n                overwrite=True,\n            )",
        "variables": [
            "self",
            "mock_write",
            "file_path",
            "file_name",
            "bucket_name",
            "dummy_dataframe",
            "s3_client"
        ],
        "docstring": "Test saving a PySpark DataFrame to S3 with an invalid file extension."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "copy_file_side_effect",
        "code_chunk": "def copy_file_side_effect(\n            s3_client,\n            src_bucket_name,\n            src_key,\n            dest_bucket_name,\n            dest_key,\n            overwrite,\n        ):\n            # Simulate copying by creating an object in the destination\n            copy_source = {\"Bucket\": src_bucket_name, \"Key\": src_key}\n            s3_client.copy_object(\n                CopySource=copy_source,\n                Bucket=dest_bucket_name,\n                Key=dest_key,\n            )\n            return True",
        "variables": [
            "src_bucket_name",
            "dest_bucket_name",
            "src_key",
            "dest_key",
            "overwrite",
            "copy_source",
            "s3_client"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_cdsw_output.py",
        "function_name": "copy_file_side_effect",
        "code_chunk": "def copy_file_side_effect(\n            s3_client,\n            src_bucket_name,\n            src_key,\n            dest_bucket_name,\n            dest_key,\n            overwrite,\n        ):\n            # Simulate copying by creating an object in the destination\n            copy_source = {\"Bucket\": src_bucket_name, \"Key\": src_key}\n            s3_client.copy_object(\n                CopySource=copy_source,\n                Bucket=dest_bucket_name,\n                Key=dest_key,\n            )\n            return True",
        "variables": [
            "src_bucket_name",
            "dest_bucket_name",
            "src_key",
            "dest_key",
            "overwrite",
            "copy_source",
            "s3_client"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_write_entry_non_empty",
        "code_chunk": "def test_write_entry_non_empty(self, mocker):\n        \"\"\"Test that the function successfully writes data to the specified table.\"\"\"\n        # Mock DataFrame and write method\n        mock_df = mocker.Mock(spec=DataFrame)\n        mock_df.write.insertInto.return_value = None\n\n        # Call function with mock DataFrame and table name\n        _write_entry(mock_df, \"test_table\")\n\n        # Assert that DataFrame write method was called with correct arguments\n        mock_df.write.insertInto.assert_called_once_with(\"test_table\")",
        "variables": [
            "self",
            "mocker",
            "mock_df"
        ],
        "docstring": "Test that the function successfully writes data to the specified table."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_write_entry_edge_cases",
        "code_chunk": "def test_write_entry_edge_cases(self, mocker):\n        \"\"\"Test for no exceptions with empty entry_df or log_table as empty string.\"\"\"\n        # Mock empty DataFrame and write method\n        mock_df = mocker.Mock(spec=DataFrame)\n        mock_df.write.insertInto.return_value = None\n\n        # Call function with empty DataFrame and empty table name\n        _write_entry(mock_df, \"\")\n\n        # Assert that DataFrame write method was called once with\n        # empty string as argument\n        mock_df.write.insertInto.assert_called_once_with(\"\")",
        "variables": [
            "self",
            "mocker",
            "mock_df"
        ],
        "docstring": "Test for no exceptions with empty entry_df or log_table as empty string."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_create_runlog_table_default_tablename",
        "code_chunk": "def test_create_runlog_table_default_tablename(self, mocker):\n        \"\"\"Test that the function creates a runlog table with default tablename.\"\"\"\n        # Mock SparkSession\n        mock_spark = mocker.Mock()\n\n        # Call function with default tablename\n        create_runlog_table(mock_spark, \"test_db\")\n\n        # Assert that the main table and _reserved_ids table were created\n        # with correct names\n        mock_spark.sql.assert_any_call(\n            \"\"\"\n        CREATE TABLE IF NOT EXISTS test_db.pipeline_runlog (\n            run_id int,\n            desc string,\n            user string,\n            datetime timestamp,\n            pipeline_name string,\n            pipeline_version string,\n            config string\n        )\n        STORED AS parquet\n    \"\"\",\n        )\n        mock_spark.sql.assert_any_call(\n            \"\"\"\n        CREATE TABLE IF NOT EXISTS test_db.pipeline_runlog_reserved_ids (\n            run_id int,\n            reserved_date timestamp\n        )\n        STORED AS parquet\n    \"\"\",\n        )",
        "variables": [
            "self",
            "mocker",
            "mock_spark"
        ],
        "docstring": "Test that the function creates a runlog table with default tablename."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_create_runlog_table_custom_tablename",
        "code_chunk": "def test_create_runlog_table_custom_tablename(self, mocker):\n        \"\"\"Test that the function creates a runlog table with a custom tablename.\"\"\"\n        # Mock SparkSession\n        mock_spark = mocker.Mock()\n\n        # Call function with custom tablename\n        create_runlog_table(mock_spark, \"test_db\", \"custom_table\")\n\n        # Assert that the main table and _reserved_ids table were created\n        # with correct names\n        mock_spark.sql.assert_any_call(\n            \"\"\"\n        CREATE TABLE IF NOT EXISTS test_db.custom_table (\n            run_id int,\n            desc string,\n            user string,\n            datetime timestamp,\n            pipeline_name string,\n            pipeline_version string,\n            config string\n        )\n        STORED AS parquet\n    \"\"\",\n        )\n        mock_spark.sql.assert_any_call(\n            \"\"\"\n        CREATE TABLE IF NOT EXISTS test_db.custom_table_reserved_ids (\n            run_id int,\n            reserved_date timestamp\n        )\n        STORED AS parquet\n    \"\"\",\n        )",
        "variables": [
            "self",
            "mocker",
            "mock_spark"
        ],
        "docstring": "Test that the function creates a runlog table with a custom tablename."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_reserve_id_non_empty",
        "code_chunk": "def test_reserve_id_non_empty(self, mocker):\n        \"\"\"Tests successful creation and storage of a new ID based on the last run ID.\n\n        Checks if the function can correctly retrieve the last run ID from\n        the reserved IDs table, increment it to generate a new ID, and then\n        record the new ID along with the current timestamp in the reserved IDs table.\n        \"\"\"\n        # Mock SparkSession\n        spark_mock = mocker.Mock()\n        spark_mock.read.table.return_value.select.return_value.first.return_value = (1,)\n        spark_mock.createDataFrame.return_value = mocker.Mock()\n\n        # Mock _write_entry function\n        mock_write_entry = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog._write_entry\",\n        )\n\n        # Mock pyspark.sql.functions.max\n        mocker.patch(\"pyspark.sql.functions.max\", return_value=mocker.Mock())\n\n        # Call function\n        result = reserve_id(spark_mock)\n\n        # Assert result\n        assert result == 2\n\n        # Assert SparkSession methods were called correctly\n        spark_mock.read.table.assert_called_once_with(\"pipeline_runlog_reserved_ids\")\n        spark_mock.createDataFrame.assert_called_once_with(\n            [(2, mocker.ANY)],\n            \"run_id INT, reserved_date TIMESTAMP\",\n        )\n        mock_write_entry.assert_called_once_with(\n            mocker.ANY,\n            \"pipeline_runlog_reserved_ids\",\n        )",
        "variables": [
            "spark_mock",
            "self",
            "mocker",
            "result",
            "mock_write_entry"
        ],
        "docstring": "Tests successful creation and storage of a new ID based on the last run ID.\n\nChecks if the function can correctly retrieve the last run ID from\nthe reserved IDs table, increment it to generate a new ID, and then\nrecord the new ID along with the current timestamp in the reserved IDs table."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_reserve_id_edge_case",
        "code_chunk": "def test_reserve_id_edge_case(self, mocker):\n        \"\"\"Tests handling of edge cases with the reserved IDs table.\n\n        Ensures the function gracefully manages scenarios where the\n        reserved IDs table is empty, nonexistent, lacks a \"run_id\" column,\n        or lacks a \"reserved_date\" column.\n        \"\"\"\n        # Mock SparkSession\n        spark_mock = mocker.Mock()\n        spark_mock.read.table.return_value.select.return_value.first.return_value = [\n            None,\n        ]\n        spark_mock.createDataFrame.return_value = mocker.Mock()\n\n        # Mock _write_entry function\n        mock_write_entry = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog._write_entry\",\n        )\n\n        # Mock pyspark.sql.functions.max\n        mocker.patch(\"pyspark.sql.functions.max\", return_value=mocker.Mock())\n\n        # Call function\n        result = reserve_id(spark_mock)\n\n        # Assert result\n        assert result == 1\n\n        # Assert SparkSession methods were called correctly\n        spark_mock.read.table.assert_called_once_with(\"pipeline_runlog_reserved_ids\")\n        spark_mock.createDataFrame.assert_called_once_with(\n            [(1, mocker.ANY)],\n            \"run_id INT, reserved_date TIMESTAMP\",\n        )\n        mock_write_entry.assert_called_once_with(\n            mocker.ANY,\n            \"pipeline_runlog_reserved_ids\",\n        )",
        "variables": [
            "spark_mock",
            "self",
            "mocker",
            "result",
            "mock_write_entry"
        ],
        "docstring": "Tests handling of edge cases with the reserved IDs table.\n\nEnsures the function gracefully manages scenarios where the\nreserved IDs table is empty, nonexistent, lacks a \"run_id\" column,\nor lacks a \"reserved_date\" column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_get_run_ids_returns_non_empty",
        "code_chunk": "def test_get_run_ids_returns_non_empty(self, mocker):\n        \"\"\"Tests correct retrieval of recent run IDs for specific and all pipelines.\"\"\"\n        # Mock SparkSession and log table\n        spark_mock = mocker.Mock()\n        log_table = \"test_log_table\"\n\n        # Create test data\n        test_data = [\n            (1, \"pipeline1\", \"2022-01-01 00:00:00\"),\n            (2, \"pipeline1\", \"2022-01-06 00:00:00\"),\n            (3, \"pipeline2\", \"2022-01-03 00:00:00\"),\n            (4, \"pipeline2\", \"2022-01-04 00:00:00\"),\n            (5, \"pipeline2\", \"2022-01-05 00:00:00\"),\n        ]\n\n        # Sort the test data by datetime in descending order\n        test_data_sorted = sorted(test_data, key=lambda x: x[2], reverse=True)\n\n        # Mock DataFrame methods\n        test_df = mocker.Mock()\n        test_df.filter.return_value = test_df\n        test_df.orderBy.return_value = test_df\n        test_df.select.return_value = test_df\n        test_df.limit.return_value = test_df\n\n        # Mock SparkSession methods\n        spark_mock.read.table.return_value = test_df\n\n        # Call function for all pipelines\n        test_df.collect.return_value = [(x[0],) for x in test_data_sorted[:3]]\n        result_all = _get_run_ids(spark_mock, 3, log_table=log_table)\n\n        # Assert results; the top 3 run_ids based on datetime\n        assert result_all == [2, 5, 4]\n\n        # Call function for specific pipeline\n        test_df.collect.return_value = [\n            (x[0],) for x in test_data_sorted if x[1] == \"pipeline2\"\n        ][:2]\n        result_pipeline = _get_run_ids(\n            spark_mock,\n            2,\n            pipeline=\"pipeline2\",\n            log_table=log_table,\n        )\n\n        # Assert results; The top 2 run_ids for \"pipeline2\" based on datetime\n        assert result_pipeline == [\n            5,\n            4,\n        ]\n\n        # Assert DataFrame methods were called correctly\n        test_df.orderBy.assert_called_with(\"datetime\", ascending=False)\n        test_df.select.assert_called_with(\"run_id\")\n        test_df.filter.assert_called_with(test_df.pipeline_name == \"pipeline2\")\n        test_df.limit.assert_called_with(2)",
        "variables": [
            "result_all",
            "test_df",
            "spark_mock",
            "self",
            "mocker",
            "log_table",
            "result_pipeline",
            "x",
            "test_data",
            "test_data_sorted"
        ],
        "docstring": "Tests correct retrieval of recent run IDs for specific and all pipelines."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_get_run_ids_empty_table",
        "code_chunk": "def test_get_run_ids_empty_table(self, mocker):\n        \"\"\"Test the function for returning an empty list when the log table is empty.\"\"\"\n        # Mock SparkSession and log table\n        spark_mock = mocker.Mock()\n        log_table = \"test_log_table\"\n\n        # Mock DataFrame methods for an empty DataFrame\n        test_df = mocker.Mock()\n        test_df.filter.return_value = test_df\n        test_df.orderBy.return_value = test_df\n        test_df.select.return_value = test_df\n        test_df.limit.return_value = test_df\n\n        # Mock SparkSession methods\n        spark_mock.read.table.return_value = test_df\n\n        # Set collect() method to return an empty list\n        test_df.collect.return_value = []\n\n        # Call function for empty log table\n        result_empty = _get_run_ids(spark_mock, 3, log_table=log_table)\n\n        # Assert results\n        assert result_empty == []\n\n        # Assert DataFrame methods were called correctly\n        test_df.orderBy.assert_called_with(\"datetime\", ascending=False)\n        test_df.select.assert_called_with(\"run_id\")\n        test_df.limit.assert_called_with(3)",
        "variables": [
            "test_df",
            "spark_mock",
            "self",
            "mocker",
            "log_table",
            "result_empty"
        ],
        "docstring": "Test the function for returning an empty list when the log table is empty."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_get_last_run_id_general_pipeline_non_empty",
        "code_chunk": "def test_get_last_run_id_general_pipeline_non_empty(self, mocker):\n        \"\"\"Test retrieving the last run ID for a general pipeline with at least one log entry.\"\"\"\n        # Mock SparkSession and _get_run_ids function\n        spark_mock = mocker.Mock()\n\n        # Patch _get_run_ids function and return a Mock object\n        get_run_ids_mock = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog._get_run_ids\",\n            return_value=[3, 2, 1],\n        )\n\n        # Call function\n        result = get_last_run_id(spark_mock)\n\n        # Assert result\n        assert result == 3\n\n        # Assert _get_run_ids was called correctly\n        get_run_ids_mock.assert_called_once_with(spark_mock, 1, None, \"pipeline_runlog\")",
        "variables": [
            "spark_mock",
            "self",
            "mocker",
            "result",
            "get_run_ids_mock"
        ],
        "docstring": "Test retrieving the last run ID for a general pipeline with at least one log entry."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_get_last_run_id_specific_pipeline_empty",
        "code_chunk": "def test_get_last_run_id_specific_pipeline_empty(self, mocker):\n        \"\"\"Test retrieving the last run ID for a specific pipeline with no log entries.\"\"\"\n        # Mock SparkSession and _get_run_ids function\n        spark_mock = mocker.Mock()\n\n        # Patch _get_run_ids function and return a Mock object\n        get_run_ids_mock = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog._get_run_ids\",\n            return_value=[],\n        )\n\n        # Call function with specific pipeline and empty log table\n        result = get_last_run_id(spark_mock, pipeline=\"test_pipeline\")\n\n        # Assert result is None\n        assert result is None\n\n        # Assert _get_run_ids was called correctly\n        get_run_ids_mock.assert_called_once_with(\n            spark_mock,\n            1,\n            \"test_pipeline\",\n            \"pipeline_runlog\",\n        )",
        "variables": [
            "spark_mock",
            "self",
            "mocker",
            "result",
            "get_run_ids_mock"
        ],
        "docstring": "Test retrieving the last run ID for a specific pipeline with no log entries."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_penultimate_run_id_non_empty",
        "code_chunk": "def test_penultimate_run_id_non_empty(self, mocker):\n        \"\"\"Test retrieving the penultimate run ID.\n\n        Tests the functionality for retrieving the penultimate run ID for a\n        pipeline with at least two entries in the log table.\n        \"\"\"\n        # Mock SparkSession and _get_run_ids function\n        spark_mock = mocker.Mock()\n\n        # Patch _get_run_ids function and return a Mock object\n        get_run_ids_mock = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog._get_run_ids\",\n            return_value=[3, 2, 1],\n        )\n\n        # Call function\n        result = get_penultimate_run_id(spark_mock, pipeline=\"test_pipeline\")\n\n        # Assert result\n        assert result == 2\n\n        # Assert _get_run_ids was called correctly\n        get_run_ids_mock.assert_called_once_with(\n            spark_mock,\n            2,\n            \"test_pipeline\",\n            \"pipeline_runlog\",\n        )",
        "variables": [
            "spark_mock",
            "self",
            "mocker",
            "result",
            "get_run_ids_mock"
        ],
        "docstring": "Test retrieving the penultimate run ID.\n\nTests the functionality for retrieving the penultimate run ID for a\npipeline with at least two entries in the log table."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_penultimate_run_id_edge_cases",
        "code_chunk": "def test_penultimate_run_id_edge_cases(self, mocker):\n        \"\"\"Test retrieving the penultimate run ID in various edge cases.\n\n        This test covers the functionality for retrieving the penultimate run ID in\n        the following scenarios:\n        - For a pipeline with only one entry in the log table\n        - For a pipeline with no entries in the log table\n        - For the general log table with only one entry\n        - For the general log table with no entries.\n        \"\"\"\n        # Mock SparkSession and _get_run_ids function\n        spark_mock = mocker.Mock()\n\n        ## Test Case 1\n\n        # Patch _get_run_ids function and return a Mock object\n        get_run_ids_mock = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog._get_run_ids\",\n            return_value=[1],\n        )\n\n        # Call function with specific pipeline and non-empty log table\n        result = get_penultimate_run_id(spark_mock, pipeline=\"test_pipeline\")\n\n        # Assert result is None\n        assert result is None\n\n        # Assert _get_run_ids was called correctly\n        get_run_ids_mock.assert_called_once_with(\n            spark_mock,\n            2,\n            \"test_pipeline\",\n            \"pipeline_runlog\",\n        )\n\n        ## Test Case 2\n\n        # Patch _get_run_ids function and return a Mock object\n        get_run_ids_mock = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog._get_run_ids\",\n            return_value=[],\n        )\n\n        # Call function with specific pipeline and empty log table\n        result = get_penultimate_run_id(spark_mock, pipeline=\"test_pipeline\")\n\n        # Assert result is None\n        assert result is None\n\n        # Assert _get_run_ids was called correctly\n        get_run_ids_mock.assert_called_once_with(\n            spark_mock,\n            2,\n            \"test_pipeline\",\n            \"pipeline_runlog\",\n        )",
        "variables": [
            "spark_mock",
            "self",
            "mocker",
            "result",
            "get_run_ids_mock"
        ],
        "docstring": "Test retrieving the penultimate run ID in various edge cases.\n\nThis test covers the functionality for retrieving the penultimate run ID in\nthe following scenarios:\n- For a pipeline with only one entry in the log table\n- For a pipeline with no entries in the log table\n- For the general log table with only one entry\n- For the general log table with no entries."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_create_runlog_entry",
        "code_chunk": "def test_create_runlog_entry(self, mocker):\n        \"\"\"Tests function for returning a DataFrame with log entry for valid inputs.\"\"\"\n        # Mock SparkSession\n        spark_mock = mocker.Mock()\n\n        # Set up test data\n        run_id = 1\n        desc = \"test description\"\n        version = \"1.0\"\n        config = {\"param1\": \"value1\", \"param2\": \"value2\"}\n        pipeline = \"test_pipeline\"\n\n        # Mock createDataFrame function\n        mock_df = mocker.Mock()\n        mock_df.columns = [\n            \"run_id\",\n            \"desc\",\n            \"user\",\n            \"datetime\",\n            \"pipeline_name\",\n            \"pipeline_version\",\n            \"config\",\n        ]\n        spark_mock.createDataFrame.return_value = mock_df\n\n        # Call function\n        result = create_runlog_entry(\n            spark_mock,\n            run_id,\n            desc,\n            version,\n            config,\n            pipeline,\n        )\n\n        # Assert result is a DataFrame with the correct columns and values\n        assert result == mock_df\n        assert result.columns == [\n            \"run_id\",\n            \"desc\",\n            \"user\",\n            \"datetime\",\n            \"pipeline_name\",\n            \"pipeline_version\",\n            \"config\",\n        ]",
        "variables": [
            "spark_mock",
            "self",
            "mocker",
            "result",
            "mock_df",
            "config",
            "desc",
            "pipeline",
            "run_id",
            "version"
        ],
        "docstring": "Tests function for returning a DataFrame with log entry for valid inputs."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_create_runlog_entry_edge_cases",
        "code_chunk": "def test_create_runlog_entry_edge_cases(self, mocker):\n        \"\"\"Tests the function for raising an error with invalid inputs.\"\"\"\n        # Mock SparkSession\n        spark_mock = mocker.Mock()\n\n        # Set up test data with invalid config object\n        run_id = 1\n        desc = \"test description\"\n        version = \"1.0\"\n        config = object()\n        pipeline = \"test_pipeline\"\n\n        # Call function and assert it raises a ValueError\n        with pytest.raises(ValueError):\n            create_runlog_entry(spark_mock, run_id, desc, version, config, pipeline)",
        "variables": [
            "spark_mock",
            "self",
            "mocker",
            "config",
            "desc",
            "pipeline",
            "run_id",
            "version"
        ],
        "docstring": "Tests the function for raising an error with invalid inputs."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_add_runlog_entry",
        "code_chunk": "def test_add_runlog_entry(self, mocker):\n        \"\"\"Test adding an entry to the runlog with a newly reserved run_id.\"\"\"\n        # Mock SparkSession\n        spark_mock = mocker.Mock()\n\n        # Set up test data\n        desc = \"test description\"\n        version = \"1.0\"\n        config = {\"param1\": \"value1\", \"param2\": \"value2\"}\n        pipeline = \"test_pipeline\"\n        log_table = \"test_log_table\"\n\n        # Mock reserve_id, create_runlog_entry, _write_entry\n        reserve_id_mock = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog.reserve_id\",\n            return_value=1,\n        )\n        entry_mock = mocker.Mock()\n        create_runlog_entry_mock = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog.create_runlog_entry\",\n            return_value=entry_mock,\n        )\n        _write_entry_mock = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog._write_entry\",\n        )\n\n        # Call function\n        result = add_runlog_entry(\n            spark_mock,\n            desc,\n            version,\n            config,\n            pipeline,\n            log_table,\n        )\n\n        # Assert functions were called correctly\n        reserve_id_mock.assert_called_once_with(spark_mock, log_table)\n        create_runlog_entry_mock.assert_called_once_with(\n            spark_mock,\n            1,\n            desc,\n            version,\n            config,\n            pipeline,\n        )\n        _write_entry_mock.assert_called_once_with(entry_mock, log_table)\n\n        # Assert function returned correct value\n        assert result == entry_mock",
        "variables": [
            "create_runlog_entry_mock",
            "spark_mock",
            "self",
            "mocker",
            "log_table",
            "entry_mock",
            "result",
            "config",
            "reserve_id_mock",
            "desc",
            "_write_entry_mock",
            "pipeline",
            "version"
        ],
        "docstring": "Test adding an entry to the runlog with a newly reserved run_id."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_add_runlog_entry_specified_id",
        "code_chunk": "def test_add_runlog_entry_specified_id(self, mocker):\n        \"\"\"Tests adding an entry to the runlog with a specified run_id.\"\"\"\n        # Mock SparkSession\n        spark_mock = mocker.Mock()\n\n        # Set up test data\n        run_id = 2\n        desc = \"test description\"\n        version = \"1.0\"\n        config = {\"param1\": \"value1\", \"param2\": \"value2\"}\n        pipeline = \"test_pipeline\"\n        log_table = \"test_log_table\"\n\n        # Mock create_runlog_entry, _write_entry\n        entry_mock = mocker.Mock()\n        create_runlog_entry_mock = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog.create_runlog_entry\",\n            return_value=entry_mock,\n        )\n        _write_entry_mock = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog._write_entry\",\n        )\n\n        # Call function\n        result = add_runlog_entry(\n            spark_mock,\n            desc,\n            version,\n            config,\n            pipeline,\n            log_table,\n            run_id,\n        )\n\n        # Assert functions were called correctly\n        create_runlog_entry_mock.assert_called_once_with(\n            spark_mock,\n            run_id,\n            desc,\n            version,\n            config,\n            pipeline,\n        )\n        _write_entry_mock.assert_called_once_with(entry_mock, log_table)\n\n        # Assert function returned correct value\n        assert result == entry_mock",
        "variables": [
            "create_runlog_entry_mock",
            "spark_mock",
            "self",
            "mocker",
            "log_table",
            "entry_mock",
            "result",
            "config",
            "desc",
            "_write_entry_mock",
            "pipeline",
            "run_id",
            "version"
        ],
        "docstring": "Tests adding an entry to the runlog with a specified run_id."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_write_runlog_file",
        "code_chunk": "def test_write_runlog_file(self, mocker):\n        \"\"\"Tests successful creation of a text file in HDFS with runlog metadata.\"\"\"\n        # Mock SparkSession\n        spark_mock = mocker.Mock()\n\n        # Set up test data\n        runlog_table = \"test_log_table\"\n        runlog_id = 1\n        path = \"/test/path\"\n\n        # Mock _parse_runlog_as_string and create_txt_from_string\n        parse_mock = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog._parse_runlog_as_string\",\n        )\n        parse_mock.return_value = \"test metadata\"\n        create_mock = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog.create_txt_from_string\",\n        )\n\n        # Call function\n        write_runlog_file(spark_mock, runlog_table, runlog_id, path)\n\n        # Assert functions were called correctly\n        parse_mock.assert_called_once_with(spark_mock, runlog_table, runlog_id)\n        create_mock.assert_called_once_with(path, \"test metadata\")",
        "variables": [
            "runlog_table",
            "create_mock",
            "spark_mock",
            "self",
            "mocker",
            "path",
            "runlog_id",
            "parse_mock"
        ],
        "docstring": "Tests successful creation of a text file in HDFS with runlog metadata."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\cdp\\io\\test_pipeline_runlog.py",
        "function_name": "test_write_runlog_file_edge_case",
        "code_chunk": "def test_write_runlog_file_edge_case(self, mocker):\n        \"\"\"Tests FileNotFoundError when the specified path is not found.\"\"\"\n        # Mock SparkSession\n        spark_mock = mocker.Mock()\n\n        # Set up test data\n        runlog_table = \"test_log_table\"\n        runlog_id = 1\n        path = \"/nonexistent/path\"\n\n        # Mock _parse_runlog_as_string\n        parse_mock = mocker.patch(\n            \"rdsa_utils.cdp.io.pipeline_runlog._parse_runlog_as_string\",\n        )\n        parse_mock.return_value = \"test metadata\"\n\n        # Call function and assert FileNotFoundError is raised\n        with pytest.raises(FileNotFoundError):\n            write_runlog_file(spark_mock, runlog_table, runlog_id, path)",
        "variables": [
            "runlog_table",
            "spark_mock",
            "self",
            "mocker",
            "path",
            "runlog_id",
            "parse_mock"
        ],
        "docstring": "Tests FileNotFoundError when the specified path is not found."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "mock_client",
        "code_chunk": "def mock_client():\n    \"\"\"Mock GCS client.\"\"\"\n    return mock.Mock(spec=storage.Client)",
        "variables": [],
        "docstring": "Mock GCS client."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "mock_bucket",
        "code_chunk": "def mock_bucket(mock_client):\n    \"\"\"Mock GCS bucket.\"\"\"\n    bucket = mock.Mock(spec=storage.Bucket)\n    mock_client.bucket.return_value = bucket\n    return bucket",
        "variables": [
            "bucket",
            "mock_client"
        ],
        "docstring": "Mock GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "mock_blob",
        "code_chunk": "def mock_blob(mock_bucket):\n    \"\"\"Mock GCS blob.\"\"\"\n    blob = mock.Mock(spec=storage.Blob)\n    mock_bucket.blob.return_value = blob\n    return blob",
        "variables": [
            "mock_bucket",
            "blob"
        ],
        "docstring": "Mock GCS blob."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "mock_list_blobs",
        "code_chunk": "def mock_list_blobs(mock_client):\n    \"\"\"Mock list_blobs method.\"\"\"\n    mock_client.list_blobs.return_value = iter([mock.Mock()])\n    return mock_client.list_blobs",
        "variables": [
            "mock_client"
        ],
        "docstring": "Mock list_blobs method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "mock_path",
        "code_chunk": "def mock_path():\n    \"\"\"Mock Path object.\"\"\"\n    with mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.Path\") as mock_path:\n        yield mock_path",
        "variables": [
            "mock_path"
        ],
        "docstring": "Mock Path object."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_is_gcs_directory",
        "code_chunk": "def test_is_gcs_directory(self, mock_client, mock_list_blobs):\n        \"\"\"Test if a GCS object is a directory.\"\"\"\n        assert is_gcs_directory(\n            mock_client,\n            \"bucket\",\n            \"path\",\n        ), \"Expected path to be recognized as a GCS directory.\"",
        "variables": [
            "self",
            "mock_list_blobs",
            "mock_client"
        ],
        "docstring": "Test if a GCS object is a directory."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_is_not_gcs_directory",
        "code_chunk": "def test_is_not_gcs_directory(self, mock_client):\n        \"\"\"Test if a GCS object is not a directory.\"\"\"\n        mock_client.list_blobs.return_value = iter([])\n        assert not is_gcs_directory(\n            mock_client,\n            \"bucket\",\n            \"path\",\n        ), \"Expected path to not be recognized as a GCS directory.\"",
        "variables": [
            "self",
            "mock_client"
        ],
        "docstring": "Test if a GCS object is not a directory."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_file_exists",
        "code_chunk": "def test_file_exists(self, mock_client, mock_blob):\n        \"\"\"Test if a file exists in GCS bucket.\"\"\"\n        mock_blob.exists.return_value = True\n        assert file_exists(\n            mock_client,\n            \"bucket\",\n            \"path\",\n        ), \"Expected file to exist in GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_client"
        ],
        "docstring": "Test if a file exists in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_file_not_exists",
        "code_chunk": "def test_file_not_exists(self, mock_client, mock_blob):\n        \"\"\"Test if a file does not exist in GCS bucket.\"\"\"\n        mock_blob.exists.return_value = False\n        assert not file_exists(\n            mock_client,\n            \"bucket\",\n            \"path\",\n        ), \"Expected file to not exist in GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_client"
        ],
        "docstring": "Test if a file does not exist in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_upload_file_success",
        "code_chunk": "def test_upload_file_success(self, mock_path, mock_client, mock_blob):\n        \"\"\"Test successful upload of a file to GCS bucket.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\"):\n            mock_blob.exists.return_value = False\n            mock_path.return_value.exists.return_value = True\n            assert upload_file(\n                mock_client,\n                \"bucket\",\n                \"/local/path\",\n                \"path\",\n                overwrite=True,\n            ), \"Expected file to be uploaded successfully to GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_path",
            "mock_client"
        ],
        "docstring": "Test successful upload of a file to GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_upload_file_already_exists",
        "code_chunk": "def test_upload_file_already_exists(self, mock_path, mock_client, mock_blob):\n        \"\"\"Test upload fails if file already exists in GCS bucket.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\"):\n            mock_blob.exists.return_value = True\n            mock_path.return_value.exists.return_value = True\n            assert not upload_file(\n                mock_client,\n                \"bucket\",\n                \"/local/path\",\n                \"path\",\n            ), \"Expected upload to fail as file already exists in GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_path",
            "mock_client"
        ],
        "docstring": "Test upload fails if file already exists in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_upload_file_local_not_exists",
        "code_chunk": "def test_upload_file_local_not_exists(self, mock_path, mock_client, mock_blob):\n        \"\"\"Test upload fails if local file does not exist.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\"):\n            mock_path.return_value.exists.return_value = False\n            assert not upload_file(\n                mock_client,\n                \"bucket\",\n                \"/local/path\",\n                \"path\",\n            ), \"Expected upload to fail as local file does not exist.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_path",
            "mock_client"
        ],
        "docstring": "Test upload fails if local file does not exist."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_download_file_success",
        "code_chunk": "def test_download_file_success(self, mock_path, mock_client, mock_blob):\n        \"\"\"Test successful download of a file from GCS bucket.\"\"\"\n        with mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\"):\n            mock_blob.exists.return_value = True\n            mock_path.return_value.exists.return_value = True\n            assert download_file(\n                mock_client,\n                \"bucket\",\n                \"path\",\n                \"/local/path\",\n                overwrite=True,\n            ), \"Expected file to be downloaded successfully from GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_path",
            "mock_client"
        ],
        "docstring": "Test successful download of a file from GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_download_file_not_exists",
        "code_chunk": "def test_download_file_not_exists(self, mock_path, mock_client, mock_blob):\n        \"\"\"Test download fails if file does not exist in GCS bucket.\"\"\"\n        with mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\"):\n            mock_blob.exists.return_value = False\n            mock_path.return_value.exists.return_value = True\n            assert not download_file(\n                mock_client,\n                \"bucket\",\n                \"path\",\n                \"/local/path\",\n            ), \"Expected download to fail as file does not exist in GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_path",
            "mock_client"
        ],
        "docstring": "Test download fails if file does not exist in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_download_file_local_exists_no_overwrite",
        "code_chunk": "def test_download_file_local_exists_no_overwrite(\n        self,\n        mock_path,\n        mock_client,\n        mock_blob,\n    ):\n        \"\"\"Test download fails if local file exists and overwrite is False.\"\"\"\n        with mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\"):\n            mock_blob.exists.return_value = True\n            mock_path.return_value.exists.return_value = True\n            assert not download_file(\n                mock_client,\n                \"bucket\",\n                \"path\",\n                \"/local/path\",\n                overwrite=False,\n            ), \"Expected download to fail as local file exists and overwrite is False.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_path",
            "mock_client"
        ],
        "docstring": "Test download fails if local file exists and overwrite is False."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_delete_file_success",
        "code_chunk": "def test_delete_file_success(self, mock_client, mock_blob):\n        \"\"\"Test successful deletion of a file from GCS bucket.\"\"\"\n        with mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\"):\n            mock_blob.exists.return_value = True\n            assert delete_file(\n                mock_client,\n                \"bucket\",\n                \"path\",\n            ), \"Expected file to be deleted successfully from GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_client"
        ],
        "docstring": "Test successful deletion of a file from GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_delete_file_not_exists",
        "code_chunk": "def test_delete_file_not_exists(self, mock_client, mock_blob):\n        \"\"\"Test deletion fails if file does not exist in GCS bucket.\"\"\"\n        with mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\"):\n            mock_blob.exists.return_value = False\n            assert not delete_file(\n                mock_client,\n                \"bucket\",\n                \"path\",\n            ), \"Expected deletion to fail as file does not exist in GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_client"
        ],
        "docstring": "Test deletion fails if file does not exist in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_delete_file_exception",
        "code_chunk": "def test_delete_file_exception(self, mock_client, mock_blob):\n        \"\"\"Test deletion fails if an exception occurs.\"\"\"\n        with mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\"):\n            mock_blob.exists.return_value = True\n            mock_blob.delete.side_effect = Exception(\"Deletion failed\")\n            assert not delete_file(\n                mock_client,\n                \"bucket\",\n                \"path\",\n            ), \"Expected deletion to fail due to an exception.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_client"
        ],
        "docstring": "Test deletion fails if an exception occurs."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_copy_file_success",
        "code_chunk": "def test_copy_file_success(self, mock_client, mock_blob):\n        \"\"\"Test successful copy of a file within GCS buckets.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.is_gcs_directory\",\n            return_value=False,\n        ):\n            mock_blob.exists.return_value = True\n            assert copy_file(\n                mock_client,\n                \"source_bucket\",\n                \"source_path\",\n                \"dest_bucket\",\n                \"dest_path\",\n                overwrite=True,\n            ), \"Expected file to be copied successfully within GCS buckets.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_client"
        ],
        "docstring": "Test successful copy of a file within GCS buckets."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_copy_file_not_exists",
        "code_chunk": "def test_copy_file_not_exists(self, mock_client, mock_blob):\n        \"\"\"Test copy fails if source file does not exist in GCS bucket.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.is_gcs_directory\",\n            return_value=False,\n        ):\n            mock_blob.exists.return_value = False\n            assert not copy_file(\n                mock_client,\n                \"source_bucket\",\n                \"source_path\",\n                \"dest_bucket\",\n                \"dest_path\",\n            ), \"Expected copy to fail as source file does not exist in GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_client"
        ],
        "docstring": "Test copy fails if source file does not exist in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_copy_file_is_directory",
        "code_chunk": "def test_copy_file_is_directory(self, mock_client, mock_blob):\n        \"\"\"Test copy fails if source object is a directory.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.is_gcs_directory\",\n            return_value=True,\n        ):\n            assert not copy_file(\n                mock_client,\n                \"source_bucket\",\n                \"source_path\",\n                \"dest_bucket\",\n                \"dest_path\",\n            ), \"Expected copy to fail as source object is a directory.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_client"
        ],
        "docstring": "Test copy fails if source object is a directory."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_create_folder_success",
        "code_chunk": "def test_create_folder_success(self, mock_client, mock_blob):\n        \"\"\"Test successful creation of a folder in GCS bucket.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\"):\n            mock_blob.exists.return_value = False\n            assert create_folder_on_gcs(\n                mock_client,\n                \"bucket\",\n                \"folder/\",\n            ), \"Expected folder to be created successfully in GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_client"
        ],
        "docstring": "Test successful creation of a folder in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_create_existing_folder",
        "code_chunk": "def test_create_existing_folder(self, mock_client, mock_blob):\n        \"\"\"Test creation succeeds if folder already exists in GCS bucket.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\"):\n            mock_blob.exists.return_value = True\n            assert create_folder_on_gcs(\n                mock_client,\n                \"bucket\",\n                \"folder/\",\n            ), \"Expected creation to succeed as folder already exists in GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_client"
        ],
        "docstring": "Test creation succeeds if folder already exists in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_create_folder_exception",
        "code_chunk": "def test_create_folder_exception(self, mock_client, mock_blob):\n        \"\"\"Test creation fails if an exception occurs.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\"):\n            mock_blob.exists.return_value = False\n            mock_blob.upload_from_string.side_effect = Exception(\"Creation failed\")\n            assert not create_folder_on_gcs(\n                mock_client,\n                \"bucket\",\n                \"folder/\",\n            ), \"Expected creation to fail due to an exception.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_client"
        ],
        "docstring": "Test creation fails if an exception occurs."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_upload_folder_success",
        "code_chunk": "def test_upload_folder_success(self, mock_path, mock_client, mock_blob):\n        \"\"\"Test successful upload of a folder to GCS bucket.\"\"\"\n        mock_path.return_value.is_dir.return_value = True\n        mock_path.return_value.rglob.return_value = [mock_path]\n        mock_path.return_value.is_file.return_value = True\n        mock_blob.exists.return_value = False\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\"):\n            assert upload_folder(\n                mock_client,\n                \"bucket\",\n                \"/local/path\",\n                \"prefix\",\n                overwrite=True,\n            ), \"Expected folder to be uploaded successfully to GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_path",
            "mock_client"
        ],
        "docstring": "Test successful upload of a folder to GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_upload_folder_local_not_exists",
        "code_chunk": "def test_upload_folder_local_not_exists(self, mock_path, mock_client):\n        \"\"\"Test upload fails if local folder does not exist.\"\"\"\n        mock_path.return_value.is_dir.return_value = False\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\"):\n            assert not upload_folder(\n                mock_client,\n                \"bucket\",\n                \"/local/path\",\n                \"prefix\",\n                overwrite=True,\n            ), \"Expected upload to fail as local folder does not exist.\"",
        "variables": [
            "self",
            "mock_path",
            "mock_client"
        ],
        "docstring": "Test upload fails if local folder does not exist."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_upload_folder_file_already_exists",
        "code_chunk": "def test_upload_folder_file_already_exists(self, mock_path, mock_client, mock_blob):\n        \"\"\"Test upload fails if a file in the local folder already exists in GCS bucket.\"\"\"\n        mock_path.return_value.is_dir.return_value = True\n        mock_path.return_value.rglob.return_value = [mock_path]\n        mock_path.return_value.is_file.return_value = True\n        mock_blob.exists.return_value = True\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\"):\n            assert not upload_folder(\n                mock_client,\n                \"bucket\",\n                \"/local/path\",\n                \"prefix\",\n                overwrite=False,\n            ), \"Expected upload to fail as file already exists in GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_path",
            "mock_client"
        ],
        "docstring": "Test upload fails if a file in the local folder already exists in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_list_files_success",
        "code_chunk": "def test_list_files_success(self, mock_client, mock_list_blobs):\n        \"\"\"Test successful listing of files in GCS bucket.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\"):\n            mock_blob = mock.Mock()\n            mock_blob.name = \"file_name\"\n            mock_list_blobs.return_value = iter([mock_blob])\n            assert list_files(mock_client, \"bucket\", \"prefix\") == [\n                \"file_name\",\n            ], \"Expected to successfully list files in GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_list_blobs",
            "mock_client"
        ],
        "docstring": "Test successful listing of files in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_list_files_empty",
        "code_chunk": "def test_list_files_empty(self, mock_client):\n        \"\"\"Test listing returns empty if no files match prefix.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\"):\n            mock_client.list_blobs.return_value = iter([])\n            assert (\n                list_files(mock_client, \"bucket\", \"prefix\") == []\n            ), \"Expected to return an empty list as no files match the prefix.\"",
        "variables": [
            "self",
            "mock_client"
        ],
        "docstring": "Test listing returns empty if no files match prefix."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_download_folder_success",
        "code_chunk": "def test_download_folder_success(self, mock_path, mock_client, mock_list_blobs):\n        \"\"\"Test successful download of a folder from GCS bucket.\"\"\"\n        mock_path.return_value.exists.return_value = True\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.is_gcs_directory\",\n            return_value=True,\n        ):\n            assert download_folder(\n                mock_client,\n                \"bucket\",\n                \"prefix\",\n                \"/local/path\",\n                overwrite=True,\n            ), \"Expected folder to be downloaded successfully from GCS bucket.\"",
        "variables": [
            "self",
            "mock_path",
            "mock_list_blobs",
            "mock_client"
        ],
        "docstring": "Test successful download of a folder from GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_download_folder_not_exists",
        "code_chunk": "def test_download_folder_not_exists(self, mock_path, mock_client):\n        \"\"\"Test download fails if folder does not exist in GCS bucket.\"\"\"\n        mock_path.return_value.exists.return_value = True\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.is_gcs_directory\",\n            return_value=False,\n        ):\n            assert not download_folder(\n                mock_client,\n                \"bucket\",\n                \"prefix\",\n                \"/local/path\",\n                overwrite=True,\n            ), \"Expected download to fail as folder does not exist in GCS bucket.\"",
        "variables": [
            "self",
            "mock_path",
            "mock_client"
        ],
        "docstring": "Test download fails if folder does not exist in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_move_file_success",
        "code_chunk": "def test_move_file_success(self, mock_client, mock_blob):\n        \"\"\"Test successful move of a file within/between GCS buckets.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.is_gcs_directory\",\n            return_value=False,\n        ):\n            mock_blob.exists.return_value = True\n            assert move_file(\n                mock_client,\n                \"source_bucket\",\n                \"source_path\",\n                \"dest_bucket\",\n                \"dest_path\",\n            ), \"Expected file to be moved successfully within/between GCS buckets.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_client"
        ],
        "docstring": "Test successful move of a file within/between GCS buckets."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_move_file_not_exists",
        "code_chunk": "def test_move_file_not_exists(self, mock_client, mock_blob):\n        \"\"\"Test move fails if source file does not exist in GCS bucket.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.is_gcs_directory\",\n            return_value=False,\n        ):\n            mock_blob.exists.return_value = False\n            assert not move_file(\n                mock_client,\n                \"source_bucket\",\n                \"source_path\",\n                \"dest_bucket\",\n                \"dest_path\",\n            ), \"Expected move to fail as source file does not exist in GCS bucket.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_client"
        ],
        "docstring": "Test move fails if source file does not exist in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_move_file_is_directory",
        "code_chunk": "def test_move_file_is_directory(self, mock_client):\n        \"\"\"Test move fails if source object is a directory.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.is_gcs_directory\",\n            return_value=True,\n        ):\n            assert not move_file(\n                mock_client,\n                \"source_bucket\",\n                \"source_path\",\n                \"dest_bucket\",\n                \"dest_path\",\n            ), \"Expected move to fail as source object is a directory.\"",
        "variables": [
            "self",
            "mock_client"
        ],
        "docstring": "Test move fails if source object is a directory."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_delete_folder_success",
        "code_chunk": "def test_delete_folder_success(self, mock_client, mock_list_blobs):\n        \"\"\"Test successful deletion of a folder in GCS bucket.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.is_gcs_directory\",\n            return_value=True,\n        ):\n            assert delete_folder(\n                mock_client,\n                \"bucket\",\n                \"folder/\",\n            ), \"Expected folder to be deleted successfully in GCS bucket.\"",
        "variables": [
            "self",
            "mock_list_blobs",
            "mock_client"
        ],
        "docstring": "Test successful deletion of a folder in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_delete_folder_not_exists",
        "code_chunk": "def test_delete_folder_not_exists(self, mock_client):\n        \"\"\"Test deletion fails if folder does not exist in GCS bucket.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.is_gcs_directory\",\n            return_value=False,\n        ):\n            assert not delete_folder(\n                mock_client,\n                \"bucket\",\n                \"folder/\",\n            ), \"Expected deletion to fail as folder does not exist in GCS bucket.\"",
        "variables": [
            "self",
            "mock_client"
        ],
        "docstring": "Test deletion fails if folder does not exist in GCS bucket."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\helpers\\test_gcp_utils.py",
        "function_name": "test_delete_folder_exception",
        "code_chunk": "def test_delete_folder_exception(self, mock_client, mock_list_blobs):\n        \"\"\"Test deletion fails if an exception occurs.\"\"\"\n        with mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash\",\n        ), mock.patch(\n            \"rdsa_utils.gcp.helpers.gcp_utils.is_gcs_directory\",\n            return_value=True,\n        ):\n            mock_blob = mock.Mock()\n            mock_blob.delete.side_effect = Exception(\"Deletion failed\")\n            mock_list_blobs.return_value = iter([mock_blob])\n            assert not delete_folder(\n                mock_client,\n                \"bucket\",\n                \"folder/\",\n            ), \"Expected deletion to fail due to an exception.\"",
        "variables": [
            "self",
            "mock_blob",
            "mock_list_blobs",
            "mock_client"
        ],
        "docstring": "Test deletion fails if an exception occurs."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\io\\test_inputs.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\io\\test_inputs.py",
        "function_name": "test_method",
        "code_chunk": "def test_method(\n        self,\n        columns,\n        date_column,\n        date_range,\n        column_filter_dict,\n        expected,\n    ):\n        \"\"\"Test expected behaviour.\"\"\"\n        table_path = \"database_name.table_name\"\n\n        result = build_sql_query(\n            table_path,\n            columns=columns,\n            date_column=date_column,\n            date_range=date_range,\n            column_filter_dict=column_filter_dict,\n        )\n\n        # Use textwrap.dedent to remove leading whitespace from the\n        # string for comparing\n        expected = textwrap.dedent(expected)\n\n        assert result.strip(\"\\n\") == expected.strip(\"\\n\")",
        "variables": [
            "columns",
            "table_path",
            "self",
            "date_range",
            "column_filter_dict",
            "expected",
            "date_column",
            "result"
        ],
        "docstring": "Test expected behaviour."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\gcp\\io\\test_outputs.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "input_funct",
        "code_chunk": "def input_funct(s1: str):\n    \"\"\"Spark col function to use as test input.\"\"\"\n    return s1",
        "variables": [
            "s1"
        ],
        "docstring": "Spark col function to use as test input."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "input_funct_with_exclude",
        "code_chunk": "def input_funct_with_exclude(s1: str):\n    \"\"\"Spark col function with exclude parameter to use as test input.\"\"\"\n    return s1",
        "variables": [
            "s1"
        ],
        "docstring": "Spark col function with exclude parameter to use as test input."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self, create_spark_df):\n        \"\"\"Test expected functionality.\"\"\"\n        input_schema = T.StructType(\n            [\n                T.StructField(\"code\", T.StringType(), True),\n                T.StructField(\"shop\", T.StringType(), True),\n                T.StructField(\"collection_date\", T.DateType(), True),\n                T.StructField(\"values\", T.IntegerType(), False),\n            ],\n        )\n        input_df = create_spark_df(\n            [\n                (input_schema),\n                (\"banana\", \"shop_1\", to_datetime(\"2022-11-01\"), 20),\n                (\"banana\", \"shop_1\", to_datetime(\"2022-11-08\"), 21),\n                (\"oranges\", \"shop_1\", to_datetime(\"2022-12-01\"), 22),\n                (\"oranges\", \"shop_1\", to_datetime(\"2022-12-08\"), 23),\n            ],\n        )\n\n        column_list = [\"code\", \"shop\", \"collection_date\"]\n        nullable = False\n        actual = set_df_columns_nullable(\n            df=input_df,\n            column_list=column_list,\n            nullable=nullable,\n        )\n\n        output_schema = T.StructType(\n            [\n                T.StructField(\"code\", T.StringType(), False),\n                T.StructField(\"shop\", T.StringType(), False),\n                T.StructField(\"collection_date\", T.DateType(), False),\n                T.StructField(\"values\", T.IntegerType(), False),\n            ],\n        )\n        expected = create_spark_df(\n            [\n                (output_schema),\n                (\"banana\", \"shop_1\", to_datetime(\"2022-11-01\"), 20),\n                (\"banana\", \"shop_1\", to_datetime(\"2022-11-08\"), 21),\n                (\"oranges\", \"shop_1\", to_datetime(\"2022-12-01\"), 22),\n                (\"oranges\", \"shop_1\", to_datetime(\"2022-12-08\"), 23),\n            ],\n        )\n\n        assert_df_equality(actual, expected)",
        "variables": [
            "column_list",
            "output_schema",
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "nullable",
            "actual",
            "input_schema"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self, to_spark, id_vars, value_vars, expected):\n        \"\"\"Test expected functionality.\"\"\"\n        input_data = to_spark(\n            [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n            [\"col1\", \"col2\", \"col3\", \"col4\"],\n        )\n        actual = melt(df=input_data, id_vars=id_vars, value_vars=value_vars)\n        assert_df_equality(actual, to_spark(expected), ignore_nullable=True)",
        "variables": [
            "value_vars",
            "input_data",
            "self",
            "expected",
            "to_spark",
            "id_vars",
            "actual"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_string_positive_case",
        "code_chunk": "def test_string_positive_case(self, spark_session):\n        \"\"\"Test string input converts as expected.\"\"\"\n        string_input = \"i_am_string_therefore_i_am_?1234!\"\n        assert isinstance(_convert_to_spark_col(string_input), SparkCol)",
        "variables": [
            "self",
            "spark_session",
            "string_input"
        ],
        "docstring": "Test string input converts as expected."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_funct_positive_case",
        "code_chunk": "def test_funct_positive_case(self, spark_session):\n        \"\"\"Test function input converts as expected.\"\"\"\n        funct_input = input_funct(\"cheese\")\n        assert isinstance(_convert_to_spark_col(funct_input), SparkCol)",
        "variables": [
            "self",
            "spark_session",
            "funct_input"
        ],
        "docstring": "Test function input converts as expected."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_funct_negative_case",
        "code_chunk": "def test_funct_negative_case(self):\n        \"\"\"Test function input with exclude parameters does not convert.\"\"\"\n        assert isinstance(input_funct_with_exclude(\"cheese\"), str)",
        "variables": [
            "self"
        ],
        "docstring": "Test function input with exclude parameters does not convert."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_value_errors_raised",
        "code_chunk": "def test_value_errors_raised(self, func_input):\n        \"\"\"Test value errors raised for convert_to_spark_col.\"\"\"\n        with pytest.raises(ValueError):\n            _convert_to_spark_col(func_input)",
        "variables": [
            "func_input",
            "self"
        ],
        "docstring": "Test value errors raised for convert_to_spark_col."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected_one_column",
        "code_chunk": "def test_expected_one_column(self, to_spark):\n        \"\"\"Test expected functionality for one column.\"\"\"\n        input_data = to_spark([\"banana\", \"banana\"], \"string\").toDF(\"code\")\n        assert to_list(input_data) == [\"banana\", \"banana\"]",
        "variables": [
            "input_data",
            "self",
            "to_spark"
        ],
        "docstring": "Test expected functionality for one column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected_two_columns",
        "code_chunk": "def test_expected_two_columns(self, create_spark_df):\n        \"\"\"Test expected functionality for two columns.\"\"\"\n        input_data = create_spark_df(\n            [\n                (\"code\", \"values\"),\n                (\"banana\", 22),\n                (\"banana\", 23),\n            ],\n        )\n        assert to_list(input_data) == [[\"banana\", 22], [\"banana\", 23]]",
        "variables": [
            "input_data",
            "self",
            "create_spark_df"
        ],
        "docstring": "Test expected functionality for two columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_map_column_names",
        "code_chunk": "def test_map_column_names(self, create_spark_df):\n        \"\"\"Test column names are mapped to given values.\"\"\"\n        input_df = create_spark_df(\n            [\n                (\"col_A\", \"col_B\", \"col_Y\", \"col_D\", \"col_Z\"),\n                (\"aaa\", \"bbb\", \"ccc\", \"ddd\", \"eee\"),\n            ],\n        )\n\n        actual = map_column_names(\n            input_df,\n            {\"col_Y\": \"col_C\", \"col_Z\": \"col_E\"},\n        )\n\n        expected = create_spark_df(\n            [\n                (\"col_A\", \"col_B\", \"col_C\", \"col_D\", \"col_E\"),\n                (\"aaa\", \"bbb\", \"ccc\", \"ddd\", \"eee\"),\n            ],\n        )\n\n        assert_df_equality(actual, expected)",
        "variables": [
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "actual"
        ],
        "docstring": "Test column names are mapped to given values."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_non_empty_df",
        "code_chunk": "def test_non_empty_df(self, create_spark_df):\n        \"\"\"Test whether spark df contains any records.\"\"\"\n        non_empty_df = create_spark_df(\n            [\n                (\"col_a\", \"col_b\"),\n                (\"aaa\", \"bbb\"),\n            ],\n        )\n\n        assert is_df_empty(non_empty_df) is False",
        "variables": [
            "self",
            "non_empty_df",
            "create_spark_df"
        ],
        "docstring": "Test whether spark df contains any records."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_empty_df",
        "code_chunk": "def test_empty_df(self, create_spark_df):\n        \"\"\"Test whether spark df contains any records.\"\"\"\n        empty_df = create_spark_df(\n            [\n                (\"col_a\", \"col_b\"),\n                (\"aaa\", \"bbb\"),\n            ],\n        ).filter(\n            F.col(\"col_a\") == \"bbb\",\n        )  # Drop rows.\n\n        assert is_df_empty(empty_df) is True",
        "variables": [
            "self",
            "empty_df",
            "create_spark_df"
        ],
        "docstring": "Test whether spark df contains any records."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self, create_spark_df, input_df, expected):\n        \"\"\"Test expected functionality.\"\"\"\n        expected = create_spark_df(expected)\n\n        actual = unpack_list_col(\n            create_spark_df(input_df),\n            list_col=\"to_unpack\",\n            unpacked_col=\"to_unpack\",\n        )\n\n        assert_df_equality(actual, expected)",
        "variables": [
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "actual"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_collects_columns_to_mapping_when_schemas_are_consistent",
        "code_chunk": "def test_collects_columns_to_mapping_when_schemas_are_consistent(\n        self,\n        create_spark_df,\n    ):\n        \"\"\"Collects to a mapping of colname to value for each column in the list.\"\"\"\n        input_df = create_spark_df(\n            [\n                (\"item\", \"available\"),\n                (\"bacon\", \"yes\"),\n                (\"toast\", \"yes\"),\n                (\"egg\", \"no\"),\n            ],\n        )\n\n        actual = input_df.withColumn(\n            \"menu\",\n            create_colname_to_value_map([\"item\", \"available\"]),\n        )\n\n        expected = create_spark_df(\n            [\n                (\"item\", \"available\", \"menu\"),\n                (\"bacon\", \"yes\", {\"item\": \"bacon\", \"available\": \"yes\"}),\n                (\"toast\", \"yes\", {\"item\": \"toast\", \"available\": \"yes\"}),\n                (\"egg\", \"no\", {\"item\": \"egg\", \"available\": \"no\"}),\n            ],\n        )\n\n        assert_df_equality(actual, expected, ignore_nullable=True)",
        "variables": [
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "actual"
        ],
        "docstring": "Collects to a mapping of colname to value for each column in the list."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_coerces_number_types_to_string_when_collecting_to_map",
        "code_chunk": "def test_coerces_number_types_to_string_when_collecting_to_map(\n        self,\n        create_spark_df,\n    ):\n        \"\"\"Coerce to string as MapType requires consistent schema.\"\"\"\n        input_df = create_spark_df(\n            [\n                (\"item\", \"cost\"),\n                (\"bacon\", 2.0),\n                (\"toast\", 0.5),\n                (\"egg\", 1.0),\n            ],\n        )\n\n        actual = input_df.withColumn(\n            \"menu\",\n            create_colname_to_value_map([\"item\", \"cost\"]),\n        )\n\n        expected = create_spark_df(\n            [\n                (\"item\", \"cost\", \"menu\"),\n                (\"bacon\", 2.0, {\"item\": \"bacon\", \"cost\": \"2.0\"}),\n                (\"toast\", 0.5, {\"item\": \"toast\", \"cost\": \"0.5\"}),\n                (\"egg\", 1.0, {\"item\": \"egg\", \"cost\": \"1.0\"}),\n            ],\n        )\n\n        assert_df_equality(actual, expected, ignore_nullable=True)",
        "variables": [
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "actual"
        ],
        "docstring": "Coerce to string as MapType requires consistent schema."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self, create_spark_df, expected, partition_cols, order_cols):\n        \"\"\"Test expected functionality.\"\"\"\n        input_data = create_spark_df(\n            [\n                (\"code\", \"shop\", \"collection_date\", \"values\"),\n                (\"banana\", \"shop_1\", to_datetime(\"2022-11-01\"), 20),\n                (\"banana\", \"shop_1\", to_datetime(\"2022-11-08\"), 21),\n                (\"oranges\", \"shop_1\", to_datetime(\"2022-12-01\"), 22),\n                (\"oranges\", \"shop_1\", to_datetime(\"2022-12-08\"), 23),\n            ],\n        )\n        window_spec = get_window_spec(\n            partition_cols=partition_cols,\n            order_cols=order_cols,\n        )\n        assert_df_equality(\n            input_data.withColumn(\"test\", F.sum(\"values\").over(window_spec)),\n            create_spark_df(expected),\n            ignore_row_order=True,\n        )",
        "variables": [
            "order_cols",
            "input_data",
            "self",
            "partition_cols",
            "expected",
            "create_spark_df",
            "window_spec"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self, create_spark_df, input_df, expected, group, ascending):\n        \"\"\"Test the numeric column ranked as expected.\"\"\"\n        input_df = create_spark_df(input_df)\n        expected = create_spark_df(expected)\n\n        actual = input_df.withColumn(\n            \"rank\",\n            rank_numeric(\"expenditure\", group, ascending),\n        )\n\n        assert_df_equality(\n            actual,\n            expected.withColumn(\"rank\", F.col(\"rank\").astype(\"int\")),\n            ignore_row_order=True,\n            ignore_nullable=True,\n        )",
        "variables": [
            "ascending",
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "actual",
            "group"
        ],
        "docstring": "Test the numeric column ranked as expected."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_value_errors_raised",
        "code_chunk": "def test_value_errors_raised(self):\n        \"\"\"Test value errors raised for rank_numeric.\"\"\"\n        with pytest.raises(ValueError):\n            rank_numeric([\"expenditure\"], \"group\", False)",
        "variables": [
            "self"
        ],
        "docstring": "Test value errors raised for rank_numeric."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_calc_median_price",
        "code_chunk": "def test_calc_median_price(self, create_spark_df):\n        \"\"\"Test the median is calculated per grouping level.\"\"\"\n        groups = [\"group\", \"other_group\"]\n\n        input_df = create_spark_df(\n            (\n                [\n                    (\"period\", \"group\", \"other_group\", \"price\"),\n                    (to_date(\"2021-01-01\"), \"group_1\", \"other_group_1\", 1.0),\n                    (to_date(\"2021-01-07\"), \"group_1\", \"other_group_1\", 1.0),\n                    (to_date(\"2021-01-01\"), \"group_2\", \"other_group_2\", 5.0),\n                    (to_date(\"2021-01-07\"), \"group_2\", \"other_group_2\", 5.0),\n                    (to_date(\"2021-01-14\"), \"group_2\", \"other_group_2\", 5.1),\n                    (to_date(\"2021-01-01\"), \"group_3\", \"other_group_3\", 2.3),\n                ]\n            ),\n        )\n\n        expected = create_spark_df(\n            (\n                [\n                    (\"period\", \"group\", \"other_group\", \"price\", \"median\"),\n                    (to_date(\"2021-01-01\"), \"group_1\", \"other_group_1\", 1.0, 1.0),\n                    (to_date(\"2021-01-07\"), \"group_1\", \"other_group_1\", 1.0, 1.0),\n                    (to_date(\"2021-01-01\"), \"group_2\", \"other_group_2\", 5.0, 5.0),\n                    (to_date(\"2021-01-07\"), \"group_2\", \"other_group_2\", 5.0, 5.0),\n                    (to_date(\"2021-01-14\"), \"group_2\", \"other_group_2\", 5.1, 5.0),\n                    (to_date(\"2021-01-01\"), \"group_3\", \"other_group_3\", 2.3, 2.3),\n                ]\n            ),\n        )\n\n        actual = input_df.withColumn(\"median\", calc_median_price(groups, \"price\"))\n\n        assert_df_equality(actual, expected, ignore_row_order=True)",
        "variables": [
            "self",
            "expected",
            "input_df",
            "groups",
            "create_spark_df",
            "actual"
        ],
        "docstring": "Test the median is calculated per grouping level."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "input_df_fixture",
        "code_chunk": "def input_df_fixture(self, create_spark_df) -> SparkDF:\n        \"\"\"Provide a basic spark dataframe.\"\"\"\n        return create_spark_df(\n            [\n                (\"column_a\", \"column_b\", \"column_c\"),\n                (\"AA1\", \"BB1\", \"CC1\"),\n                (\"AA2\", \"BB2\", \"CC2\"),\n            ],\n        )",
        "variables": [
            "self",
            "create_spark_df"
        ],
        "docstring": "Provide a basic spark dataframe."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected_with_struct_cols",
        "code_chunk": "def test_expected_with_struct_cols(\n        self,\n        create_spark_df,\n        input_df,\n        struct_cols,\n        struct_col_name,\n        expected_schema,\n        expected_data,\n    ):\n        \"\"\"Test expected functionality when creating struct col from existing columns.\"\"\"\n        expected = create_spark_df([(expected_schema), *expected_data])\n        result = convert_cols_to_struct_col(\n            df=input_df,\n            struct_cols=struct_cols,\n            struct_col_name=struct_col_name,\n        )\n        assert_df_equality(\n            result,\n            expected,\n            ignore_column_order=True,\n            ignore_nullable=True,\n        )",
        "variables": [
            "expected_data",
            "expected_schema",
            "self",
            "expected",
            "input_df",
            "result",
            "create_spark_df",
            "struct_col_name",
            "struct_cols"
        ],
        "docstring": "Test expected functionality when creating struct col from existing columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected_no_struct_cols",
        "code_chunk": "def test_expected_no_struct_cols(\n        self,\n        create_spark_df,\n        input_df,\n        struct_cols,\n        struct_col_name,\n        no_struct_col_type,\n        no_struct_col_value,\n        expected_schema,\n        expected_data,\n    ):\n        \"\"\"Test expected functionality when no  cols being used to create new struct cols.\"\"\"\n        expected = create_spark_df([(expected_schema), *expected_data])\n        result = convert_cols_to_struct_col(\n            df=input_df,\n            struct_cols=struct_cols,\n            struct_col_name=struct_col_name,\n            no_struct_col_type=no_struct_col_type,\n            no_struct_col_value=no_struct_col_value,\n        )\n        assert_df_equality(\n            result,\n            expected,\n            ignore_column_order=True,\n            ignore_nullable=True,\n        )",
        "variables": [
            "expected_data",
            "expected_schema",
            "self",
            "no_struct_col_type",
            "expected",
            "input_df",
            "result",
            "create_spark_df",
            "struct_col_name",
            "struct_cols",
            "no_struct_col_value"
        ],
        "docstring": "Test expected functionality when no  cols being used to create new struct cols."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_raises_value_error",
        "code_chunk": "def test_raises_value_error(self, input_df_fixture):\n        \"\"\"Test ValueError raised when specified struct_cols are not present in input_df.\"\"\"\n        with pytest.raises(ValueError):\n            convert_cols_to_struct_col(\n                df=input_df_fixture,\n                struct_cols=[\"column_c\", \"column_ch\"],\n                struct_col_name=\"struct_col\",\n            )",
        "variables": [
            "self",
            "input_df_fixture"
        ],
        "docstring": "Test ValueError raised when specified struct_cols are not present in input_df."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self, create_spark_df, ascending, expected_data):\n        \"\"\"Test expected outputs.\"\"\"\n        input_df = create_spark_df(\n            [\n                (\"group\", \"week_start_date\", \"price\"),\n                (\"a\", to_datetime(\"2022-05-20\"), 5),\n                (\"a\", to_datetime(\"2022-05-21\"), 6),\n                (\"a\", to_datetime(\"2022-05-22\"), 7),\n                (\"b\", to_datetime(\"2022-04-02\"), 1),\n                (\"b\", to_datetime(\"2022-04-06\"), 2),\n                (\"b\", to_datetime(\"2022-04-07\"), 3),\n            ],\n        )\n        expected = create_spark_df(expected_data)\n\n        actual = select_first_obs_appearing_in_group(\n            df=input_df,\n            group=[\"group\"],\n            date_col=\"week_start_date\",\n            ascending=ascending,\n        )\n\n        assert_df_equality(\n            actual,\n            expected,\n        )",
        "variables": [
            "ascending",
            "expected_data",
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "actual"
        ],
        "docstring": "Test expected outputs."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_method",
        "code_chunk": "def test_method(self, create_spark_df, input_df, expected):\n        \"\"\"Test expected functionality.\n\n        Note it is non-trivial to implement column names within the struct\n        being defined, so left out, these then default to `_n` where n is the\n        ordered position in the struct for the column.\n        \"\"\"\n        actual = convert_struc_col_to_columns(df=create_spark_df(input_df))\n\n        assert_df_equality(actual, create_spark_df(expected))",
        "variables": [
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "actual"
        ],
        "docstring": "Test expected functionality.\n\nNote it is non-trivial to implement column names within the struct\nbeing defined, so left out, these then default to `_n` where n is the\nordered position in the struct for the column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_convert_nested_structs",
        "code_chunk": "def test_convert_nested_structs(self, create_spark_df):\n        \"\"\"Test expected functionality for recursive flattening.\"\"\"\n        actual = convert_struc_col_to_columns(\n            df=create_spark_df(\n                [\n                    (\"string_col\", \"struct_col\"),\n                    (\"a\", ((1, 2), (3, 4))),\n                    (\"b\", ((9, 8), (7, 6))),\n                ],\n            ),\n            convert_nested_structs=True,\n        )\n\n        assert_df_equality(\n            actual,\n            create_spark_df(\n                [\n                    (\"string_col\", \"_1\", \"_2\", \"_1\", \"_2\"),\n                    (\"a\", 1, 2, 3, 4),\n                    (\"b\", 9, 8, 7, 6),\n                ],\n            ),\n        )",
        "variables": [
            "actual",
            "self",
            "create_spark_df"
        ],
        "docstring": "Test expected functionality for recursive flattening."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_cut_lineage",
        "code_chunk": "def test_cut_lineage(self, spark_session: SparkSession) -> None:\n        \"\"\"Test that cut_lineage returns a DataFrame and doesn't raise any\n        exceptions during the process.\n        \"\"\"\n        # Create a mock DataFrame with all necessary attributes\n        df = MagicMock(spec=SparkDF)\n        df._jdf = MagicMock()\n        df._jdf.toJavaRDD.return_value = MagicMock()\n        df._jdf.schema.return_value = MagicMock()\n        df.sql_ctx = spark_session\n        spark_session._jsqlContext = MagicMock()\n        spark_session._jsqlContext.createDataFrame.return_value = MagicMock()\n        try:\n            new_df = cut_lineage(df)\n            assert isinstance(new_df, SparkDF)\n        except Exception:\n            pytest.fail(\"cut_lineage raised Exception unexpectedly!\")",
        "variables": [
            "self",
            "new_df",
            "spark_session",
            "df"
        ],
        "docstring": "Test that cut_lineage returns a DataFrame and doesn't raise any\nexceptions during the process."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_cut_lineage_error",
        "code_chunk": "def test_cut_lineage_error(self) -> None:\n        \"\"\"Test that cut_lineage raises an exception when an error occurs during\n        the lineage cutting process.\n        \"\"\"\n        # Create a mock DataFrame with all necessary attributes\n        df = MagicMock(spec=SparkDF)\n        df._jdf = MagicMock()\n        df._jdf.toJavaRDD.side_effect = Exception(\n            \"An error occurred during the lineage cutting process.\",\n        )\n        with pytest.raises(\n            Exception,\n            match=\"An error occurred during the lineage cutting process.\",\n        ):\n            cut_lineage(df)",
        "variables": [
            "self",
            "df"
        ],
        "docstring": "Test that cut_lineage raises an exception when an error occurs during\nthe lineage cutting process."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_find_spark_dataframes",
        "code_chunk": "def test_find_spark_dataframes(\n        self,\n        spark_session: SparkSession,\n        create_spark_df: Callable,\n    ) -> None:\n        \"\"\"Test that find_spark_dataframes correctly identifies DataFrames and\n        dictionaries containing DataFrames.\n        \"\"\"\n        input_schema = T.StructType(\n            [\n                T.StructField(\"name\", T.StringType(), True),\n                T.StructField(\"department\", T.StringType(), True),\n                T.StructField(\"salary\", T.IntegerType(), True),\n            ],\n        )\n        df = create_spark_df(\n            [\n                (input_schema),\n                (\"John\", \"Sales\", 20),\n                (\"Jane\", \"Marketing\", 21),\n            ],\n        )\n        locals_dict = {\n            \"df\": df,\n            \"not_df\": \"I'm not a DataFrame\",\n            \"df_dict\": {\"df1\": df, \"df2\": df},\n        }\n\n        result = find_spark_dataframes(locals_dict)\n\n        assert \"df\" in result\n        assert \"df_dict\" in result\n        assert \"not_df\" not in result\n\n        assert isinstance(result[\"df\"], SparkDF)\n        assert isinstance(result[\"df_dict\"], dict)\n        assert all(isinstance(val, SparkDF) for val in result[\"df_dict\"].values())",
        "variables": [
            "locals_dict",
            "spark_session",
            "df",
            "self",
            "result",
            "val",
            "create_spark_df",
            "input_schema"
        ],
        "docstring": "Test that find_spark_dataframes correctly identifies DataFrames and\ndictionaries containing DataFrames."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_create_spark_session_valid_sizes",
        "code_chunk": "def test_create_spark_session_valid_sizes(self, session_size: str) -> None:\n        \"\"\"Test create_spark_session with valid sizes.\"\"\"\n        spark = create_spark_session(size=session_size)\n        assert isinstance(\n            spark,\n            SparkSession,\n        ), \"The function should return a SparkSession instance.\"\n        spark.stop()",
        "variables": [
            "self",
            "spark",
            "session_size"
        ],
        "docstring": "Test create_spark_session with valid sizes."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_create_spark_session_invalid_sizes",
        "code_chunk": "def test_create_spark_session_invalid_sizes(self, session_size: str) -> None:\n        \"\"\"Test create_spark_session with invalid sizes.\"\"\"\n        with pytest.raises(ValueError):\n            create_spark_session(size=session_size)",
        "variables": [
            "self",
            "session_size"
        ],
        "docstring": "Test create_spark_session with invalid sizes."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_create_spark_session_with_extra_configs",
        "code_chunk": "def test_create_spark_session_with_extra_configs(\n        self,\n    ) -> None:\n        \"\"\"Test create_spark_session with extra configurations.\"\"\"\n        extra_configs = {\"spark.ui.enabled\": \"false\"}\n        spark = create_spark_session(app_name=\"default\", extra_configs=extra_configs)\n        assert (\n            spark.conf.get(\"spark.ui.enabled\") == \"false\"\n        ), \"Extra configurations should be applied.\"\n        spark.stop()",
        "variables": [
            "self",
            "extra_configs",
            "spark"
        ],
        "docstring": "Test create_spark_session with extra configurations."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "custom_spark_session",
        "code_chunk": "def custom_spark_session(self):\n        \"\"\"Spark session fixture for this test class.\"\"\"\n        spark = (\n            SparkSession.builder.master(\"local[2]\")\n            .appName(\"test_load_csv\")\n            .getOrCreate()\n        )\n        yield spark\n        spark.stop()",
        "variables": [
            "self",
            "spark"
        ],
        "docstring": "Spark session fixture for this test class."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "create_temp_csv",
        "code_chunk": "def create_temp_csv(self, tmp_path, data):\n        \"\"\"Create a temporary CSV file.\"\"\"\n        temp_file = tmp_path / \"test.csv\"\n        temp_file.write_text(data)\n        return str(temp_file)",
        "variables": [
            "self",
            "tmp_path",
            "data",
            "temp_file"
        ],
        "docstring": "Create a temporary CSV file."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_load_csv_basic",
        "code_chunk": "def test_load_csv_basic(self, custom_spark_session, tmp_path):\n        \"\"\"Test loading CSV file.\"\"\"\n        temp_file = self.create_temp_csv(tmp_path, self.data_basic)\n        df = load_csv(custom_spark_session, temp_file)\n        assert df.count() == 3\n        assert len(df.columns) == 3",
        "variables": [
            "tmp_path",
            "df",
            "self",
            "temp_file",
            "custom_spark_session"
        ],
        "docstring": "Test loading CSV file."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_load_csv_multiline",
        "code_chunk": "def test_load_csv_multiline(self, custom_spark_session, tmp_path):\n        \"\"\"Test loading multiline CSV file.\"\"\"\n        temp_file = self.create_temp_csv(tmp_path, self.data_multiline)\n        df = load_csv(custom_spark_session, temp_file, multiLine=True)\n        assert df.count() == 2\n        assert len(df.columns) == 3",
        "variables": [
            "tmp_path",
            "df",
            "self",
            "temp_file",
            "custom_spark_session"
        ],
        "docstring": "Test loading multiline CSV file."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_load_csv_keep_columns",
        "code_chunk": "def test_load_csv_keep_columns(self, custom_spark_session, tmp_path):\n        \"\"\"Test keeping specific columns.\"\"\"\n        temp_file = self.create_temp_csv(tmp_path, self.data_basic)\n        df = load_csv(custom_spark_session, temp_file, keep_columns=[\"col1\", \"col2\"])\n        assert df.count() == 3\n        assert len(df.columns) == 2\n        assert \"col1\" in df.columns\n        assert \"col2\" in df.columns\n        assert \"col3\" not in df.columns",
        "variables": [
            "tmp_path",
            "df",
            "self",
            "temp_file",
            "custom_spark_session"
        ],
        "docstring": "Test keeping specific columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_load_csv_drop_columns",
        "code_chunk": "def test_load_csv_drop_columns(self, custom_spark_session, tmp_path):\n        \"\"\"Test dropping specific columns.\"\"\"\n        temp_file = self.create_temp_csv(tmp_path, self.data_basic)\n        df = load_csv(custom_spark_session, temp_file, drop_columns=[\"col2\"])\n        assert df.count() == 3\n        assert len(df.columns) == 2\n        assert \"col1\" in df.columns\n        assert \"col3\" in df.columns\n        assert \"col2\" not in df.columns",
        "variables": [
            "tmp_path",
            "df",
            "self",
            "temp_file",
            "custom_spark_session"
        ],
        "docstring": "Test dropping specific columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_load_csv_rename_columns",
        "code_chunk": "def test_load_csv_rename_columns(self, custom_spark_session, tmp_path):\n        \"\"\"Test renaming columns.\"\"\"\n        temp_file = self.create_temp_csv(tmp_path, self.data_basic)\n        df = load_csv(\n            custom_spark_session,\n            temp_file,\n            rename_columns={\"col1\": \"new_col1\", \"col3\": \"new_col3\"},\n        )\n        assert df.count() == 3\n        assert len(df.columns) == 3\n        assert \"new_col1\" in df.columns\n        assert \"col1\" not in df.columns\n        assert \"new_col3\" in df.columns\n        assert \"col3\" not in df.columns",
        "variables": [
            "tmp_path",
            "df",
            "self",
            "temp_file",
            "custom_spark_session"
        ],
        "docstring": "Test renaming columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_load_csv_missing_keep_column",
        "code_chunk": "def test_load_csv_missing_keep_column(self, custom_spark_session, tmp_path):\n        \"\"\"Test error when keep column is missing.\"\"\"\n        temp_file = self.create_temp_csv(tmp_path, self.data_basic)\n        with pytest.raises(ValueError):\n            load_csv(custom_spark_session, temp_file, keep_columns=[\"col4\"])",
        "variables": [
            "custom_spark_session",
            "self",
            "tmp_path",
            "temp_file"
        ],
        "docstring": "Test error when keep column is missing."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_load_csv_missing_drop_column",
        "code_chunk": "def test_load_csv_missing_drop_column(self, custom_spark_session, tmp_path):\n        \"\"\"Test error when drop column is missing.\"\"\"\n        temp_file = self.create_temp_csv(tmp_path, self.data_basic)\n        with pytest.raises(ValueError):\n            load_csv(custom_spark_session, temp_file, drop_columns=[\"col4\"])",
        "variables": [
            "custom_spark_session",
            "self",
            "tmp_path",
            "temp_file"
        ],
        "docstring": "Test error when drop column is missing."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_load_csv_missing_rename_column",
        "code_chunk": "def test_load_csv_missing_rename_column(self, custom_spark_session, tmp_path):\n        \"\"\"Test error when rename column is missing.\"\"\"\n        temp_file = self.create_temp_csv(tmp_path, self.data_basic)\n        with pytest.raises(ValueError):\n            load_csv(\n                custom_spark_session,\n                temp_file,\n                rename_columns={\"col4\": \"new_col4\"},\n            )",
        "variables": [
            "custom_spark_session",
            "self",
            "tmp_path",
            "temp_file"
        ],
        "docstring": "Test error when rename column is missing."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_load_csv_with_encoding",
        "code_chunk": "def test_load_csv_with_encoding(self, custom_spark_session, tmp_path):\n        \"\"\"Test loading CSV with a specific encoding.\"\"\"\n        temp_file = self.create_temp_csv(tmp_path, self.data_basic)\n        df = load_csv(custom_spark_session, temp_file, encoding=\"ISO-8859-1\")\n        assert df.count() == 3\n        assert len(df.columns) == 3",
        "variables": [
            "tmp_path",
            "df",
            "self",
            "temp_file",
            "custom_spark_session"
        ],
        "docstring": "Test loading CSV with a specific encoding."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_load_csv_with_custom_delimiter",
        "code_chunk": "def test_load_csv_with_custom_delimiter(self, custom_spark_session, tmp_path):\n        \"\"\"Test loading CSV with a custom delimiter.\"\"\"\n        data_with_semicolon = \"\"\"col1;col2;col3\n1;A;foo\n2;B;bar\n3;C;baz\n\"\"\"\n        temp_file = self.create_temp_csv(tmp_path, data_with_semicolon)\n        df = load_csv(custom_spark_session, temp_file, sep=\";\")\n        assert df.count() == 3\n        assert len(df.columns) == 3",
        "variables": [
            "tmp_path",
            "df",
            "self",
            "temp_file",
            "custom_spark_session",
            "data_with_semicolon"
        ],
        "docstring": "Test loading CSV with a custom delimiter."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_load_csv_with_infer_schema",
        "code_chunk": "def test_load_csv_with_infer_schema(self, custom_spark_session, tmp_path):\n        \"\"\"Test loading CSV with schema inference.\"\"\"\n        temp_file = self.create_temp_csv(tmp_path, self.data_basic)\n        df = load_csv(custom_spark_session, temp_file, inferSchema=True)\n        assert df.schema[\"col1\"].dataType.typeName() == \"integer\"\n        assert df.schema[\"col2\"].dataType.typeName() == \"string\"\n        assert df.schema[\"col3\"].dataType.typeName() == \"string\"",
        "variables": [
            "tmp_path",
            "df",
            "self",
            "temp_file",
            "custom_spark_session"
        ],
        "docstring": "Test loading CSV with schema inference."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_load_csv_with_custom_quote",
        "code_chunk": "def test_load_csv_with_custom_quote(self, custom_spark_session, tmp_path):\n        \"\"\"Test loading CSV with a custom quote character.\"\"\"\n        data_with_custom_quote = \"\"\"col1,col2,col3\n1,A,foo\n2,B,'bar'\n3,C,'baz'\n\"\"\"\n        temp_file = self.create_temp_csv(tmp_path, data_with_custom_quote)\n        df = load_csv(custom_spark_session, temp_file, quote=\"'\")\n        assert df.count() == 3\n        assert len(df.columns) == 3\n        assert df.filter(df.col3 == \"bar\").count() == 1\n        assert df.filter(df.col3 == \"baz\").count() == 1",
        "variables": [
            "tmp_path",
            "df",
            "self",
            "data_with_custom_quote",
            "temp_file",
            "custom_spark_session"
        ],
        "docstring": "Test loading CSV with a custom quote character."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "create_external_table",
        "code_chunk": "def create_external_table(self, spark_session: SparkSession):\n        \"\"\"Create a mock external Hive table for testing.\"\"\"\n        spark = (\n            SparkSession.builder.master(\"local[2]\")\n            .appName(\"test_external_table\")\n            .enableHiveSupport()\n            .getOrCreate()\n        )\n        table_name = \"test_db.test_table\"\n        spark.sql(\"CREATE DATABASE IF NOT EXISTS test_db\")\n        schema = T.StructType([T.StructField(\"name\", T.StringType(), True)])\n        df = spark.createDataFrame([(\"Alice\",), (\"Bob\",)], schema)\n        df.write.mode(\"overwrite\").saveAsTable(table_name)\n        yield table_name, spark\n        spark.sql(f\"DROP TABLE {table_name}\")\n        spark.sql(\"DROP DATABASE test_db\")\n        spark.stop()",
        "variables": [
            "spark_session",
            "df",
            "self",
            "schema",
            "spark",
            "table_name"
        ],
        "docstring": "Create a mock external Hive table for testing."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "create_partitioned_table",
        "code_chunk": "def create_partitioned_table(self, spark_session: SparkSession):\n        \"\"\"Create a mock partitioned external Hive table for testing.\"\"\"\n        spark = (\n            SparkSession.builder.master(\"local[2]\")\n            .appName(\"test_partitioned_table\")\n            .enableHiveSupport()\n            .getOrCreate()\n        )\n        table_name = \"test_db.test_partitioned_table\"\n        spark.sql(\"CREATE DATABASE IF NOT EXISTS test_db\")\n        schema = T.StructType(\n            [\n                T.StructField(\"name\", T.StringType(), True),\n                T.StructField(\"year\", T.IntegerType(), True),\n            ],\n        )\n        df = spark.createDataFrame([(\"Alice\", 2020), (\"Bob\", 2021)], schema)\n        df.write.mode(\"overwrite\").partitionBy(\"year\").saveAsTable(table_name)\n        yield table_name, spark\n        spark.sql(f\"DROP TABLE {table_name}\")\n        spark.sql(\"DROP DATABASE test_db\")\n        spark.stop()",
        "variables": [
            "spark_session",
            "df",
            "self",
            "schema",
            "spark",
            "table_name"
        ],
        "docstring": "Create a mock partitioned external Hive table for testing."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_truncate_table",
        "code_chunk": "def test_truncate_table(self, create_external_table):\n        \"\"\"Test truncating an external Hive table.\"\"\"\n        table_name, spark_session = create_external_table\n        truncate_external_hive_table(spark_session, table_name)\n        truncated_df = spark_session.table(table_name)\n        assert truncated_df.count() == 0",
        "variables": [
            "spark_session",
            "self",
            "create_external_table",
            "truncated_df",
            "table_name"
        ],
        "docstring": "Test truncating an external Hive table."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_schema_preservation",
        "code_chunk": "def test_schema_preservation(self, create_external_table):\n        \"\"\"Test schema preservation after truncation.\"\"\"\n        table_name, spark_session = create_external_table\n        original_schema = spark_session.table(table_name).schema\n        truncate_external_hive_table(spark_session, table_name)\n        truncated_schema = spark_session.table(table_name).schema\n        assert original_schema == truncated_schema",
        "variables": [
            "spark_session",
            "self",
            "create_external_table",
            "original_schema",
            "table_name",
            "truncated_schema"
        ],
        "docstring": "Test schema preservation after truncation."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_truncate_partitioned_table",
        "code_chunk": "def test_truncate_partitioned_table(self, create_partitioned_table):\n        \"\"\"Test truncating a partitioned external Hive table.\"\"\"\n        table_name, spark_session = create_partitioned_table\n        truncate_external_hive_table(spark_session, table_name)\n        truncated_df = spark_session.table(table_name)\n        assert truncated_df.count() == 0",
        "variables": [
            "spark_session",
            "self",
            "create_partitioned_table",
            "truncated_df",
            "table_name"
        ],
        "docstring": "Test truncating a partitioned external Hive table."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_truncate_without_database",
        "code_chunk": "def test_truncate_without_database(self, create_external_table):\n        \"\"\"Test truncating a table when only table name is provided.\"\"\"\n        table_name, spark_session = create_external_table\n\n        # Set the current database and pass only the table name\n        spark_session.catalog.setCurrentDatabase(\"test_db\")\n        truncate_external_hive_table(spark_session, \"test_table\")\n        truncated_df = spark_session.table(table_name)\n        assert truncated_df.count() == 0",
        "variables": [
            "spark_session",
            "self",
            "create_external_table",
            "truncated_df",
            "table_name"
        ],
        "docstring": "Test truncating a table when only table name is provided."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_partition_preservation",
        "code_chunk": "def test_partition_preservation(self, create_partitioned_table):\n        \"\"\"Test partition preservation after truncation.\"\"\"\n        table_name, spark_session = create_partitioned_table\n        original_partitions = spark_session.sql(\n            f\"SHOW PARTITIONS {table_name}\",\n        ).collect()\n\n        truncate_external_hive_table(spark_session, table_name)\n        remaining_partitions = spark_session.sql(\n            f\"SHOW PARTITIONS {table_name}\",\n        ).collect()\n\n        # The partition should be dropped after truncation\n        assert len(original_partitions) > 0\n        assert len(remaining_partitions) == 0",
        "variables": [
            "spark_session",
            "self",
            "original_partitions",
            "create_partitioned_table",
            "remaining_partitions",
            "table_name"
        ],
        "docstring": "Test partition preservation after truncation."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_no_exceptions",
        "code_chunk": "def test_no_exceptions(self, create_external_table):\n        \"\"\"Test no exceptions are raised during truncation.\"\"\"\n        table_name, spark_session = create_external_table\n        try:\n            truncate_external_hive_table(spark_session, table_name)\n        except Exception as e:\n            pytest.fail(f\"Truncation raised an exception: {e}\")",
        "variables": [
            "spark_session",
            "self",
            "create_external_table",
            "table_name",
            "e"
        ],
        "docstring": "Test no exceptions are raised during truncation."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self, mock_logger, mock_time, create_spark_df):\n        \"\"\"Test caching a DataFrame and timing the process.\"\"\"\n        input_df = create_spark_df([(\"A\", \"B\", \"C\"), (1, 2, 3)])\n\n        # Mock time.time() return values\n        mock_time.side_effect = [100.0, 100.25]  # start_time, end_time\n\n        cache_time_df(input_df)\n\n        elapsed_time = round(100.25 - 100.0, 2)  # Should be 0.25\n        mock_logger.assert_called_once()\n        log_message = mock_logger.call_args[0][0]\n        assert f\"Cached in {elapsed_time} seconds\" in log_message",
        "variables": [
            "self",
            "elapsed_time",
            "mock_logger",
            "input_df",
            "create_spark_df",
            "log_message",
            "mock_time"
        ],
        "docstring": "Test caching a DataFrame and timing the process."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_invalid_input",
        "code_chunk": "def test_invalid_input(self):\n        \"\"\"Test invalid input type raises an error.\"\"\"\n        with pytest.raises(TypeError, match=\"Input must be a PySpark DataFrame\"):\n            cache_time_df([\"not\", \"a\", \"DataFrame\"])",
        "variables": [
            "self"
        ],
        "docstring": "Test invalid input type raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self, create_spark_df):\n        \"\"\"Test counting nulls in a DataFrame.\"\"\"\n        input_df = create_spark_df(\n            [(\"col1\", \"col2\", \"col3\"), (1, 2, None), (None, 4, None), (3, None, \"C\")],\n        )\n\n        expected = pd.DataFrame({\"col1\": [1], \"col2\": [1], \"col3\": [2]})\n\n        actual = count_nulls(input_df)\n\n        assert actual.equals(expected)",
        "variables": [
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "actual"
        ],
        "docstring": "Test counting nulls in a DataFrame."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_with_subset_columns",
        "code_chunk": "def test_with_subset_columns(self, create_spark_df):\n        \"\"\"Test counting nulls with a subset of columns.\"\"\"\n        input_df = create_spark_df(\n            [(\"col1\", \"col2\", \"col3\"), (1, 2, None), (None, 4, None), (3, None, \"C\")],\n        )\n\n        expected = pd.DataFrame({\"col1\": [1], \"col2\": [1]})\n\n        actual = count_nulls(input_df, subset_cols=[\"col1\", \"col2\"])\n\n        assert actual.equals(expected)",
        "variables": [
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "actual"
        ],
        "docstring": "Test counting nulls with a subset of columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_invalid_input",
        "code_chunk": "def test_invalid_input(self):\n        \"\"\"Test invalid DataFrame input raises an error.\"\"\"\n        with pytest.raises(TypeError, match=\"Input must be a PySpark DataFrame\"):\n            count_nulls([\"not\", \"a\", \"DataFrame\"])",
        "variables": [
            "self"
        ],
        "docstring": "Test invalid DataFrame input raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_invalid_subset_cols",
        "code_chunk": "def test_invalid_subset_cols(self, create_spark_df):\n        \"\"\"Test invalid subset_cols input raises an error.\"\"\"\n        input_df = create_spark_df(\n            [(\"col1\", \"col2\", \"col3\"), (1, 2, None), (None, 4, None), (3, None, \"C\")],\n        )\n\n        with pytest.raises(\n            TypeError,\n            match=\"subset_cols must be a list, a string, or None\",\n        ):\n            count_nulls(input_df, subset_cols=12345)\n\n        with pytest.raises(\n            TypeError,\n            match=\"All elements of subset_cols must be strings\",\n        ):\n            count_nulls(input_df, subset_cols=[\"col1\", 12345])",
        "variables": [
            "self",
            "create_spark_df",
            "input_df"
        ],
        "docstring": "Test invalid subset_cols input raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_sum",
        "code_chunk": "def test_sum(self, create_spark_df):\n        \"\"\"Test summing values in a column.\"\"\"\n        input_df = create_spark_df([(\"col1 INT\"), (1,), (2,), (3,)])\n        result = aggregate_col(input_df, \"col1\", \"sum\")\n        assert result == 6",
        "variables": [
            "self",
            "create_spark_df",
            "result",
            "input_df"
        ],
        "docstring": "Test summing values in a column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_invalid_operation",
        "code_chunk": "def test_invalid_operation(self, create_spark_df):\n        \"\"\"Test invalid operation raises ValueError.\"\"\"\n        input_df = create_spark_df([(\"col1 INT\"), (1,), (2,), (3,)])\n        with pytest.raises(ValueError, match=\"`operation` must be one of\"):\n            aggregate_col(input_df, \"col1\", \"invalid\")",
        "variables": [
            "self",
            "create_spark_df",
            "input_df"
        ],
        "docstring": "Test invalid operation raises ValueError."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_remove_null",
        "code_chunk": "def test_remove_null(self, create_spark_df):\n        \"\"\"Test removing null values from the unique list.\"\"\"\n        input_df = create_spark_df([\"col1 INT\", (1,), (2,), (None,)])\n        result = get_unique(input_df, \"col1\", remove_null=True)\n        assert result == [1, 2]",
        "variables": [
            "self",
            "create_spark_df",
            "result",
            "input_df"
        ],
        "docstring": "Test removing null values from the unique list."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_keep_null",
        "code_chunk": "def test_keep_null(self, create_spark_df):\n        \"\"\"Test keeping null values in the unique list.\"\"\"\n        input_df = create_spark_df([\"col1 INT\", (1,), (2,), (None,)])\n        result = get_unique(input_df, \"col1\", remove_null=False)\n        assert result == [1, 2, None]",
        "variables": [
            "self",
            "create_spark_df",
            "result",
            "input_df"
        ],
        "docstring": "Test keeping null values in the unique list."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_invalid_column",
        "code_chunk": "def test_invalid_column(self, create_spark_df):\n        \"\"\"Test invalid column raises an error.\"\"\"\n        input_df = create_spark_df([\"col1 INT\", (1,), (2,), (3,)])\n        with pytest.raises(TypeError, match=\"Column name must be a string\"):\n            get_unique(input_df, 123)",
        "variables": [
            "self",
            "create_spark_df",
            "input_df"
        ],
        "docstring": "Test invalid column raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_with_id_col",
        "code_chunk": "def test_with_id_col(self, create_spark_df):\n        \"\"\"Test dropping duplicates with a specified ID column.\"\"\"\n        input_df = create_spark_df(\n            [\"group_col STRING, id_col INT\", (\"A\", 1), (\"A\", 2), (\"B\", 3), (\"B\", 4)],\n        )\n        result_df = drop_duplicates_reproducible(input_df, \"group_col\", id_col=\"id_col\")\n        expected_df = create_spark_df(\n            [\"group_col STRING, id_col INT\", (\"A\", 1), (\"B\", 3)],\n        )\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "input_df",
            "create_spark_df",
            "result_df"
        ],
        "docstring": "Test dropping duplicates with a specified ID column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_without_id_col",
        "code_chunk": "def test_without_id_col(self, create_spark_df):\n        \"\"\"Test dropping duplicates without a specified ID column.\"\"\"\n        input_df = create_spark_df(\n            [\"group_col STRING, value_col INT\", (\"A\", 1), (\"A\", 2), (\"B\", 3), (\"B\", 4)],\n        )\n        result_df = drop_duplicates_reproducible(input_df, \"group_col\")\n        assert result_df.select(\"group_col\").distinct().count() == 2",
        "variables": [
            "self",
            "result_df",
            "create_spark_df",
            "input_df"
        ],
        "docstring": "Test dropping duplicates without a specified ID column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_invalid_column",
        "code_chunk": "def test_invalid_column(self, create_spark_df):\n        \"\"\"Test invalid column raises an error.\"\"\"\n        input_df = create_spark_df([\"group_col STRING, id_col INT\", (\"A\", 1), (\"B\", 3)])\n        with pytest.raises(TypeError, match=\"col must be a string\"):\n            drop_duplicates_reproducible(input_df, 123)",
        "variables": [
            "self",
            "create_spark_df",
            "input_df"
        ],
        "docstring": "Test invalid column raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_apply_function",
        "code_chunk": "def test_apply_function(self, create_spark_df):\n        \"\"\"Test applying a function to multiple columns.\"\"\"\n        input_df = create_spark_df([\"col1 INT, col2 INT\", (1, 2), (3, 4)])\n\n        def increment_column(df, col):\n            \"\"\"Test method.\"\"\"\n            return df.withColumn(col, F.col(col) + 1)\n\n        result_df = apply_col_func(input_df, [\"col1\", \"col2\"], increment_column)\n        expected_df = create_spark_df([\"col1 INT, col2 INT\", (2, 3), (4, 5)])\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "input_df",
            "create_spark_df",
            "result_df"
        ],
        "docstring": "Test applying a function to multiple columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_invalid_column",
        "code_chunk": "def test_invalid_column(self, create_spark_df):\n        \"\"\"Test invalid column list raises an error.\"\"\"\n        input_df = create_spark_df([\"col1 INT, col2 INT\", (1, 2), (3, 4)])\n        with pytest.raises(TypeError, match=\"cols must be a list of strings.\"):\n            apply_col_func(input_df, \"not_a_list\", lambda df, col: df)",
        "variables": [
            "self",
            "create_spark_df",
            "input_df"
        ],
        "docstring": "Test invalid column list raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_invalid_function",
        "code_chunk": "def test_invalid_function(self, create_spark_df):\n        \"\"\"Test invalid function raises an error.\"\"\"\n        input_df = create_spark_df([\"col1 INT, col2 INT\", (1, 2), (3, 4)])\n        with pytest.raises(TypeError, match=\"func must be a callable function\"):\n            apply_col_func(input_df, [\"col1\", \"col2\"], \"not_a_function\")",
        "variables": [
            "self",
            "create_spark_df",
            "input_df"
        ],
        "docstring": "Test invalid function raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_random_uniform_default_bounds",
        "code_chunk": "def test_random_uniform_default_bounds(self, create_spark_df):\n        \"\"\"Test random uniform column with default bounds (0 to 1).\"\"\"\n        input_df = create_spark_df([\"id INT\", (1,), (2,), (3,)])\n        result_df = pyspark_random_uniform(input_df, \"random_col\")\n        assert \"random_col\" in result_df.columns\n        assert (\n            result_df.filter(\n                (F.col(\"random_col\") < 0) | (F.col(\"random_col\") > 1),\n            ).count()\n            == 0\n        )",
        "variables": [
            "self",
            "result_df",
            "create_spark_df",
            "input_df"
        ],
        "docstring": "Test random uniform column with default bounds (0 to 1)."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_random_uniform_custom_bounds",
        "code_chunk": "def test_random_uniform_custom_bounds(self, create_spark_df):\n        \"\"\"Test random uniform column with custom bounds.\"\"\"\n        input_df = create_spark_df([\"id INT\", (1,), (2,), (3,)])\n        result_df = pyspark_random_uniform(\n            input_df,\n            \"random_col\",\n            lower_bound=5,\n            upper_bound=10,\n        )\n        assert \"random_col\" in result_df.columns\n        assert (\n            result_df.filter(\n                (F.col(\"random_col\") < 5) | (F.col(\"random_col\") > 10),\n            ).count()\n            == 0\n        )",
        "variables": [
            "self",
            "result_df",
            "create_spark_df",
            "input_df"
        ],
        "docstring": "Test random uniform column with custom bounds."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_random_uniform_with_seed",
        "code_chunk": "def test_random_uniform_with_seed(self, create_spark_df):\n        \"\"\"Test random uniform column with a fixed seed.\"\"\"\n        input_df = create_spark_df([\"id INT\", (1,), (2,), (3,)])\n        result_df_1 = pyspark_random_uniform(input_df, \"random_col\", seed=42)\n        result_df_2 = pyspark_random_uniform(input_df, \"random_col\", seed=42)\n        assert_df_equality(result_df_1, result_df_2, ignore_nullable=True)",
        "variables": [
            "result_df_1",
            "self",
            "result_df_2",
            "input_df",
            "create_spark_df"
        ],
        "docstring": "Test random uniform column with a fixed seed."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_cumulative_array_basic",
        "code_chunk": "def test_cumulative_array_basic(self, create_spark_df):\n        \"\"\"Test creating a cumulative array column.\"\"\"\n        input_df = create_spark_df(\n            [\"id INT, values ARRAY<DOUBLE>\", (1, [1.0, 2.0, 3.0]), (2, [4.0, 5.0])],\n        )\n        result_df = cumulative_array(input_df, \"values\", \"cumulative_values\")\n        expected_df = create_spark_df(\n            [\n                \"id INT, values ARRAY<DOUBLE>, cumulative_values ARRAY<DOUBLE>\",\n                (1, [1.0, 2.0, 3.0], [1.0, 3.0, 6.0]),\n                (2, [4.0, 5.0], [4.0, 9.0]),\n            ],\n        )\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "input_df",
            "create_spark_df",
            "result_df"
        ],
        "docstring": "Test creating a cumulative array column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_cumulative_array_empty",
        "code_chunk": "def test_cumulative_array_empty(self, create_spark_df):\n        \"\"\"Test cumulative array column with an empty array.\"\"\"\n        input_df = create_spark_df([\"id INT, values ARRAY<DOUBLE>\", (1, [])])\n        result_df = cumulative_array(input_df, \"values\", \"cumulative_values\")\n        expected_df = create_spark_df(\n            [\n                \"id INT, values ARRAY<DOUBLE>, cumulative_values ARRAY<DOUBLE>\",\n                (1, [], []),\n            ],\n        )\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "input_df",
            "create_spark_df",
            "result_df"
        ],
        "docstring": "Test cumulative array column with an empty array."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_union_mismatched_basic",
        "code_chunk": "def test_union_mismatched_basic(self, create_spark_df):\n        \"\"\"Test union of DataFrames with mismatched columns.\"\"\"\n        df1 = create_spark_df([\"id INT, name STRING\", (1, \"Alice\"), (2, \"Bob\")])\n        df2 = create_spark_df([\"id INT, age INT\", (3, 30), (4, 40)])\n        result_df = union_mismatched_dfs(df1, df2)\n        expected_df = create_spark_df(\n            [\n                \"id INT, name STRING, age INT\",\n                (1, \"Alice\", None),\n                (2, \"Bob\", None),\n                (3, None, 30),\n                (4, None, 40),\n            ],\n        )\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "create_spark_df",
            "df1",
            "result_df",
            "df2"
        ],
        "docstring": "Test union of DataFrames with mismatched columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_union_mismatched_no_overlap",
        "code_chunk": "def test_union_mismatched_no_overlap(self, create_spark_df):\n        \"\"\"Test union of DataFrames with no overlapping columns.\"\"\"\n        df1 = create_spark_df([\"id INT\", (1,), (2,)])\n        df2 = create_spark_df([\"name STRING\", (\"Alice\",), (\"Bob\",)])\n        result_df = union_mismatched_dfs(df1, df2)\n        expected_df = create_spark_df(\n            [\n                \"id INT, name STRING\",\n                (1, None),\n                (2, None),\n                (None, \"Alice\"),\n                (None, \"Bob\"),\n            ],\n        )\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "create_spark_df",
            "df1",
            "result_df",
            "df2"
        ],
        "docstring": "Test union of DataFrames with no overlapping columns."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_union_mismatched_empty_df",
        "code_chunk": "def test_union_mismatched_empty_df(self, create_spark_df):\n        \"\"\"Test union where one DataFrame is empty.\"\"\"\n        df1 = create_spark_df([\"id INT, name STRING\", (1, \"Alice\")])\n        df2 = create_spark_df([\"id INT, name STRING\"])\n        result_df = union_mismatched_dfs(df1, df2)\n        expected_df = create_spark_df([\"id INT, name STRING\", (1, \"Alice\")])\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "create_spark_df",
            "df1",
            "result_df",
            "df2"
        ],
        "docstring": "Test union where one DataFrame is empty."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_sum_columns_basic",
        "code_chunk": "def test_sum_columns_basic(self, create_spark_df):\n        \"\"\"Test summing multiple columns into a new column.\"\"\"\n        input_df = create_spark_df(\n            [\"col1 INT, col2 INT, col3 INT\", (1, 2, 3), (4, 5, 6)],\n        )\n        result_df = sum_columns(input_df, [\"col1\", \"col2\"], \"sum_col\")\n        expected_df = create_spark_df(\n            [\"col1 INT, col2 INT, col3 INT, sum_col INT\", (1, 2, 3, 3), (4, 5, 6, 9)],\n        )\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "input_df",
            "create_spark_df",
            "result_df"
        ],
        "docstring": "Test summing multiple columns into a new column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_sum_columns_invalid_cols",
        "code_chunk": "def test_sum_columns_invalid_cols(self, create_spark_df):\n        \"\"\"Test invalid column names raise an error.\"\"\"\n        input_df = create_spark_df([\"col1 INT, col2 INT\", (1, 2)])\n        with pytest.raises(TypeError, match=\"cols_to_sum must be a list\"):\n            sum_columns(input_df, \"not_a_list\", \"sum_col\")",
        "variables": [
            "self",
            "create_spark_df",
            "input_df"
        ],
        "docstring": "Test invalid column names raise an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_sum_columns_invalid_output_col",
        "code_chunk": "def test_sum_columns_invalid_output_col(self, create_spark_df):\n        \"\"\"Test invalid output column raises an error.\"\"\"\n        input_df = create_spark_df([\"col1 INT, col2 INT\", (1, 2)])\n        with pytest.raises(TypeError, match=\"output_col must be a string\"):\n            sum_columns(input_df, [\"col1\", \"col2\"], 123)",
        "variables": [
            "self",
            "create_spark_df",
            "input_df"
        ],
        "docstring": "Test invalid output column raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_set_nulls_basic",
        "code_chunk": "def test_set_nulls_basic(self, create_spark_df):\n        \"\"\"Test replacing specified values with nulls.\"\"\"\n        input_df = create_spark_df([\"col1 STRING\", (\"A\",), (\"B\",), (\"C\",)])\n        result_df = set_nulls(input_df, \"col1\", [\"B\", \"C\"])\n        expected_df = create_spark_df([\"col1 STRING\", (\"A\",), (None,), (None,)])\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "input_df",
            "create_spark_df",
            "result_df"
        ],
        "docstring": "Test replacing specified values with nulls."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_set_nulls_single_value",
        "code_chunk": "def test_set_nulls_single_value(self, create_spark_df):\n        \"\"\"Test replacing a single value with nulls.\"\"\"\n        input_df = create_spark_df([\"col1 STRING\", (\"A\",), (\"B\",), (\"C\",)])\n        result_df = set_nulls(input_df, \"col1\", \"B\")\n        expected_df = create_spark_df([\"col1 STRING\", (\"A\",), (None,), (\"C\",)])\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "input_df",
            "create_spark_df",
            "result_df"
        ],
        "docstring": "Test replacing a single value with nulls."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_set_nulls_invalid_column",
        "code_chunk": "def test_set_nulls_invalid_column(self, create_spark_df):\n        \"\"\"Test invalid column raises an error.\"\"\"\n        input_df = create_spark_df([\"col1 STRING\", (\"A\",), (\"B\",)])\n        with pytest.raises(TypeError, match=\"column must be a string\"):\n            set_nulls(input_df, 123, [\"A\"])",
        "variables": [
            "self",
            "create_spark_df",
            "input_df"
        ],
        "docstring": "Test invalid column raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_union_multi_basic",
        "code_chunk": "def test_union_multi_basic(self, create_spark_df):\n        \"\"\"Test union of multiple DataFrames.\"\"\"\n        df1 = create_spark_df([\"id INT, name STRING\", (1, \"Alice\"), (2, \"Bob\")])\n        df2 = create_spark_df([\"id INT, name STRING\", (3, \"Charlie\"), (4, \"Diana\")])\n        result_df = union_multi_dfs([df1, df2])\n        expected_df = create_spark_df(\n            [\n                \"id INT, name STRING\",\n                (1, \"Alice\"),\n                (2, \"Bob\"),\n                (3, \"Charlie\"),\n                (4, \"Diana\"),\n            ],\n        )\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "create_spark_df",
            "df1",
            "result_df",
            "df2"
        ],
        "docstring": "Test union of multiple DataFrames."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_union_multi_empty_list",
        "code_chunk": "def test_union_multi_empty_list(self):\n        \"\"\"Test union with an empty list raises an error.\"\"\"\n        with pytest.raises(ValueError, match=\"df_list must not be empty\"):\n            union_multi_dfs([])",
        "variables": [
            "self"
        ],
        "docstring": "Test union with an empty list raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_union_multi_invalid_list",
        "code_chunk": "def test_union_multi_invalid_list(self, create_spark_df):\n        \"\"\"Test union with a non-DataFrame list raises an error.\"\"\"\n        df1 = create_spark_df([\"id INT, name STRING\", (1, \"Alice\")])\n        with pytest.raises(\n            TypeError,\n            match=\"All elements in df_list must be PySpark DataFrames.\",\n        ):\n            union_multi_dfs([df1, \"not_a_dataframe\"])",
        "variables": [
            "self",
            "create_spark_df",
            "df1"
        ],
        "docstring": "Test union with a non-DataFrame list raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_join_multi_inner",
        "code_chunk": "def test_join_multi_inner(self, create_spark_df):\n        \"\"\"Test inner join of multiple DataFrames.\"\"\"\n        df1 = create_spark_df([\"id INT, name STRING\", (1, \"Alice\"), (2, \"Bob\")])\n        df2 = create_spark_df([\"id INT, age INT\", (1, 25), (2, 30)])\n        df3 = create_spark_df(\n            [\"id INT, city STRING\", (1, \"New York\"), (2, \"Los Angeles\")],\n        )\n\n        result_df = join_multi_dfs([df1, df2, df3], on=\"id\", how=\"inner\")\n        expected_df = create_spark_df(\n            [\n                \"id INT, name STRING, age INT, city STRING\",\n                (1, \"Alice\", 25, \"New York\"),\n                (2, \"Bob\", 30, \"Los Angeles\"),\n            ],\n        )\n\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "create_spark_df",
            "df3",
            "result_df",
            "df1",
            "df2"
        ],
        "docstring": "Test inner join of multiple DataFrames."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_join_multi_outer",
        "code_chunk": "def test_join_multi_outer(self, create_spark_df):\n        \"\"\"Test outer join of multiple DataFrames.\"\"\"\n        df1 = create_spark_df([\"id INT, name STRING\", (1, \"Alice\"), (2, \"Bob\")])\n        df2 = create_spark_df([\"id INT, age INT\", (1, 25), (3, 40)])\n\n        result_df = join_multi_dfs([df1, df2], on=\"id\", how=\"outer\")\n        expected_df = create_spark_df(\n            [\n                \"id INT, name STRING, age INT\",\n                (1, \"Alice\", 25),\n                (2, \"Bob\", None),\n                (3, None, 40),\n            ],\n        )\n\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "create_spark_df",
            "df1",
            "result_df",
            "df2"
        ],
        "docstring": "Test outer join of multiple DataFrames."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_join_multi_invalid_how",
        "code_chunk": "def test_join_multi_invalid_how(self, create_spark_df):\n        \"\"\"Test invalid join type raises an error.\"\"\"\n        df1 = create_spark_df([\"id INT, name STRING\", (1, \"Alice\")])\n        df2 = create_spark_df([\"id INT, age INT\", (1, 25)])\n\n        with pytest.raises(ValueError, match=\"'how' must be one of\"):\n            join_multi_dfs([df1, df2], on=\"id\", how=\"invalid\")",
        "variables": [
            "self",
            "create_spark_df",
            "df2",
            "df1"
        ],
        "docstring": "Test invalid join type raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_map_column_values_basic",
        "code_chunk": "def test_map_column_values_basic(self, create_spark_df):\n        \"\"\"Test basic dictionary replacement.\"\"\"\n        input_df = create_spark_df([\"col1 STRING\", (\"A\",), (\"B\",), (\"C\",)])\n\n        result_df = map_column_values(input_df, {\"A\": \"Apple\", \"B\": \"Banana\"}, \"col1\")\n        expected_df = create_spark_df([\"col1 STRING\", (\"Apple\",), (\"Banana\",), (\"C\",)])\n\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "input_df",
            "create_spark_df",
            "result_df"
        ],
        "docstring": "Test basic dictionary replacement."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_map_column_values_with_output_col",
        "code_chunk": "def test_map_column_values_with_output_col(self, create_spark_df):\n        \"\"\"Test dictionary replacement with a specified output column.\"\"\"\n        input_df = create_spark_df([\"col1 STRING\", (\"A\",), (\"B\",), (\"C\",)])\n\n        result_df = map_column_values(\n            input_df,\n            {\"A\": \"Apple\", \"B\": \"Banana\"},\n            \"col1\",\n            \"new_col\",\n        )\n        expected_df = create_spark_df(\n            [\n                \"col1 STRING, new_col STRING\",\n                (\"A\", \"Apple\"),\n                (\"B\", \"Banana\"),\n                (\"C\", \"C\"),\n            ],\n        )\n\n        assert_df_equality(result_df, expected_df, ignore_nullable=True)",
        "variables": [
            "self",
            "expected_df",
            "input_df",
            "create_spark_df",
            "result_df"
        ],
        "docstring": "Test dictionary replacement with a specified output column."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "test_map_column_values_invalid_dict",
        "code_chunk": "def test_map_column_values_invalid_dict(self, create_spark_df):\n        \"\"\"Test invalid dictionary raises an error.\"\"\"\n        input_df = create_spark_df([\"col1 STRING\", (\"A\",), (\"B\",)])\n\n        with pytest.raises(TypeError, match=\"dict_ must be a dictionary\"):\n            map_column_values(input_df, \"not_a_dict\", \"col1\")",
        "variables": [
            "self",
            "create_spark_df",
            "input_df"
        ],
        "docstring": "Test invalid dictionary raises an error."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_pyspark.py",
        "function_name": "increment_column",
        "code_chunk": "def increment_column(df, col):\n            \"\"\"Test method.\"\"\"\n            return df.withColumn(col, F.col(col) + 1)",
        "variables": [
            "col",
            "df"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_leaves_tuple_as_is",
        "code_chunk": "def test_leaves_tuple_as_is(self):\n        \"\"\"Test method.\"\"\"\n        assert tuple_convert((\"beans\", \"toast\")) == (\"beans\", \"toast\")",
        "variables": [
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_converts_list_to_tuple",
        "code_chunk": "def test_converts_list_to_tuple(self):\n        \"\"\"Test method.\"\"\"\n        assert tuple_convert([\"carnage\", \"venom\"]) == (\"carnage\", \"venom\")",
        "variables": [
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_wraps_string_in_tuple_container",
        "code_chunk": "def test_wraps_string_in_tuple_container(self):\n        \"\"\"Test method.\"\"\"\n        assert tuple_convert(\"rice\") == (\"rice\",)",
        "variables": [
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_does_not_convert_dict",
        "code_chunk": "def test_does_not_convert_dict(self):\n        \"\"\"Test that dictionary is not converted to just tuple of keys.\"\"\"\n        assert tuple_convert({\"key\": \"value\"}) == ({\"key\": \"value\"},)",
        "variables": [
            "self"
        ],
        "docstring": "Test that dictionary is not converted to just tuple of keys."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_converts_none_to_empty",
        "code_chunk": "def test_converts_none_to_empty(self):\n        \"\"\"Test that when None passed tuple doesn't contain None value.\"\"\"\n        assert tuple_convert(None) == ()",
        "variables": [
            "self"
        ],
        "docstring": "Test that when None passed tuple doesn't contain None value."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_wraps_other_objs_in_tuple_container",
        "code_chunk": "def test_wraps_other_objs_in_tuple_container(self, obj):\n        \"\"\"Test method.\"\"\"\n        assert tuple_convert(obj) == (obj,)",
        "variables": [
            "self",
            "obj"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_leaves_list_as_is",
        "code_chunk": "def test_leaves_list_as_is(self):\n        \"\"\"Test method.\"\"\"\n        assert list_convert([\"beans\", \"toast\"]) == [\"beans\", \"toast\"]",
        "variables": [
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_converts_tuple_to_list",
        "code_chunk": "def test_converts_tuple_to_list(self):\n        \"\"\"Test method.\"\"\"\n        assert list_convert((\"carnage\", \"venom\")) == [\"carnage\", \"venom\"]",
        "variables": [
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_wraps_string_in_list_container",
        "code_chunk": "def test_wraps_string_in_list_container(self):\n        \"\"\"Test method.\"\"\"\n        assert list_convert(\"rice\") == [\"rice\"]",
        "variables": [
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_does_not_convert_dict",
        "code_chunk": "def test_does_not_convert_dict(self):\n        \"\"\"Test that dictionary is not converted to just list of keys.\"\"\"\n        assert list_convert({\"key\": \"value\"}) == [{\"key\": \"value\"}]",
        "variables": [
            "self"
        ],
        "docstring": "Test that dictionary is not converted to just list of keys."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_converts_none_to_empty",
        "code_chunk": "def test_converts_none_to_empty(self):\n        \"\"\"Test that when None passed list doesn't contain None value.\"\"\"\n        assert list_convert(None) == []",
        "variables": [
            "self"
        ],
        "docstring": "Test that when None passed list doesn't contain None value."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_wraps_other_objs_in_list_container",
        "code_chunk": "def test_wraps_other_objs_in_list_container(self, obj):\n        \"\"\"Test method.\"\"\"\n        assert list_convert(obj) == [obj]",
        "variables": [
            "self",
            "obj"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        sections = [[1, 2], [3, 4]]\n        elements_to_add = [5, 6]\n        extend_lists(\n            sections,\n            elements_to_add,\n        )\n        expected = [[1, 2, 5, 6], [3, 4, 5, 6]]\n        assert sections == expected",
        "variables": [
            "self",
            "elements_to_add",
            "expected",
            "sections"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "base_dict",
        "code_chunk": "def base_dict(self):\n        \"\"\"Create base dictionary used across all tests.\"\"\"\n        return {\n            \"var1\": \"value1\",\n            \"var2\": {\"var3\": 1.1, \"var4\": 4.4},\n            \"var5\": [1, 2, 3],\n            \"var6\": {\n                \"var7\": {\n                    \"var9\": \"helo\",\n                },\n            },\n        }",
        "variables": [
            "self"
        ],
        "docstring": "Create base dictionary used across all tests."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_method",
        "code_chunk": "def test_method(self, config, override_dict, expected):\n        \"\"\"Test expected behaviour.\"\"\"\n        result = overwrite_dictionary(config, override_dict)\n        assert result == expected",
        "variables": [
            "self",
            "result",
            "expected",
            "override_dict",
            "config"
        ],
        "docstring": "Test expected behaviour."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_raises_when_key_missing",
        "code_chunk": "def test_raises_when_key_missing(self, base_dict):\n        \"\"\"Test error raised if override key isn't present in base_dict.\"\"\"\n        override_dict = {\"var10\": \"value404\"}\n        with pytest.raises(ValueError):\n            overwrite_dictionary(base_dict, override_dict)",
        "variables": [
            "self",
            "base_dict",
            "override_dict"
        ],
        "docstring": "Test error raised if override key isn't present in base_dict."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_with_single_input",
        "code_chunk": "def test_with_single_input(self):\n        \"\"\"Test method functionality with single input.\"\"\"\n        input_dict = {\"key1\": 1, \"key2\": [2, 3, 4]}\n        result = list(calc_product_of_dict_values(**input_dict))\n\n        expected = [\n            {\"key1\": 1, \"key2\": 2},\n            {\"key1\": 1, \"key2\": 3},\n            {\"key1\": 1, \"key2\": 4},\n        ]\n        assert result == expected",
        "variables": [
            "self",
            "input_dict",
            "expected",
            "result"
        ],
        "docstring": "Test method functionality with single input."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_with_many_input",
        "code_chunk": "def test_with_many_input(self):\n        \"\"\"Test method functionality with many inputs.\"\"\"\n        input_dict1 = {\"key1\": 1, \"key2\": [2, 3, 4]}\n        input_dict2 = {\"key3\": \"raaaaa\", \"key4\": [\"a\", \"b\"]}\n        result = list(calc_product_of_dict_values(**input_dict1, **input_dict2))\n\n        expected = [\n            {\"key1\": 1, \"key2\": 2, \"key3\": \"raaaaa\", \"key4\": \"a\"},\n            {\"key1\": 1, \"key2\": 2, \"key3\": \"raaaaa\", \"key4\": \"b\"},\n            {\"key1\": 1, \"key2\": 3, \"key3\": \"raaaaa\", \"key4\": \"a\"},\n            {\"key1\": 1, \"key2\": 3, \"key3\": \"raaaaa\", \"key4\": \"b\"},\n            {\"key1\": 1, \"key2\": 4, \"key3\": \"raaaaa\", \"key4\": \"a\"},\n            {\"key1\": 1, \"key2\": 4, \"key3\": \"raaaaa\", \"key4\": \"b\"},\n        ]\n        assert result == expected",
        "variables": [
            "self",
            "input_dict2",
            "expected",
            "result",
            "input_dict1"
        ],
        "docstring": "Test method functionality with many inputs."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_with_dictionary_as_value",
        "code_chunk": "def test_with_dictionary_as_value(self):\n        \"\"\"Test method functionality when there is a nested dictionary.\n\n        Expected behaviour is to treat the value as a single object (i.e. don't\n        compute product of its values).\n        \"\"\"\n        input_dict = {\n            \"key1\": 1,\n            \"key2\": [2, 3, 4],\n            \"key3\": {\"key4\": [7, 8, 9], \"key5\": [4, 5, 6]},\n        }\n\n        result = list(calc_product_of_dict_values(**input_dict))\n\n        expected = [\n            {\"key1\": 1, \"key2\": 2, \"key3\": {\"key4\": [7, 8, 9], \"key5\": [4, 5, 6]}},\n            {\"key1\": 1, \"key2\": 3, \"key3\": {\"key4\": [7, 8, 9], \"key5\": [4, 5, 6]}},\n            {\"key1\": 1, \"key2\": 4, \"key3\": {\"key4\": [7, 8, 9], \"key5\": [4, 5, 6]}},\n        ]\n        assert result == expected",
        "variables": [
            "self",
            "input_dict",
            "expected",
            "result"
        ],
        "docstring": "Test method functionality when there is a nested dictionary.\n\nExpected behaviour is to treat the value as a single object (i.e. don't\ncompute product of its values)."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_year_month_format",
        "code_chunk": "def test_year_month_format(\n        self,\n        dates,\n    ):\n        \"\"\"Test different date formats for start and end date.\"\"\"\n        actual = convert_date_strings_to_datetimes(*dates)\n\n        expected = (\n            pd.Timestamp(\"2021-01-01 00:00:00\"),\n            pd.Timestamp(\"2021-03-31 23:59:59.999999\"),\n        )\n        assert actual == expected",
        "variables": [
            "actual",
            "self",
            "dates",
            "expected"
        ],
        "docstring": "Test different date formats for start and end date."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_year_month_day_format",
        "code_chunk": "def test_year_month_day_format(self):\n        \"\"\"Test for yyyy-mm-dd format.\"\"\"\n        dates = (\"2021-01-01\", \"2021-03-01\")\n\n        actual = convert_date_strings_to_datetimes(*dates)\n\n        expected = (\n            pd.Timestamp(\"2021-01-01 00:00:00\"),\n            pd.Timestamp(\"2021-03-01 23:59:59.999999\"),\n        )\n        assert actual == expected",
        "variables": [
            "actual",
            "self",
            "dates",
            "expected"
        ],
        "docstring": "Test for yyyy-mm-dd format."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_time_it_execution",
        "code_chunk": "def test_time_it_execution(self, mock_logger):\n        \"\"\"Test with a function that takes arguments.\"\"\"\n\n        @time_it()\n        def sample_function(delay):\n            sleep(delay)\n            return \"Done\"\n\n        result = sample_function(1)\n\n        assert result == \"Done\"\n\n        mock_logger.assert_called_once()\n        log_message = mock_logger.call_args[0][0]\n        assert \"Executed sample_function in\" in log_message\n        assert \"seconds\" in log_message",
        "variables": [
            "self",
            "log_message",
            "mock_logger",
            "result"
        ],
        "docstring": "Test with a function that takes arguments."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_time_it_no_arguments",
        "code_chunk": "def test_time_it_no_arguments(self, mock_logger):\n        \"\"\"Test with a function that takes no arguments.\"\"\"\n\n        @time_it()\n        def sample_function():\n            return \"No arguments\"\n\n        result = sample_function()\n\n        assert result == \"No arguments\"\n\n        mock_logger.assert_called_once()\n        log_message = mock_logger.call_args[0][0]\n        assert \"Executed sample_function in\" in log_message\n        assert \"seconds\" in log_message",
        "variables": [
            "self",
            "log_message",
            "mock_logger",
            "result"
        ],
        "docstring": "Test with a function that takes no arguments."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_list_difference",
        "code_chunk": "def test_list_difference(self):\n        \"\"\"Test method.\"\"\"\n        a = [1, 2, 3, 4]\n        b = [3, 4, 5, 6]\n        result = setdiff(a, b)\n        assert set(result) == {1, 2}",
        "variables": [
            "b",
            "self",
            "a",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_string_difference",
        "code_chunk": "def test_string_difference(self):\n        \"\"\"Test method.\"\"\"\n        a = \"abcdef\"\n        b = \"bdf\"\n        result = setdiff(a, b)\n        assert set(result) == {\"a\", \"c\", \"e\"}",
        "variables": [
            "b",
            "self",
            "a",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_set_difference",
        "code_chunk": "def test_set_difference(self):\n        \"\"\"Test method.\"\"\"\n        a = {1, 2, 3}\n        b = {2, 3, 4}\n        result = setdiff(a, b)\n        assert set(result) == {1}",
        "variables": [
            "b",
            "self",
            "a",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_range_difference",
        "code_chunk": "def test_range_difference(self):\n        \"\"\"Test method.\"\"\"\n        a = range(5)\n        b = range(2, 7)\n        result = setdiff(a, b)\n        assert set(result) == {0, 1}",
        "variables": [
            "b",
            "self",
            "a",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_dict_keys_difference",
        "code_chunk": "def test_dict_keys_difference(self):\n        \"\"\"Test method.\"\"\"\n        a = {\"a\": 1, \"b\": 2, \"c\": 3}\n        b = {\"b\": 4, \"d\": 5}\n        result = setdiff(a.keys(), b.keys())\n        assert set(result) == {\"a\", \"c\"}",
        "variables": [
            "b",
            "self",
            "a",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_empty_a",
        "code_chunk": "def test_empty_a(self):\n        \"\"\"Test method.\"\"\"\n        a = []\n        b = [1, 2, 3]\n        result = setdiff(a, b)\n        assert result == []",
        "variables": [
            "b",
            "self",
            "a",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_empty_b",
        "code_chunk": "def test_empty_b(self):\n        \"\"\"Test method.\"\"\"\n        a = [1, 2, 3]\n        b = []\n        result = setdiff(a, b)\n        assert set(result) == {1, 2, 3}",
        "variables": [
            "b",
            "self",
            "a",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_both_empty",
        "code_chunk": "def test_both_empty(self):\n        \"\"\"Test method.\"\"\"\n        a = []\n        b = []\n        result = setdiff(a, b)\n        assert result == []",
        "variables": [
            "b",
            "self",
            "a",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_duplicates_in_a",
        "code_chunk": "def test_duplicates_in_a(self):\n        \"\"\"Test method.\"\"\"\n        a = [1, 2, 2, 3, 4]\n        b = [3, 4]\n        result = setdiff(a, b)\n        assert set(result) == {1, 2}",
        "variables": [
            "b",
            "self",
            "a",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_non_iterable_a",
        "code_chunk": "def test_non_iterable_a(self):\n        \"\"\"Test method.\"\"\"\n        b = [1, 2, 3]\n        with pytest.raises(TypeError):\n            setdiff(123, b)",
        "variables": [
            "b",
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_non_iterable_b",
        "code_chunk": "def test_non_iterable_b(self):\n        \"\"\"Test method.\"\"\"\n        a = [1, 2, 3]\n        with pytest.raises(TypeError):\n            setdiff(a, 123)",
        "variables": [
            "self",
            "a"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_non_iterable_both",
        "code_chunk": "def test_non_iterable_both(self):\n        \"\"\"Test method.\"\"\"\n        with pytest.raises(TypeError):\n            setdiff(123, 456)",
        "variables": [
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_mixed_types",
        "code_chunk": "def test_mixed_types(self):\n        \"\"\"Test method.\"\"\"\n        a = [1, \"a\", 3.5, (2, 3)]\n        b = [\"a\", 3.5]\n        result = setdiff(a, b)\n        assert set(result) == {1, (2, 3)}",
        "variables": [
            "b",
            "self",
            "a",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_flatten_nested_list",
        "code_chunk": "def test_flatten_nested_list(self):\n        \"\"\"Test method.\"\"\"\n        iterable = [1, [2, 3], [4, [5, 6]]]\n        result = flatten_iterable(iterable)\n        assert result == [1, 2, 3, 4, [5, 6]]",
        "variables": [
            "self",
            "iterable",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_flatten_nested_tuple",
        "code_chunk": "def test_flatten_nested_tuple(self):\n        \"\"\"Test method.\"\"\"\n        iterable = (1, (2, 3), (4, (5, 6)))\n        result = flatten_iterable(iterable, types_to_flatten=(tuple,))\n        assert result == [1, 2, 3, 4, (5, 6)]",
        "variables": [
            "self",
            "iterable",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_flatten_mixed_types",
        "code_chunk": "def test_flatten_mixed_types(self):\n        \"\"\"Test method.\"\"\"\n        iterable = [1, (2, 3), [4, 5]]\n        result = flatten_iterable(iterable, types_to_flatten=(list, tuple))\n        assert result == [1, 2, 3, 4, 5]",
        "variables": [
            "self",
            "iterable",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_flatten_no_types_to_flatten",
        "code_chunk": "def test_flatten_no_types_to_flatten(self):\n        \"\"\"Test method.\"\"\"\n        iterable = [1, [2, 3], (4, 5)]\n        result = flatten_iterable(iterable, types_to_flatten=())\n        assert result == [1, [2, 3], (4, 5)]",
        "variables": [
            "self",
            "iterable",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_flatten_empty_iterable",
        "code_chunk": "def test_flatten_empty_iterable(self):\n        \"\"\"Test method.\"\"\"\n        iterable = []\n        result = flatten_iterable(iterable)\n        assert result == []",
        "variables": [
            "self",
            "iterable",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_flatten_flat_iterable",
        "code_chunk": "def test_flatten_flat_iterable(self):\n        \"\"\"Test method.\"\"\"\n        iterable = [1, 2, 3]\n        result = flatten_iterable(iterable)\n        assert result == [1, 2, 3]",
        "variables": [
            "self",
            "iterable",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_flatten_with_strings",
        "code_chunk": "def test_flatten_with_strings(self):\n        \"\"\"Test method.\"\"\"\n        iterable = [\"abc\", [1, 2], (3, 4)]\n        result = flatten_iterable(iterable, types_to_flatten=str)\n        assert result == [\"a\", \"b\", \"c\", [1, 2], (3, 4)]",
        "variables": [
            "self",
            "iterable",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_flatten_invalid_iterable",
        "code_chunk": "def test_flatten_invalid_iterable(self):\n        \"\"\"Test method.\"\"\"\n        with pytest.raises(TypeError):\n            flatten_iterable(123)",
        "variables": [
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_flatten_invalid_types_to_flatten",
        "code_chunk": "def test_flatten_invalid_types_to_flatten(self):\n        \"\"\"Test method.\"\"\"\n        with pytest.raises(TypeError):\n            flatten_iterable([1, [2, 3]], types_to_flatten=123)",
        "variables": [
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_flatten_non_type_in_tuple",
        "code_chunk": "def test_flatten_non_type_in_tuple(self):\n        \"\"\"Test method.\"\"\"\n        with pytest.raises(ValueError):\n            flatten_iterable([1, [2, 3]], types_to_flatten=(list, 123))",
        "variables": [
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_flatten_generator_input",
        "code_chunk": "def test_flatten_generator_input(self):\n        \"\"\"Test method.\"\"\"\n        iterable = (i for i in [[1, 2], [3, 4]])\n        result = flatten_iterable(iterable)\n        assert result == [1, 2, 3, 4]",
        "variables": [
            "i",
            "self",
            "iterable",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_convert_to_float",
        "code_chunk": "def test_convert_to_float(self):\n        \"\"\"Test method.\"\"\"\n        input_data = [1, 2, 3]\n        result = convert_types_iterable(input_data)\n        assert result == [1.0, 2.0, 3.0]",
        "variables": [
            "input_data",
            "self",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_convert_to_string",
        "code_chunk": "def test_convert_to_string(self):\n        \"\"\"Test method.\"\"\"\n        input_data = (10, 20, 30)\n        result = convert_types_iterable(input_data, dtype=str)\n        assert result == [\"10\", \"20\", \"30\"]",
        "variables": [
            "input_data",
            "self",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_convert_to_int",
        "code_chunk": "def test_convert_to_int(self):\n        \"\"\"Test method.\"\"\"\n        input_data = [\"10\", \"20\", \"30\"]\n        result = convert_types_iterable(input_data, dtype=int)\n        assert result == [10, 20, 30]",
        "variables": [
            "input_data",
            "self",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_with_range",
        "code_chunk": "def test_with_range(self):\n        \"\"\"Test method.\"\"\"\n        input_data = range(5)\n        result = convert_types_iterable(input_data, dtype=str)\n        assert result == [\"0\", \"1\", \"2\", \"3\", \"4\"]",
        "variables": [
            "input_data",
            "self",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_empty_input",
        "code_chunk": "def test_empty_input(self):\n        \"\"\"Test method.\"\"\"\n        input_data = []\n        result = convert_types_iterable(input_data)\n        assert result == []",
        "variables": [
            "input_data",
            "self",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_invalid_iterable",
        "code_chunk": "def test_invalid_iterable(self):\n        \"\"\"Test method.\"\"\"\n        with pytest.raises(TypeError):\n            convert_types_iterable(123)",
        "variables": [
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_invalid_dtype",
        "code_chunk": "def test_invalid_dtype(self):\n        \"\"\"Test method.\"\"\"\n        with pytest.raises(TypeError):\n            convert_types_iterable([1, 2, 3], dtype=123)",
        "variables": [
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_non_type_in_tuple",
        "code_chunk": "def test_non_type_in_tuple(self):\n        \"\"\"Test method.\"\"\"\n        with pytest.raises(TypeError):\n            convert_types_iterable([1, 2, 3], dtype=(int, 123))",
        "variables": [
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_mixed_types",
        "code_chunk": "def test_mixed_types(self):\n        \"\"\"Test method.\"\"\"\n        input_data = [1.1, \"2\", 3]\n        result = convert_types_iterable(input_data, dtype=int)\n        assert result == [1, 2, 3]",
        "variables": [
            "input_data",
            "self",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_nested_iterables",
        "code_chunk": "def test_nested_iterables(self):\n        \"\"\"Test method.\"\"\"\n        input_data = [[1, 2], [3, 4]]\n        with pytest.raises(TypeError):\n            convert_types_iterable(input_data)",
        "variables": [
            "input_data",
            "self"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_interleave_lists",
        "code_chunk": "def test_interleave_lists(self):\n        \"\"\"Test method.\"\"\"\n        iterable1 = [1, 2, 3]\n        iterable2 = [4, 5, 6]\n        result = interleave_iterables(iterable1, iterable2)\n        assert result == [1, 4, 2, 5, 3, 6]",
        "variables": [
            "iterable2",
            "self",
            "iterable1",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_interleave_tuples",
        "code_chunk": "def test_interleave_tuples(self):\n        \"\"\"Test method.\"\"\"\n        iterable1 = (1, 2, 3)\n        iterable2 = (\"a\", \"b\", \"c\")\n        result = interleave_iterables(iterable1, iterable2)\n        assert result == [1, \"a\", 2, \"b\", 3, \"c\"]",
        "variables": [
            "iterable2",
            "self",
            "iterable1",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_interleave_strings",
        "code_chunk": "def test_interleave_strings(self):\n        \"\"\"Test method.\"\"\"\n        iterable1 = \"ABC\"\n        iterable2 = \"123\"\n        result = interleave_iterables(iterable1, iterable2)\n        assert result == [\"A\", \"1\", \"B\", \"2\", \"C\", \"3\"]",
        "variables": [
            "iterable2",
            "self",
            "iterable1",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_interleave_ranges",
        "code_chunk": "def test_interleave_ranges(self):\n        \"\"\"Test method.\"\"\"\n        iterable1 = range(3)\n        iterable2 = range(10, 13)\n        result = interleave_iterables(iterable1, iterable2)\n        assert result == [0, 10, 1, 11, 2, 12]",
        "variables": [
            "iterable2",
            "self",
            "iterable1",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_empty_iterables",
        "code_chunk": "def test_empty_iterables(self):\n        \"\"\"Test method.\"\"\"\n        iterable1 = []\n        iterable2 = []\n        result = interleave_iterables(iterable1, iterable2)\n        assert result == []",
        "variables": [
            "iterable2",
            "self",
            "iterable1",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_different_lengths",
        "code_chunk": "def test_different_lengths(self):\n        \"\"\"Test method.\"\"\"\n        iterable1 = [1, 2]\n        iterable2 = [3]\n        with pytest.raises(ValueError):\n            interleave_iterables(iterable1, iterable2)",
        "variables": [
            "iterable2",
            "self",
            "iterable1"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_invalid_iterable1",
        "code_chunk": "def test_invalid_iterable1(self):\n        \"\"\"Test method.\"\"\"\n        iterable1 = 123\n        iterable2 = [1, 2, 3]\n        with pytest.raises(TypeError):\n            interleave_iterables(iterable1, iterable2)",
        "variables": [
            "iterable2",
            "self",
            "iterable1"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_invalid_iterable2",
        "code_chunk": "def test_invalid_iterable2(self):\n        \"\"\"Test method.\"\"\"\n        iterable1 = [1, 2, 3]\n        iterable2 = 456\n        with pytest.raises(TypeError):\n            interleave_iterables(iterable1, iterable2)",
        "variables": [
            "iterable2",
            "self",
            "iterable1"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_mixed_iterable_types",
        "code_chunk": "def test_mixed_iterable_types(self):\n        \"\"\"Test method.\"\"\"\n        iterable1 = [1, 2, 3]\n        iterable2 = (\"a\", \"b\", \"c\")\n        result = interleave_iterables(iterable1, iterable2)\n        assert result == [1, \"a\", 2, \"b\", 3, \"c\"]",
        "variables": [
            "iterable2",
            "self",
            "iterable1",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_interleave_nested_iterables",
        "code_chunk": "def test_interleave_nested_iterables(self):\n        \"\"\"Test method.\"\"\"\n        iterable1 = [[1], [2], [3]]\n        iterable2 = [[4], [5], [6]]\n        result = interleave_iterables(iterable1, iterable2)\n        assert result == [[1], [4], [2], [5], [3], [6]]",
        "variables": [
            "iterable2",
            "self",
            "iterable1",
            "result"
        ],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_pairwise_list",
        "code_chunk": "def test_pairwise_list(self):\n        \"\"\"Test pairwise function with a list.\"\"\"\n        result = list(pairwise_iterable([1, 2, 3, 4]))\n        expected = [(1, 2), (2, 3), (3, 4)]\n        assert result == expected",
        "variables": [
            "self",
            "expected",
            "result"
        ],
        "docstring": "Test pairwise function with a list."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_pairwise_string",
        "code_chunk": "def test_pairwise_string(self):\n        \"\"\"Test pairwise function with a string.\"\"\"\n        result = list(pairwise_iterable(\"abcde\"))\n        expected = [(\"a\", \"b\"), (\"b\", \"c\"), (\"c\", \"d\"), (\"d\", \"e\")]\n        assert result == expected",
        "variables": [
            "self",
            "expected",
            "result"
        ],
        "docstring": "Test pairwise function with a string."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_pairwise_tuple",
        "code_chunk": "def test_pairwise_tuple(self):\n        \"\"\"Test pairwise function with a tuple.\"\"\"\n        result = list(pairwise_iterable((10, 20, 30)))\n        expected = [(10, 20), (20, 30)]\n        assert result == expected",
        "variables": [
            "self",
            "expected",
            "result"
        ],
        "docstring": "Test pairwise function with a tuple."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_pairwise_empty",
        "code_chunk": "def test_pairwise_empty(self):\n        \"\"\"Test pairwise function with an empty iterable.\"\"\"\n        result = list(pairwise_iterable([]))\n        expected = []\n        assert result == expected",
        "variables": [
            "self",
            "expected",
            "result"
        ],
        "docstring": "Test pairwise function with an empty iterable."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_pairwise_single_element",
        "code_chunk": "def test_pairwise_single_element(self):\n        \"\"\"Test pairwise function with a single element.\"\"\"\n        result = list(pairwise_iterable([1]))\n        expected = []\n        assert result == expected",
        "variables": [
            "self",
            "expected",
            "result"
        ],
        "docstring": "Test pairwise function with a single element."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_pairwise_non_iterable",
        "code_chunk": "def test_pairwise_non_iterable(self):\n        \"\"\"Test pairwise function with a non-iterable input.\"\"\"\n        with pytest.raises(TypeError, match=\"Input must be an iterable.\"):\n            pairwise_iterable(1)",
        "variables": [
            "self"
        ],
        "docstring": "Test pairwise function with a non-iterable input."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_pairwise_custom_iterable",
        "code_chunk": "def test_pairwise_custom_iterable(self):\n        \"\"\"Test pairwise function with a custom iterable (e.g., a generator).\"\"\"\n\n        def my_gen():\n            \"\"\"Test method.\"\"\"\n            yield 5\n            yield 10\n            yield 15\n\n        result = list(pairwise_iterable(my_gen()))\n        expected = [(5, 10), (10, 15)]\n        assert result == expected",
        "variables": [
            "self",
            "expected",
            "result"
        ],
        "docstring": "Test pairwise function with a custom iterable (e.g., a generator)."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_merge_multi_inner",
        "code_chunk": "def test_merge_multi_inner(self):\n        \"\"\"Test merge_multi with 'inner' merge.\"\"\"\n        df1 = pd.DataFrame({\"key\": [\"A\", \"B\", \"C\"], \"value1\": [1, 2, 3]})\n        df2 = pd.DataFrame({\"key\": [\"A\", \"B\"], \"value2\": [4, 5]})\n        df3 = pd.DataFrame({\"key\": [\"A\"], \"value3\": [6]})\n\n        result = merge_multi_dfs([df1, df2, df3], on=\"key\", how=\"inner\")\n        expected = pd.DataFrame(\n            {\"key\": [\"A\"], \"value1\": [1], \"value2\": [4], \"value3\": [6]},\n        )\n        pd.testing.assert_frame_equal(result, expected)",
        "variables": [
            "self",
            "result",
            "expected",
            "df3",
            "df1",
            "df2"
        ],
        "docstring": "Test merge_multi with 'inner' merge."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_merge_multi_outer",
        "code_chunk": "def test_merge_multi_outer(self):\n        \"\"\"Test merge_multi with 'outer' merge.\"\"\"\n        df1 = pd.DataFrame({\"key\": [\"A\", \"B\", \"C\"], \"value1\": [1, 2, 3]})\n        df2 = pd.DataFrame({\"key\": [\"A\", \"B\"], \"value2\": [4, 5]})\n\n        result = merge_multi_dfs([df1, df2], on=\"key\", how=\"outer\", fillna_val=0)\n        expected = pd.DataFrame(\n            {\"key\": [\"A\", \"B\", \"C\"], \"value1\": [1, 2, 3], \"value2\": [4, 5, 0]},\n        )\n        pd.testing.assert_frame_equal(result, expected, check_dtype=False)",
        "variables": [
            "self",
            "result",
            "expected",
            "df1",
            "df2"
        ],
        "docstring": "Test merge_multi with 'outer' merge."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_merge_multi_left",
        "code_chunk": "def test_merge_multi_left(self):\n        \"\"\"Test merge_multi with 'left' merge.\"\"\"\n        df1 = pd.DataFrame({\"key\": [\"A\", \"B\", \"C\"], \"value1\": [1, 2, 3]})\n        df2 = pd.DataFrame({\"key\": [\"A\", \"B\"], \"value2\": [4, 5]})\n\n        result = merge_multi_dfs([df1, df2], on=\"key\", how=\"left\")\n        expected = pd.DataFrame(\n            {\"key\": [\"A\", \"B\", \"C\"], \"value1\": [1, 2, 3], \"value2\": [4, 5, None]},\n        )\n        pd.testing.assert_frame_equal(result, expected)",
        "variables": [
            "self",
            "result",
            "expected",
            "df1",
            "df2"
        ],
        "docstring": "Test merge_multi with 'left' merge."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_merge_multi_right",
        "code_chunk": "def test_merge_multi_right(self):\n        \"\"\"Test merge_multi with 'right' merge.\"\"\"\n        df1 = pd.DataFrame({\"key\": [\"A\", \"B\"], \"value1\": [1, 2]})\n        df2 = pd.DataFrame({\"key\": [\"A\", \"B\", \"C\"], \"value2\": [4, 5, 6]})\n\n        result = merge_multi_dfs([df1, df2], on=\"key\", how=\"right\")\n        expected = pd.DataFrame(\n            {\"key\": [\"A\", \"B\", \"C\"], \"value1\": [1, 2, None], \"value2\": [4, 5, 6]},\n        )\n        pd.testing.assert_frame_equal(result, expected)",
        "variables": [
            "self",
            "result",
            "expected",
            "df1",
            "df2"
        ],
        "docstring": "Test merge_multi with 'right' merge."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_merge_multi_fillna",
        "code_chunk": "def test_merge_multi_fillna(self):\n        \"\"\"Test merge_multi with filling missing values.\"\"\"\n        df1 = pd.DataFrame({\"key\": [\"A\", \"B\"], \"value1\": [1, 2]})\n        df2 = pd.DataFrame({\"key\": [\"A\", \"C\"], \"value2\": [4, 5]})\n\n        result = merge_multi_dfs([df1, df2], on=\"key\", how=\"outer\", fillna_val=0)\n        expected = pd.DataFrame(\n            {\"key\": [\"A\", \"B\", \"C\"], \"value1\": [1, 2, 0], \"value2\": [4, 0, 5]},\n        )\n        pd.testing.assert_frame_equal(result, expected, check_dtype=False)",
        "variables": [
            "self",
            "result",
            "expected",
            "df1",
            "df2"
        ],
        "docstring": "Test merge_multi with filling missing values."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_merge_multi_invalid_how",
        "code_chunk": "def test_merge_multi_invalid_how(self):\n        \"\"\"Test merge_multi with an invalid 'how' value.\"\"\"\n        df1 = pd.DataFrame({\"key\": [\"A\", \"B\"], \"value1\": [1, 2]})\n        df2 = pd.DataFrame({\"key\": [\"A\", \"B\"], \"value2\": [4, 5]})\n\n        with pytest.raises(ValueError, match=\"`how` Must be one of\"):\n            merge_multi_dfs([df1, df2], on=\"key\", how=\"invalid_method\")",
        "variables": [
            "self",
            "df2",
            "df1"
        ],
        "docstring": "Test merge_multi with an invalid 'how' value."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_merge_multi_non_iterable_df_list",
        "code_chunk": "def test_merge_multi_non_iterable_df_list(self):\n        \"\"\"Test merge_multi with a non-iterable `df_list`.\"\"\"\n        df1 = pd.DataFrame({\"key\": [\"A\", \"B\"], \"value1\": [1, 2]})\n\n        with pytest.raises(\n            TypeError,\n            match=\"`df_list` must be a list of pandas DataFrames.\",\n        ):\n            merge_multi_dfs(df1, on=\"key\", how=\"inner\")",
        "variables": [
            "self",
            "df1"
        ],
        "docstring": "Test merge_multi with a non-iterable `df_list`."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_merge_multi_non_dataframe_in_list",
        "code_chunk": "def test_merge_multi_non_dataframe_in_list(self):\n        \"\"\"Test merge_multi with a non-DataFrame element in `df_list`.\"\"\"\n        df1 = pd.DataFrame({\"key\": [\"A\", \"B\"], \"value1\": [1, 2]})\n\n        with pytest.raises(\n            TypeError,\n            match=\"`df_list` must be a list of pandas DataFrames.\",\n        ):\n            merge_multi_dfs([df1, \"not_a_dataframe\"], on=\"key\", how=\"inner\")",
        "variables": [
            "self",
            "df1"
        ],
        "docstring": "Test merge_multi with a non-DataFrame element in `df_list`."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "test_merge_multi_invalid_on",
        "code_chunk": "def test_merge_multi_invalid_on(self):\n        \"\"\"Test merge_multi with invalid 'on' parameter.\"\"\"\n        df1 = pd.DataFrame({\"key\": [\"A\", \"B\"], \"value1\": [1, 2]})\n        df2 = pd.DataFrame({\"key\": [\"A\", \"B\"], \"value2\": [4, 5]})\n\n        with pytest.raises(\n            TypeError,\n            match=\"`on` must be a string or a list of strings.\",\n        ):\n            merge_multi_dfs([df1, df2], on=123, how=\"inner\")",
        "variables": [
            "self",
            "df2",
            "df1"
        ],
        "docstring": "Test merge_multi with invalid 'on' parameter."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "sample_function",
        "code_chunk": "def sample_function(delay):\n            sleep(delay)\n            return \"Done\"",
        "variables": [
            "delay"
        ],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "sample_function",
        "code_chunk": "def sample_function():\n            return \"No arguments\"",
        "variables": [],
        "docstring": null
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\helpers\\test_python.py",
        "function_name": "my_gen",
        "code_chunk": "def my_gen():\n            \"\"\"Test method.\"\"\"\n            yield 5\n            yield 10\n            yield 15",
        "variables": [],
        "docstring": "Test method."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\conftest.py",
        "function_name": "json_config_string",
        "code_chunk": "def json_config_string() -> str:\n    \"\"\"Fixture to provide a json format string.\"\"\"\n    return (\n        \"{\"\n        '\"section_1\": {\"value_1_1\": 1, \"value_1_2\": \"a\"},'\n        '\"section_2\": {\"value_2_1\": [3, 4], \"value_2_2\": [7, 8, 9]},'\n        ' \"section_3\": {\"value_3_1\": {\"value_3_1_1\": 2, \"value_3_1_2\": \"b\"}},'\n        '\"section_4\": {\"value_4_1\": true}'\n        \"}\"\n    )",
        "variables": [],
        "docstring": "Fixture to provide a json format string."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\conftest.py",
        "function_name": "toml_config_string",
        "code_chunk": "def toml_config_string() -> str:\n    \"\"\"Fixture to provide a toml format string.\"\"\"\n    return \"\"\"\n        [section_1]\n        value_1_1 = 1\n        value_1_2 = 'a'\n\n        [section_2]\n        value_2_1 = [3, 4]\n        value_2_2 = [7, 8, 9]\n\n        [section_3]\n            [section_3.value_3_1]\n            value_3_1_1 = 2\n            value_3_1_2 = 'b'\n\n        [section_4]\n        value_4_1 = true\n    \"\"\"",
        "variables": [],
        "docstring": "Fixture to provide a toml format string."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\conftest.py",
        "function_name": "yaml_config_string",
        "code_chunk": "def yaml_config_string() -> str:\n    \"\"\"Fixture to provide a yaml format string.\"\"\"\n    return \"\"\"\n        section_1:\n            value_1_1: 1\n            value_1_2: a\n\n        section_2:\n            value_2_1:\n                - 3\n                - 4\n            value_2_2:\n                - 7\n                - 8\n                - 9\n\n        section_3:\n            value_3_1:\n                value_3_1_1: 2\n                value_3_1_2: b\n\n        section_4:\n            value_4_1: True\n    \"\"\"",
        "variables": [],
        "docstring": "Fixture to provide a yaml format string."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\conftest.py",
        "function_name": "expected_standard_config",
        "code_chunk": "def expected_standard_config() -> Dict[str, Any]:\n    \"\"\"Fixture providing the loaded config from loading the temp file.\"\"\"\n    return {\n        \"section_1\": {\n            \"value_1_1\": 1,\n            \"value_1_2\": \"a\",\n        },\n        \"section_2\": {\n            \"value_2_1\": [3, 4],\n            \"value_2_2\": [7, 8, 9],\n        },\n        \"section_3\": {\n            \"value_3_1\": {\n                \"value_3_1_1\": 2,\n                \"value_3_1_2\": \"b\",\n            },\n        },\n        \"section_4\": {\n            \"value_4_1\": True,\n        },\n    }",
        "variables": [],
        "docstring": "Fixture providing the loaded config from loading the temp file."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_config.py",
        "function_name": "config_json",
        "code_chunk": "def config_json(self, json_config_string, tmp_path_factory) -> Path:\n        \"\"\"Fixture to create json file that exists for the length of the class tests.\"\"\"\n        config_file = tmp_path_factory.mktemp(\"data\") / \"config.json\"\n        config_file.write_text(json_config_string)\n        return config_file",
        "variables": [
            "self",
            "tmp_path_factory",
            "config_file",
            "json_config_string"
        ],
        "docstring": "Fixture to create json file that exists for the length of the class tests."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_config.py",
        "function_name": "config_toml",
        "code_chunk": "def config_toml(self, toml_config_string, tmp_path_factory) -> Path:\n        \"\"\"Fixture to create toml file that exists for the length of the class tests.\"\"\"\n        config_file = tmp_path_factory.mktemp(\"data\") / \"config.toml\"\n        config_file.write_text(toml_config_string)\n        return config_file",
        "variables": [
            "self",
            "tmp_path_factory",
            "config_file",
            "toml_config_string"
        ],
        "docstring": "Fixture to create toml file that exists for the length of the class tests."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_config.py",
        "function_name": "config_yaml",
        "code_chunk": "def config_yaml(self, yaml_config_string, tmp_path_factory) -> Path:\n        \"\"\"Fixture to create yaml file that exists for the length of the class tests.\"\"\"\n        config_file = tmp_path_factory.mktemp(\"data\") / \"config.yaml\"\n        config_file.write_text(yaml_config_string)\n        return config_file",
        "variables": [
            "self",
            "tmp_path_factory",
            "config_file",
            "yaml_config_string"
        ],
        "docstring": "Fixture to create yaml file that exists for the length of the class tests."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_config.py",
        "function_name": "test_load_config_file_type",
        "code_chunk": "def test_load_config_file_type(\n        self,\n        config_file,\n        config_type,\n        expected_config,\n    ):\n        \"\"\"Test class can load different file types.\"\"\"\n        actual = LoadConfig(\n            config_path=config_file,\n            config_type=config_type,\n        )\n\n        assert actual.config == expected_config",
        "variables": [
            "self",
            "config_file",
            "config_type",
            "actual",
            "expected_config"
        ],
        "docstring": "Test class can load different file types."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_config.py",
        "function_name": "test_has_config_contents_as_attributes",
        "code_chunk": "def test_has_config_contents_as_attributes(\n        self,\n        config_yaml,\n        expected_standard_config,\n    ):\n        \"\"\"Test returned class has an attribute for each section of config.\"\"\"\n        actual = LoadConfig(\n            config_path=config_yaml,\n        )\n\n        for key, value in expected_standard_config.items():\n            print(key)  # noqa: T201\n            assert hasattr(actual, key)\n            assert getattr(actual, key) == value",
        "variables": [
            "expected_standard_config",
            "self",
            "config_yaml",
            "key",
            "actual",
            "value"
        ],
        "docstring": "Test returned class has an attribute for each section of config."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_config.py",
        "function_name": "test_raises_config_error_applying_unsupported_config_file_type",
        "code_chunk": "def test_raises_config_error_applying_unsupported_config_file_type(\n        self,\n        config_yaml,\n    ):\n        \"\"\"Test class can raises ConfigError for unsupported config file type.\"\"\"\n        with pytest.raises(\n            ConfigError,\n            match=\"No config parser present for file type = xlsx\",\n        ):\n            LoadConfig(\n                config_path=config_yaml,\n                config_type=\"xlsx\",\n            )",
        "variables": [
            "self",
            "config_yaml"
        ],
        "docstring": "Test class can raises ConfigError for unsupported config file type."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_config.py",
        "function_name": "test_expected_applying_config_override",
        "code_chunk": "def test_expected_applying_config_override(\n        self,\n        config_yaml,\n        expected_standard_config,\n    ):\n        \"\"\"Test class applies config overrides.\"\"\"\n        config_overrides = {\n            \"section_1\": {\n                \"value_1_2\": \"z\",\n            },\n            \"section_2\": {\n                \"value_2_2\": [9, 8, 7],\n            },\n            \"section_3\": {\n                \"value_3_1\": {\n                    \"value_3_1_2\": \"y\",\n                },\n            },\n            \"section_4\": {\n                \"value_4_1\": True,\n            },\n        }\n\n        actual = LoadConfig(\n            config_path=config_yaml,\n            config_overrides=config_overrides,\n        )\n\n        expected = {\n            \"section_1\": {\n                \"value_1_1\": 1,\n                \"value_1_2\": \"z\",\n            },\n            \"section_2\": {\n                \"value_2_1\": [3, 4],\n                \"value_2_2\": [9, 8, 7],\n            },\n            \"section_3\": {\n                \"value_3_1\": {\n                    \"value_3_1_1\": 2,\n                    \"value_3_1_2\": \"y\",\n                },\n            },\n            \"section_4\": {\n                \"value_4_1\": True,\n            },\n        }\n\n        assert actual.config == expected\n\n        assert actual.config_original == expected_standard_config",
        "variables": [
            "expected_standard_config",
            "self",
            "config_yaml",
            "expected",
            "actual",
            "config_overrides"
        ],
        "docstring": "Test class applies config overrides."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_config.py",
        "function_name": "test_raises_config_error_applying_bad_config_override",
        "code_chunk": "def test_raises_config_error_applying_bad_config_override(\n        self,\n        config_yaml,\n    ):\n        \"\"\"Test class raises ConfigError for bad config overrides.\"\"\"\n        with pytest.raises(ConfigError, match=\"not in the base dictionary\"):\n            LoadConfig(\n                config_path=config_yaml,\n                config_overrides={\"section_4\": {\"value_4_2\": \"not in base config\"}},\n            )",
        "variables": [
            "self",
            "config_yaml"
        ],
        "docstring": "Test class raises ConfigError for bad config overrides."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_config.py",
        "function_name": "test_expected_apply_all_config_validators",
        "code_chunk": "def test_expected_apply_all_config_validators(\n        self,\n        config_yaml,\n        expected_standard_config,\n    ):\n        \"\"\"Test applying validation maintains config values.\"\"\"\n        actual = LoadConfig(\n            config_path=config_yaml,\n            config_validators={\n                \"section_1\": ValidatorSection1,\n                \"section_2\": ValidatorSection2,\n                \"section_3\": ValidatorSection3,\n                \"section_4\": ValidatorSection4,\n            },\n        )\n\n        assert actual.config == expected_standard_config",
        "variables": [
            "actual",
            "self",
            "config_yaml",
            "expected_standard_config"
        ],
        "docstring": "Test applying validation maintains config values."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_config.py",
        "function_name": "test_raise_warning_if_apply_some_config_validators",
        "code_chunk": "def test_raise_warning_if_apply_some_config_validators(\n        self,\n        config_yaml,\n        config_validators,\n        expected_standard_config,\n    ):\n        \"\"\"Test applying only some validators raises appropriate warnings.\"\"\"\n        with pytest.warns(\n            UserWarning,\n            match=\"No validator provided, config contents unvalidated.\",\n        ):\n            actual = LoadConfig(\n                config_path=config_yaml,\n                config_validators=config_validators,\n            )\n            assert actual.config == expected_standard_config",
        "variables": [
            "expected_standard_config",
            "self",
            "config_yaml",
            "config_validators",
            "actual"
        ],
        "docstring": "Test applying only some validators raises appropriate warnings."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_config.py",
        "function_name": "test_raises_validation_error_config_override_changes_value_type_and_apply_config_validators",
        "code_chunk": "def test_raises_validation_error_config_override_changes_value_type_and_apply_config_validators(\n        self,\n        config_yaml,\n    ):\n        \"\"\"Test that if config override changes value type and using validators then validation error raised.\"\"\"\n        with pytest.raises(ValidationError, match=\"value_4_1\"):\n            LoadConfig(\n                config_path=config_yaml,\n                config_overrides={\"section_4\": {\"value_4_1\": \"should be bool\"}},\n                config_validators={\n                    \"section_1\": ValidatorSection1,\n                    \"section_2\": ValidatorSection2,\n                    \"section_3\": ValidatorSection3,\n                    \"section_4\": ValidatorSection4,\n                },\n            )",
        "variables": [
            "self",
            "config_yaml"
        ],
        "docstring": "Test that if config override changes value type and using validators then validation error raised."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_input.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(\n        self,\n        json_config_string,\n        expected_standard_config,\n    ):\n        \"\"\"Test expected functionality.\"\"\"\n        actual = parse_json(json_config_string)\n\n        assert actual == expected_standard_config",
        "variables": [
            "actual",
            "self",
            "expected_standard_config",
            "json_config_string"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_input.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(\n        self,\n        toml_config_string,\n        expected_standard_config,\n    ):\n        \"\"\"Test expected functionality.\"\"\"\n        actual = parse_toml(toml_config_string)\n\n        assert actual == expected_standard_config",
        "variables": [
            "actual",
            "self",
            "expected_standard_config",
            "toml_config_string"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_input.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(\n        self,\n        yaml_config_string,\n        expected_standard_config,\n    ):\n        \"\"\"Test expected functionality.\"\"\"\n        actual = parse_yaml(yaml_config_string)\n\n        assert actual == expected_standard_config",
        "variables": [
            "actual",
            "self",
            "expected_standard_config",
            "yaml_config_string"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_input.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        pass",
        "variables": [
            "self"
        ],
        "docstring": "Test expected functionality."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_output.py",
        "function_name": "_setup_and_teardown",
        "code_chunk": "def _setup_and_teardown(self, tmp_path):\n        \"\"\"Set up and tear down for tests.\"\"\"\n        self.tmp_path = tmp_path\n        self.source_dir = tmp_path / \"source\"\n        self.source_dir.mkdir()\n        self.file_in_source = self.source_dir / \"file.txt\"\n        self.file_in_source.write_text(\"content\")\n        yield\n        shutil.rmtree(self.tmp_path)",
        "variables": [
            "self",
            "tmp_path"
        ],
        "docstring": "Set up and tear down for tests."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_output.py",
        "function_name": "test_zip_folder_success",
        "code_chunk": "def test_zip_folder_success(self):\n        \"\"\"Test zipping a directory successfully.\"\"\"\n        output_filename = self.tmp_path / \"output.zip\"\n        assert zip_folder(str(self.source_dir), str(output_filename), overwrite=True)\n        assert output_filename.exists()",
        "variables": [
            "self",
            "output_filename"
        ],
        "docstring": "Test zipping a directory successfully."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_output.py",
        "function_name": "test_zip_folder_no_source_directory",
        "code_chunk": "def test_zip_folder_no_source_directory(self):\n        \"\"\"Test zipping with a non-existing source directory.\"\"\"\n        non_existing_dir = self.tmp_path / \"non_existing\"\n        output_filename = self.tmp_path / \"output.zip\"\n        assert not zip_folder(str(non_existing_dir), str(output_filename))\n        assert not output_filename.exists()",
        "variables": [
            "self",
            "output_filename",
            "non_existing_dir"
        ],
        "docstring": "Test zipping with a non-existing source directory."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_output.py",
        "function_name": "test_zip_folder_output_not_zip",
        "code_chunk": "def test_zip_folder_output_not_zip(self):\n        \"\"\"Test zipping with an output filename not ending in .zip.\"\"\"\n        output_filename = self.tmp_path / \"output.txt\"\n        assert not zip_folder(str(self.source_dir), str(output_filename))\n        assert not output_filename.exists()",
        "variables": [
            "self",
            "output_filename"
        ],
        "docstring": "Test zipping with an output filename not ending in .zip."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_output.py",
        "function_name": "test_zip_folder_overwrite_false",
        "code_chunk": "def test_zip_folder_overwrite_false(self):\n        \"\"\"Test zipping with overwrite set to False.\"\"\"\n        output_filename = self.tmp_path / \"output.zip\"\n        output_filename.touch()\n        assert not zip_folder(\n            str(self.source_dir),\n            str(output_filename),\n            overwrite=False,\n        )\n        assert output_filename.exists()\n        assert output_filename.stat().st_size == 0",
        "variables": [
            "self",
            "output_filename"
        ],
        "docstring": "Test zipping with overwrite set to False."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\io\\test_output.py",
        "function_name": "test_zip_folder_overwrite_true",
        "code_chunk": "def test_zip_folder_overwrite_true(self):\n        \"\"\"Test zipping with overwrite set to True.\"\"\"\n        output_filename = self.tmp_path / \"output.zip\"\n        output_filename.touch()\n        assert zip_folder(str(self.source_dir), str(output_filename), overwrite=True)\n        assert output_filename.exists()\n        assert output_filename.stat().st_size > 0",
        "variables": [
            "self",
            "output_filename"
        ],
        "docstring": "Test zipping with overwrite set to True."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\methods\\test_averaging_methods.py",
        "function_name": "input_df",
        "code_chunk": "def input_df(create_spark_df):\n    \"\"\"Fixture containing input data for tests.\"\"\"\n    return create_spark_df(\n        [\n            (\"price\", \"quantity\", \"weight\"),\n            (0.7, 1, 0.090909091),\n            (1.0, 5, 0.454545455),\n            (1.5, 3, 0.272727273),\n            (1.4, 2, 0.181818182),\n        ],\n    )",
        "variables": [
            "create_spark_df"
        ],
        "docstring": "Fixture containing input data for tests."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\methods\\test_averaging_methods.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self, input_df, create_spark_df):\n        \"\"\"Test expected output.\"\"\"\n        actual = input_df.agg(\n            weighted_arithmetic_average(\"price\", \"weight\").alias(\"average\"),\n        )\n        expected = create_spark_df(\n            [\n                (\"average\",),\n                (1.1818182,),\n            ],\n        )\n        assert_approx_df_equality(\n            actual,\n            expected,\n            precision=1e-7,\n            ignore_nullable=True,\n        )",
        "variables": [
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "actual"
        ],
        "docstring": "Test expected output."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\methods\\test_averaging_methods.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self, input_df, create_spark_df):\n        \"\"\"Test expected output.\"\"\"\n        actual = input_df.agg(\n            weighted_geometric_average(\"price\", \"weight\").alias(\"average\"),\n        )\n        expected = create_spark_df(\n            [\n                (\"average\",),\n                (1.1495070,),\n            ],\n        )\n        assert_approx_df_equality(\n            actual,\n            expected,\n            precision=1e-7,\n            ignore_nullable=True,\n        )",
        "variables": [
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "actual"
        ],
        "docstring": "Test expected output."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\methods\\test_averaging_methods.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self, input_df, create_spark_df):\n        \"\"\"Test expected output.\"\"\"\n        actual = input_df.agg(unweighted_arithmetic_average(\"price\").alias(\"average\"))\n        expected = create_spark_df(\n            [\n                (\"average\",),\n                (1.15,),\n            ],\n        )\n        assert_approx_df_equality(\n            actual,\n            expected,\n            precision=1e-7,\n            ignore_nullable=True,\n        )",
        "variables": [
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "actual"
        ],
        "docstring": "Test expected output."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\methods\\test_averaging_methods.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self, input_df, create_spark_df):\n        \"\"\"Test expected output.\"\"\"\n        actual = input_df.agg(unweighted_geometric_average(\"price\").alias(\"average\"))\n        expected = create_spark_df(\n            [\n                (\"average\",),\n                (1.1011065,),\n            ],\n        )\n        assert_approx_df_equality(\n            actual,\n            expected,\n            precision=1e-7,\n            ignore_nullable=True,\n        )",
        "variables": [
            "self",
            "expected",
            "input_df",
            "create_spark_df",
            "actual"
        ],
        "docstring": "Test expected output."
    },
    {
        "repo_name": "rdsa-utils",
        "filepath": "D:/coding_repos/rdsa-utils\\tests\\methods\\test_averaging_methods.py",
        "function_name": "test_expected",
        "code_chunk": "def test_expected(self):\n        \"\"\"Test expected functionality.\"\"\"\n        input_df = create_spark_df(\n            [\n                (\"group\", \"quantity\"),\n                (\"first\", 1),\n                (\"first\", 5),\n                (\"first\", 3),\n                (\"first\", 2),\n                (\"second\", 2),\n                (\"second\", 6),\n                (\"second\", 4),\n                (\"second\", 3),\n            ],\n        )\n\n        expected = create_spark_df(\n            [\n                (\"group\", \"quantity\", \"weight\"),\n                (\"first\", 1, 0.090909091),\n                (\"first\", 5, 0.454545455),\n                (\"first\", 3, 0.272727273),\n                (\"first\", 2, 0.181818182),\n                (\"second\", 2, 0.133333333),\n                (\"second\", 6, 0.4),\n                (\"second\", 4, 0.266666666),\n                (\"second\", 3, 0.2),\n            ],\n        )\n\n        weights = get_weight_shares(\n            weights=\"quantity\",\n            levels=\"group\",\n        )\n        actual = input_df.withColumn(\"weight\", weights)\n        assert_approx_df_equality(\n            actual,\n            expected,\n            precision=1e-7,\n            ignore_nullable=True,\n        )",
        "variables": [
            "self",
            "expected",
            "input_df",
            "actual",
            "weights"
        ],
        "docstring": "Test expected functionality."
    }
]